{"meta":{"title":"Not late yet","subtitle":"社会喧嚣，工作枯燥，青春渐逝。幡然醒悟，重燃激情，即刻出发。","description":"关注深度学习、广告计算、推荐系统相关领域的算法与工程实践","author":"ACDance","url":"https://www.NotLate.net","root":"/"},"pages":[{"title":"关于作者","date":"2021-01-07T16:01:32.000Z","updated":"2021-03-18T15:04:38.598Z","comments":false,"path":"about/index.html","permalink":"https://www.notlate.net/about/index.html","excerpt":"","text":""},{"title":"归档","date":"2021-01-07T16:03:31.000Z","updated":"2021-03-18T15:04:38.598Z","comments":false,"path":"archives/index.html","permalink":"https://www.notlate.net/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-01-07T16:01:06.000Z","updated":"2021-03-18T15:04:38.602Z","comments":false,"path":"categories/index.html","permalink":"https://www.notlate.net/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-01-07T16:01:22.000Z","updated":"2021-03-18T15:04:38.602Z","comments":false,"path":"tags/index.html","permalink":"https://www.notlate.net/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深度学习推荐系统-总结02：美团技术团队精选博文","slug":"RecSys/[深度学习推荐系统-总结02]美团技术团队精华文章收集","date":"2021-03-11T06:04:41.000Z","updated":"2021-03-18T15:04:38.598Z","comments":true,"path":"posts/a03121e1.html","link":"","permalink":"https://www.notlate.net/posts/a03121e1.html","excerpt":"","text":"本文主要收藏“美团技术团队博客”在广告、搜索、推荐等相关领域的优秀博文与实践经验。 推荐 2020.12.10 ICDM论文：探索跨会话信息感知的推荐模型 2020.08.27 KDD Cup 2020 AutoGraph比赛冠军技术方案及在美团的实践 2020.08.20 KDD Cup 2020 Debiasing比赛冠军技术方案及在美团的实践 2019.02.21 深度学习在美团配送ETA预估中的探索与实践 2018.11.15 强化学习在美团“猜你喜欢”的实践 2018.03.29 美团“猜你喜欢”深度学习排序模型实践 2017.07.28 深度学习在美团推荐平台排序中的运用 2017.06.16 美团点评旅游搜索召回策略的演进 2017.03.24 旅游推荐系统的演进 2016.12.09 外卖排序系统特征生产框架 2016.03.03 深入FFM原理与实践 2015.01.22 美团推荐算法实践 2014.12.18 基于机器学习方法的POI品类推荐算法 搜索 2020.09.27 KDD Cup 2020多模态召回比赛亚军方案与搜索业务应用 2020.09.27 KDD Cup 2020多模态召回比赛季军方案与搜索业务应用 2020.08.20 MT-BERT在文本检索任务中的实践 2020.07.23 美团搜索中NER技术的探索与实践 2020.07.16 智能搜索模型预估框架Augur的建设与实践 2020.07.09 BERT在美团搜索核心排序的探索和实践 2020.04.16 Transformer 在美团搜索排序中的实践 2020.03.26 WSDM Cup 2020检索排序评测任务第一名经验总结 2019.11.14 美团BERT的探索和实践 2019.01.10 深度学习在搜索业务中的探索与实践 2015.12.07 美团O2O排序解决方案——线下篇 2015.11.26 美团O2O排序解决方案——线上篇 广告 2019.11.28 美团点评效果广告实验配置平台的设计与实现 2019.03.14 大众点评信息流基于文本生成的创意优化实践 2018.06.07 深度学习在美团搜索广告排序的应用实践 2018.05.11 美团广告实时索引的设计与实现 2017.12.04 美团点评联盟广告场景化定向排序机制 2017.05.05 美团DSP广告策略实践 知识图谱 2020.12.03 CIKM 2020 | 一文详解美团6篇精选论文 2019.01.17 大众点评搜索基于知识图谱的深度学习排序实践 2018.11.10 美团大脑：知识图谱的建模方法及其应用 工程实践 2020.07.01 美团万亿级 KV 存储架构与实践 2020.05.28 美团配送A/B评估体系建设实践 2019.08.15 MTFlexbox自动化埋点探索 2018.12.20 深入浅出排序学习：写给程序员的算法系统开发实践 2018.10.25 美团深度学习系统的工程实践 2018.10.18 美团基于 Flink 的实时数仓建设实践 2018.07.27 美团针对Redis Rehash机制的探索和实践 2018.03.20 每天数百亿用户行为数据，美团点评怎么实现秒级转化分析？ 2018.03.16 Redis 高负载下的中断优化 2018.01.26 美团点评基于Storm的实时数据处理实践 2017.11.17 流计算框架 Flink 与 Storm 的性能对比 2017.04.21 Leaf——美团点评分布式ID生成系统 2016.05.12 Spark性能优化指南——高级篇 2016.04.29 Spark性能优化指南——基础篇 2016.03.13 Spark在美团的实践 2015.02.10 机器学习中的数据清洗与特征处理综述 本文会持续收集更新，首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"},{"name":"工程实践","slug":"工程实践","permalink":"https://www.notlate.net/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"},{"name":"知识图谱","slug":"知识图谱","permalink":"https://www.notlate.net/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"}]},{"title":"深度学习推荐系统-总结01：推荐算法发展里程碑","slug":"RecSys/[深度学习推荐系统-总结01]推荐算法里程碑","date":"2021-03-08T06:47:31.000Z","updated":"2021-03-18T15:04:38.598Z","comments":true,"path":"posts/7ed931fc.html","link":"","permalink":"https://www.notlate.net/posts/7ed931fc.html","excerpt":"","text":"推荐算法从传统方法到深度学习方法发展里程碑 时间 作者 技术名称 说明 2003 Amazon ItemCF(基于物品的协同过滤) 不仅让Amazon的推荐系统广为人知，更让协同过滤成为今后很长时间的研究热点和业界主流的推荐模型。 2006 Netflix MF(矩阵分解) 在Netflix Prize Challenge中，以矩阵分解为主的推荐算法大放异彩，拉开了矩阵分解在业界流行的序幕。 LR(逻辑回归) 深度学习的基础性结构，能够融合多特征，成为了独立于协同过滤的推荐模型的另一个重要发展方向 2010 Rendle FM FM在2012~2014年前后，成为业界主流的推荐模型之一 2015 Yuchi Juan FFM 基于FM提出的FFM在多项CRT预估大赛中夺魁，并被Criteo、美团等公司深度应用在推荐系统、CTR预估等领域 2014 Facebook GBDT+LR 利用GBDT自动进行特征筛选和组合，解决了工程上应用LR的特征组合难度。 2017 阿里巴巴 LS-PLM 自动2012年起成为阿里巴巴主流推荐模型，直至2017年才公开，与三层神经网络及其相似，可以把它看成连接前后深度学习时代的节点。 2015 澳大利亚国立大学 AutoRec 将自编码器的思想与协同过滤结合，提出了一种单隐层神经网络推荐模型 2016 微软 Deep Crossing 深度学习架构在推荐系统中的完整应用 2016 谷歌 Wide&amp;Deep 自提出至今一直在业界发挥着巨大影响力的模型，Wide部分具有逻辑回归的优点，Deep部分具有深度神经网络的优点。 2018 阿里巴巴 DIN 是一次基于实际业务观察的模型改进，提了业界非常知名的深度兴趣网络，把注意力机制引入深度学习推荐模型，用于捕捉用户兴趣。 2019 阿里巴巴 DIEN 兴趣进化网络，DIN的演化版本，引入序列模型，用于模拟用户兴趣进化过程。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"协同过滤","slug":"协同过滤","permalink":"https://www.notlate.net/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"}]},{"title":"机器之心-2020年AI进展及2021年趋势关注重点笔记","slug":"Review/02.机器之心人工智能2021年技术趋势记录","date":"2021-02-26T12:22:15.000Z","updated":"2021-03-18T15:04:38.598Z","comments":true,"path":"posts/d837735f.html","link":"","permalink":"https://www.notlate.net/posts/d837735f.html","excerpt":"","text":"1. 人工智能技术总趋势 无监督学习是未来技术突破的主要着力点 在语言技术和视觉技术这两个人工智能感知层应用落地最成熟的两个技术领域，无监督学习都是被提及频率最高的技术发展趋势。NLP领域目前最成功的大规模预训练语言模型（BERT等）证明了无监督学习在自言语言处理类任务里的可行性与发展潜力，transformer体系结构已成为NLP任务的实际标准，开始扩展至最相关的语音技术领域，而计算机视觉领域相比NLP更早使用注意力机制，但对于trnasformer模型的应用仍然受到卷及网络结构本身的限制。由Google Brain团队开发的最新视觉transformer模型（ViT）已经在突破这种限制上作出了可行的尝试，通过对大量数据的预训练，该模型已经可以成功迁移运营在多个中小型CV数据及（如ImageNet、CIFAR-100/VTAB等）上并取得目前最先进的SOTA表现。 2018年的图领奖的共同获得者Geoffrey Hinton、Yann LeCun以及Yoshua Bengio几年的研究重点也一直放在无监督学习层面：Hinton近年来专注研究的胶囊网络的最新版本在无监督学习上找到了最合适发展方式；LeCun一致提倡的自监督学习方法则是一种“利用完整数据监督不完整数据修复”的特殊的无监督学习；而Bengio提倡的借鉴认知科学的符号推理相结合的方法同样需要研究先验知识如何降低数据的需求。三位业界巨擘都认识到无监督学习在解决深度学习缺乏场景数据和计算资源的问题上有巨大的可能性。诸多数据表明，无监督学习有利于解决目前人工智能技术的主要瓶颈之一：数据不足问题，与其希望管的少样本学习、模仿学习、自监督学习以及对比学习等都有着无限可能性。 移动设备和IoT设备开始承担更多AI/ML计算 随着移动智能手机和IoT设备的计算能力越来越强，用户和监督机构对数据隐私的保护越来越重视，同时移动网络5G的逐步部署落地，移动设备和IoT设备会进行更多的AI/ML的计算和训练。这使得人工智能离数据更近，离用户更近，也能更迅速地进行迭代升级。一方面，云计算发展依旧迅猛，并逐渐向边缘计算发展，助力物联网智能化；另一方面，AI芯片也主线走向云端、边缘端以及物联网设备终端，有实力的科技公司将AI算力打包成服务，既合理利用了自身多余的算力资源，又降低了不具备相关科研实力的中小企业可以以更低的成本使用到足够的资源，完成自身的AI开发与部署需求，这与近年来媒体讨论度颇高的无服务计算有着一致的思想理念。 图神经网络、土深度学习、大规模图计算平台将有很大发展空间 几年来，受社交网络、知识图谱以及生物信息学和神经科学中图形的重要性所驱动，将深度学习模型应用在不规则图结构的数据上逐渐成为了一种新兴趋势。这些基于图的神经网络（GNN）通常将神经网络模型应用在图中与点和边相关的特征上，传播运算结果并进行聚合，从而生成下一级的特征，目前已经在分类、嵌入、问答等多种不同的目标应用中取得最佳表现效果。2019~2020年，无论是在自然语言处理、计算机视觉、机器学习的相关学术会议上，图神经网络的相关论文均占据着不小比例。这种方法“将端到端学习与归纳推理相结合，业界普遍认为其有望解决深度学习无法处理的因果推理、可解释性等一系列瓶颈问题”。相比传统的文本、语音、图像数据格式，图结构是一种泛用性更强且更能代表现实世界真实情况的数据表示方式。随着知识图谱与推荐系统得到越来越广阔的应用，能够更切合相关数据表示的图神经网络深度学习未来大有可为。 2. 人类语言技术 大规模预训练语言模型 2018~2019年里，尤其是2019年，大规模预训练语言模型似乎成了NLP领域技术指标突破逃不开的技术方法。在ELMo之前，深度学习在自然语言处理的主要应用方向在于从词到句子级别的分布式表征，word2vec是此类方法的典型代表。虽然一时间预训练的词向量在诸多自然语言处理任务上取得了不错的进展，但词向量本身都是上下文无关的，也就是一个词的向量表示在不同语境中总是相同的，这就意味着词向量无法解决语言技术里很大的一个障碍，一词多义问题。因此研究人员们开始在具体的下游任务中基于词向量的序列来做编码，这种seq2seq端到端的思想迅速取代了word2vec的热潮，而谷歌在这个基础上研发出的自注意力机制模型transformer可算是这个思想的集大成者。也正是同一时间，来自华盛顿大学的研究者开发出了ELMo模型，通过语言模型来获得一个上下文相关的预训练表示，并通过定制成功的用在了多个不同的下游任务上取得领先的表现效果。而仅仅针对文本进行丰富的encoding尚不足以覆盖所有NLP下游任务，尤其是QA、阅读理解等包含问询关系等多种复杂模式的情况下，下游任务的网络需要更多不同的结构调整，同一个模型在多个不同任务、甚至同一任务不同数据集上的表现差别仍然有待提高，这也就促使了BERT的诞生。BERT类模型使用大规模语料数据采取更深程度的预训练，从而形成一个更为通用的模型基底。这比覆盖上下文语境含义的词向量更进一步，而涉及到下游任务时也仅需要定制一个非常轻量级的输出层而不需要对模型骨架做重大调整。 3. 计算机视觉 计算机视觉下游主要技术任务：人脸识别、物体识别、目标检测、风格迁移、姿态估计、语义分割、图像降噪等； 主要的模型方法：CNN、点云等； 学界问卷情况（摘录重点） 图像处理与生成领域：较为成熟的部分为图像降噪、图像增强、图像超分辨率等技术任务，而图像生成被认为还是停留在“工程化验证其广泛使用的可行性以及解决规划化及成本等工程问题的阶段”，仍需要充分理解图像生成内在机制，优化参数控制能力。目前图像处理与生成的速度、泛化性、鲁棒性是瓶颈，以及用户对图像生成还不可控。 图像分割领域：专家认为其实时性、边缘锐化性扔有待提高，与多任务、多模态相结合也可以产生新的研究成果。 三维重建、三维推理领域：比点云模型在专家问卷中的得分更高一点，专家认为目前整个计算机视觉领域往三维视觉、机器人控制方面靠拢。3D视觉数据量大，数据结构无需不均匀，建模精度以及建模速度是主要的瓶颈。三维视觉的成本是否能够达到2D图像的技术量级，如何解决工程应用问题实现大规模应用同样是值得探索的方向。 视频理解领域：基本技术有待突破，近期内看不到跨越语义鸿沟的捷径，也许需要等待新的人工智能框架的到来，目前视频方面相关数据缺少标注且视频相比图像计算量过大，存在冗余信息，如何处理这些冗余信息以及如何高效利用复杂信息是一些视频理解的关键问题，实时性、视频理解的颗粒度需要重点考虑，图像和视频生成模型也会有不少突破的技术方向。 4. 机器学习 机器学习的六大主要技术领域：深度学习、强化学习、迁移学习、元学习、联邦学习和自动化机器学习。 生成对抗网络的技术性突破 生成对抗网络（GAN）与2014年被提出，并广泛应用于计算机视觉领域，例如图片生成等任务。GAN被美国《麻省理工科技评论》评为2018年“全球十大突破性技术”。截止2020年，GAN已有几百种变形，影响力较大的有2018年提出的BigGAN、SNGAN和AttnGAN，以及2019年提出的AutoGAN、StyleGAN等。2019年英伟达发布了StyleGAN2，这项能“混淆显示和虚拟界限”的技术被社区认为帮助解决AI社区面临的难题，如抓握式机器臂和自动驾驶。 多模态技术的崛起 随着深度学习技术的发展，不同的研究人员在进行特征识别时，会加上更多模态的特征，这也推动了多模态学习的发展。如在内容理解任务中，可看到自2019年起，CDG广告、CSIG音视频、IEG内容推荐、PCG信息流、TEG书评广告推荐和AI平台部团队、WXG看一看团队都有融合多模态特征进行内容理解需求。微软AI明星产品小冰不仅会作诗，还会唱歌作曲、阅读两宋、撰写新闻等，多模态识别技术正是小冰越来越像人一样沟通表达的关键之一。多模态技术同样也在视频网站、电商物流、自动驾驶等领域得到广泛应用。如爱奇艺退出的“只看TA”功能，优酷视频正在使用的视频帧、人脸帧的图向量检索，都离不开多模态识别技术的支持；京东淘宝等电商平台的“拍照购”、“拍立淘”的搜索技术背后也都是在计算机视觉技术下，使用了图像、文本和高层语义属性等多模态下的信息融合，才实现高精度的“以图搜图”功能；百度提出的“多模态深度语义理解”，则让AI实现了从“看清听清”到“看懂听懂”的进化。 5. 数据智能技术 知识图谱：快速发展，模型、框架、数据集涌现 知识图谱作为一个相对年轻的技术领域，仍然处在发展期，各种知识表征模型和框架、知识图谱开放数据集层出不穷。2019年是知识图谱相关技术飞速发展的一年，世界顶级NLP大会ACL一次性收录了超30篇高质量知识图谱类论文，其中对于关系向量和图神经网络的论述将对知识图谱中关系补全、推理决策和认知计算都带来长足发展。作为集大数据和人工智能于一身的综合技术，知识图谱在人工智能浪潮中将迎来更多发展。 信息检索和推荐系统：深度学习带来突破，相关市场更加繁荣 信息检索和推荐系统的进步来源于深度学习技术的进步，无论是GAN、RNN等神经网络模型的应用，还是排序算法、优化算法的创新，信息检索和推荐系统发展的显著特征是，深度学习将发挥越来越重要的作用。近五年来信息检索的发展得益于自然语言处理和知识图谱的突破；推荐系统则越来越受到企业和市场的重视，推荐算法在视频网站、短视频平台、社交平台发挥着更加重要的角色，市场的繁荣又反哺着相关技术的创新突破。 6. 前沿智能技术 敏感技术领域的相关规章制度、法律法规愈发完善 自lan Goodfellow在2015年的ICLR会议上以熊猫与长臂猿分类的例子对生成对抗样本方法和相关知识进行论述以后，深度学习下个神经网络算法的安全性和鲁棒性越发收到研究界的重视。2016年特斯拉手气自动驾驶致死车祸发生；同年DeepMind与OpenAI两大机构都针对人工智能安全方面的缺陷和挑战问题发表了相关论文；欧盟会议投票通过了史上最严格的数据保护法案GDPR。此后人工智能安全逐渐成为一个相对独立的新兴研究领域，各国政府和相关生态也纷纷发布相应的标准和规则，AI安全的研究开始与各项AI技术进步共同发展，相互促进。 7. 技术领域大事件 1. 人类语言技术 2015 稀有词汇翻译方法Subword Units诞生 2015 Christopher D. Manning提出全局和局部注意力机制 2016 问答数据集SQuAD 2016 Facebook AI Reasearch开发出快速文本分类器FastText 2016 Parikh等在自然语言中使用attention机制。 2016 GNMT 2016 Quirk等第一次提出应用远程监督跨句子关系抽取 2017 Vaswani等人提出了基于注意力机制的google transformer 2017 OpenNMT 2018 M.E.Peters，M.Neumann等人提出ELMo（Embeddings from Language Models） 2018 OpenAI提出可迁移至多种NLP任务的通用模型GPT，结合了Transformer和无监督预训练，开启了预训练结合transformer模型的篇章 2018 Google提出并开源了基于双向Transformer的大规模预训练语言模型BERT，该预训练模型能高效抽取文本信息并应用于各种NLP任务，该研究凭借预训练模型刷新了11项NLP任务的当前最优性能记录。 2018 问答数据集SQuAD 2.0 2018 GLUE 2018 Understanding Back-Translation at Scale 2018 NAACL2018最佳论文《Deep contextualized word representations》提出了一种进行深度语境化词表征，可对词使用的复杂特征（如句法和语义）和词使用在语言语境中的变化进行建模（即对多义词进行建模） 2019 CMU与Google的研究者提出了一种泛化的自回归预训练语言模型XLNet，融合了当时最优自回归模型Transformer-XL的思路，克服了BERT本身一些先天缺陷，并在多个自然语言任务上取得了SOTA表现。 2019 Transformer-XL 2019 SuperGLUE 2019 谷歌提出预训练模型T5，参数量达到了110亿，再次刷新Glue榜单，成为全新的NLP SOTA预训练模型 2019 百度正式发布ERNIE 2.0，16项中英文任务超越BERT、XLNet，刷新SOTA。 2019 OpenAI推出了GPT模型的升级版，号称“史上最强通用NLP模型”的GPT-2，并与2019年11越开源公开了其最完整的15亿参数模型。 2019 RoBERTa（Robustly Optimized BERT Pretraining Approach） 2019 ALBERT：A Lite BERT For Self-Supervised Learning Of Language Representations 2019 HuggingFace Transformer Library 2019 DistilBERT 2019 TinyBERT 2019 英伟达语音识别模型Jasper 2020 谷歌开源语言可解释性工具LIT 2020 CMU &amp; Google Brain提出收个与任务无关的轻量级预训练模型MobileBERT 2020 AllenNLP 1.0 2020 谷歌推出了NLP系列“芝麻街”的新成员Big Bird 2020 对话智能新高度：百度发布超大规模开放域对话生成网络PLATO-2 2020 哈佛、FAIR提出基于残差能量模型的文本生成 2020 微软发布通用文档理解预训练模型LayoutLM 2020 CLUECorpus2020：当前最大的开源中文语料库以及高质量中文预训练模型集合 2020 微软发布自然语言理解的多任务工具包MT-DNN 2020 微软发布Turing-NLG，史上最大的NLG模型 2020 HuggingFace文本Tokenizer项目 2020 OpenAI推出最新的预训练语言模型GPT-3，具有1750亿参数，堪称史上最大的AI模型 2020 覆盖40种语言：谷歌发布多语言、多任务NLP新基准XTREME 2020 ELECTRA（Efficiently Learning an Encoder that Classified Token Replacements Accurately 2020 GPT API 2. 计算机视觉 2015 在Google Brain工作的基础山个，蒙特利尔大学的eYoshua Bengio对模型加入了attention（注意力机制），使解码器在生成语言的时候，能聚焦到图像上某一块区域的重点，这种机制的加入使得该模型的能力更加强大，在Flickr8k，Flickr30k和MS COCO上取得了历史最佳成绩。 2015 He等学者提出了ResNet 2015 Google 提出了FaceNet，同样是人脸识别领域的分水岭性工作 2015 Parkhi等人提出了Deep Face Recognition 2015 Olaf等提出了U-Net 2016 Liang-Chieh Chen等人提出了DeepLab 2016 Single-shot detection（SSD）被提出 2016 FAIR实验室提出了Feature Pyramid Networks for Object Detection 2016 Scott等人开发了一种新的GAN架构，以有效的桥接文本和图像建模之间的步骤 2017 Peng Wang等人提出了VQA-machine 2017 英伟达Jun-Yan Zhu等人提出了图像迁移算法CycleGAN 2018 DeepMind带来的BigGAN创造性的将正交正则化的思想引入GAN 2019 Adobe提出新型超分辨率方法：用神经网络迁移参照图像纹理 2019 谷歌公布大型图像数据集Image V5，同时宣布即将开展开放图像挑战 2019 英伟达的Ming-yu Liu等人提出了一种少样本无监督的图像到图像转换方法 2019 李飞飞等人提出Auto-DeepLab：自动搜索图像语义分割架构 2019 谷歌发布EfficientNet新架构，全面超越传统卷积神经网络 2020 源于Facebook AI的RegNet超越EfficientNet，在GPU上实现了5倍加速 3. 机器学习 （1）通用机器学习 2015 微软ResNet图像识别准确率超越人类 2017 Facebook提出目标实力分割框架Mask R-CNN，该架构较传统方法操作更简单、更灵活 2017 研究人员首次提出基于深度学习的Transformer编码解码结构 2018 Google AI十月发布的BERT论文，提出了一种深度双向Transformer模型，刷新了11种NLP任务的最佳表现 2018 英伟达发布Video-to-Video（vid2vid）合成论文，模型合成视频效果惊人 2018 华盛顿大学的Joseph Redmon等发布目标检测模型YoloV3 2018 Google 提出并开源了基于双向Transformer的大规模预训练语言模型BERT，改与训练模型能高效抽取文本信息并应用于各种NLP任务，该研究凭借预训练模型刷新了11项NLP任务的当前最优性能记录。 2019 哈佛大学和OpenAI发现“深度二次下降”的有趣现象，增大模型的参数规模，有一个区域会观察到loss反而会上升。 2019 DeepMind提出新型端到端神经网络架构PrediNet 2019 谷歌开源缩放模型EfficientNets：ImageNet准确率创纪录，效率提升10倍 2019 Google AI和Carnegie Mellon大学发布Transformer-XL （2） 生成对抗网络（GAN） 2015 Radford等人提出DCGAN，后成为很多GAN家族网络的基础 2017 Arjovsky等人提出WGAN 2017 Min Lin针对判别器和生成器训练过程中不平衡的问题，提出了softmax GAN 2017 Kim等人提出一种能够自动学习并发现跨域关系的生成对抗网络-DiscoGAN 2017 Jun-Yan Zhu等人提出CycleGAN 2018 DeepMind提出BigGAN，用于图像合成 2019 Xingyu Gong等人提出AutoGAN，自动搜索生成对抗网络的机构。 2019 Ian Goodfello等人将注意力机制引入GAN 2019 DeepMind提出“史上最强非GAN生成器” VQ-VAE-2 2019 英伟达提出新的GAN生成器架构-StyleGAN，也被称为GAN2.0 2019 清华团队首次实现量子GAN，只用1个量子比特，准确率98.8% （3）强化学习（RL） 2016 UC Berkeley发表了深度强化学习应用在机器人技术的论文。最近几年在深度强化学习领域，与UC Berkeley合作紧密的OpenAI公司和DeepMind公司两者成为行业引领者 2016 DeepMind发布AlphaGo，成为深度强化学习应用的著名案例 2016 Voloymyr Mnih等提出并行式的深度强化学习（A3C），在多数Atari游戏学习中胜出。 2016 Barret等发表论文《Neural Architecture Search With Reinforcement Learning》将强化学习应用于神经架构搜索 2017 DeepMind公司发布AlphaZero论文，进阶版的AlphaZero算法将围棋领域扩展到国际象棋、日本象棋领域，且无需人类专业知识就能击败各自领域的世界冠军。 2018 DeepMind在Nature Neuroscience发表新论文提出了一种新型的元强化学习算法 2018 OpenAI Five在Dota2游戏上战胜了优秀的人类玩家 2019 DeepMind在《Nature》杂志发表了AlphaStar的最新研究进展，展示了AI在没有任何游戏限制的情况下已经达到星际争霸2人类对战天梯的顶级水平，在Battle.net上的排名已经超越了99.8%的活跃玩家 2020 DeepMind发布分布式强化学习框架Acme，智能体并行性加强 2020 DeepMind推出Agent57，在所有雅达利游戏上超越人类玩家 2020 深度强化学习智能体在赛车游戏中一展超人表现 （4）AutoML 2017 微软发布自动机器学习工具Neural Network Intelligence 2018 谷歌开源AdaNet，可集成学习的AutoML 2018 Texas A&amp;M大学的DATA实验室和社区贡献者开发了Auto-Keras 2018 通过在AutoML中结合进化算法执行架构搜索，谷歌开发出了当前最佳的图像分类模型AmoebaNet 2019 Xingyu Gong等人提出AutoGAN，自动搜索生成对抗网络的机构。 2019 何恺明组提出新型网络设计范式，能够充分结合手工设计和神经架构搜索的优势 （5）半监督 2015 Rasmus等提出半监督ladderNet 2016 Tim等人将生成对抗网络应用在半监督学习上 2018 难打和第四范式提出Auto-SSL使半监督学习自动化 2018 Sebastian等将半监督学习和迁移学习相结合 2018 阿里提出应用LocalizedGAN进行半监督训练 2019 李飞飞团队提出一种补全视觉信息库的半监督方法 （6）无监督 2015 Radford提出非监督式的DCGANs 2018 FAIR提出用聚类方法结合卷积网络，实现无监督端到端图像分类 2018 Facebook在EMNLP2018上的论文《Phrase-Based &amp; Neural Unsupervised Machine Translation》利用跨字嵌入（Cross Word Embedding），提升了高达11BLEU 2018 发布无监督语言翻译模型，向完全无监督词对齐再进一步 2019 DeepMind最新发布神经网络MONet无监督分割大法，为游戏而生 2019 来自UCLA与Google Brain的研究者提出了一种无监督的生成模型Self-supervised GAN，它将对抗训练与自监督学习相结合 2019 斯坦福AI研究院在论文《Uncertainty Autoencoders: Learning Compressed Representations via Variational Infomation Maximization》提出新的无监督表示学习框架 2018 DeepMind在论文《Are Labels Required for Improving Adversarial Robustness？》提出无监督对抗训练只需10%的CIFAR10标签便可获得SOTA稳健精度 2019 CapsNet的作者Hinton等联合牛津大学的研究者提出了胶囊网络的改进版本–堆栈式胶囊自编码器。这种胶囊自编码器可以无监督地学习图像中的特征，并在无监督分类任务取得最佳或接近最佳的表现。这也是胶囊网络第一次在无监督领域取得新的突破。 （7）迁移学习 2015 Junlin Hu等人提出深度度量迁移学习 2017 杨强团队提出传递迁移学习 2017 迁移学习领域的新方向，与在线、增量学习结合 2018 提出一种新的终身学习框架 2018 Cybernetics提出一个通用的迁移学习框架，对不同的domain进行不同的特征变换 2019 Chen Qu等人基于迁移学习进行深度文本匹配 2019 Prithviraj等人提出利用知识图谱迁移学习来优化深度强化学习在文字冒险类游戏方面的表现 2019 Q. Shi等人将迁移学习应用在高光谱图像分类上 2020 D.Xi等人提出基于分类注意网络的领域自适应深度情感分析 （8）元学习（Meta Learning） 2016 Jane X Wang等人提出了深度元强化学习 2017 Chelsea等人提出与模型无关的元学习（model-agnostic meta-learning，MAML） 2018 Jiatao Gu等人在2018年提出的一项研究建议引入的模型不可知元学习算法（MAML），用于低资源神经机器翻译（NMT） 2018 Chengxiang Yin等人提出ADML(ADversarial Meta-Learner)，用对抗性样本优化模型的初始化对抗性设定。 2019 Zhaojiang Lin等人提出用模型不可知原算法（MAML）进行不需人工描述的，仅需少量对话样本就可以学习对话的系统 2019 Kyle等人开发了一种无监督元学习方法，显示地优化了从少量数据学习各种任务的能力，开启了无需进行显示任务描述的元学习大门 2019 伯克利大学人工智能实验室提出在线元学习，弥补了元学习缺乏持续学习的缺陷。 2019 Facebook人工智能实验室对元学习中近似嵌套优化问题解决方案的通用模式进行了形式化。 2019 伯克利AI研究院提出新的元强化学习算法 （9）联邦学习 2016 谷歌的McMahan等人发表首篇联邦学习论文，介绍了联邦学习的基础理念 2018 英特尔与宾夕法尼亚大学生物医学图像计算与分析中心（CBICA）发表研究成果，是联邦学习在医学领域的第一个概念验证性实际应用 2019 微众银行开源其首个联邦学习工业级技术框架FATE（Federated AI Technology Enabler），FATE支持联邦学习架构体系与各种机器学习算法的安全计算，实现了基于同态加密和多方计算（MPC）的安全计算协议，能够帮助多个组织机构在符合数据安全和政府法规前提下，有效和协作地进行数据使用和联合建模 2019 Yang Q.等人发表论文介绍联邦学习基本架构，定义了三大联邦学习类别以及模型架构 2019 谷歌发表论文描述基于TensorFlow构建的全球首个产品级联邦学习系统 2020 联邦学习国际标准草案完成，正式标准预计年中出台 2020 获IEEE全票通过，首个联邦学习国际标准将正式推行 4. 数据智能技术 （1）大数据 2015 Murtaza整合了从业者和学者的定义，对大数据进行了综合描述 2016 Nada等提出了大数据，分析和决策（B-DAD）框架 2016 大数据“十三五”规划出台 2019 清华大学人工智能研究院成立大数据智能研究中心 2019 阿里巴巴内部版本Blink正式开源 2019 Databricks公司推出了自己的统一分析平台（Unified Analytics Platform），目标是使客户在一个系统里解决尽可能多的数据需求 2020 100多所高校新增人工智能专业，教育部2019年度普通高等学校本科专业备案和审批结果公布 2020 人工智能和大数据在疫情防控中大显身手 2020 MIT宣布永久下架包含种族和性别歧视标签图像的Tiny Images数据集 2020 美国白宫发布首个AI年度报告 （2）数据挖掘 2016 机器学习开始兴起，数据科学在机器学习领域已经根深蒂固，深度学习苗头出现 2017 华人学者叶艳芳博士《HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Infomation Network》获KDD 2017最佳论文 2018 《Adversarial Attacks on Neural Networks for Graph Data》获得KDD 2018最佳论文 2018 华人博士Kun Dong的《Network Density of States》获得KDD 2019 最佳论文 2020 周志华等人提出可微XGBoost算法sGBM，速度提升、准确率更胜一筹 （3）知识表征&amp;知识图谱 2015 Mitchell等人提出never-ending learning（NELL），通过不断地阅读获取知识，并不断提升学习知识的能力以及利用所学知识进行推理等逻辑思维 2016 Lisa等人发表论文《Towards a Definition of Knowledge Graphs》详述了知识图谱的定义 2017 国内哆嗦高效发起cnSchema.org项目，旨在利用社区力量维护开放域知识图谱的Schema标准 2017 清华大学开源只是表示学习平台OpenKE 2018 Shib等人提出了HyTE模型，不仅能够利用时间导向进行知识图谱图推理，还能够为那些缺失时间注释的事实预测temporal scopes。 2018 Muhao等人考虑到多语言知识图谱中具有对实体的文字性描述，提出一种基于嵌入（Embedding）的策略-多语言知识图谱嵌入（KGEM）。 2019 清华大学和华为诺亚方舟实验室联合发表了一项研究，将知识图谱中的多信息实体（informative entity）可以作为外部知识改善语言表征。该研究结合大规模语料库和知识图谱训练出增强版的语言表征模型（ERNIE），性能媲美当前最优的BERT模型 2019 Junheng Hao等人提出JOIE，联合知识图谱实例和本体概念的通用表示学习 2019 OwnThink平台在Github上开源了史上最大规模1.4亿中文知识图谱 2019 微软核心业务即将上线Project Cortex: AI知识图谱大幅提升企业效率 2020 OpenKG组织先后发布多个新冠知识图谱开放数据集 （4）信息检索 2017 汪军等人提出IRGAN首次将GAN应用到信息检索任务上 2018 卡内基美隆大学和谷歌大脑的研究者提出QANet 2018 来自微软Bing的研究院在KDD 2018上发表了论文《Recurrent Binary Embedding for GPU-Enabled Exhustive Retrieval from Billion-Scale Semantic Vectors》，提出了能够生成紧凑语义表征的“循环二分嵌入（RBE）”，这些表征可存储在GPU上，RBE使得十亿级的检索能够实时进行 2019 EMNLP最佳论文《Specializing Word Embeddings (for Parsing) by Information Bottleneck》对预训练词嵌入上使用变差信息瓶颈（Variational Infomation Bottleneck）提出了新颖应用 （5）推荐系统 2015 郭贵兵等人创建了推荐系统开源项目LibRec - 一个领先的基于jav阿德推荐系统算法库 2016 Youtube发表论文，将深度神经网络应用推荐系统中，实现了从大规模可选的推荐内容中找到最有可能的推荐结果 2016 基于推荐的短视频App抖音推出并引爆社交网络 2016 第三方检测机构易观发布了一个具有“里程碑意义”的数据：“资讯信息分发市场算法推送的内容将超过50%” 2016 Benigno等人提出了一种易于处理的方法NADE(Neural Autoregressive Distribution Estimation)，以对源数据的真是分布进行近似计算，并且可以再几个试验性数据集中产生最一流的推荐精度（与其他基于深度学习的推荐模型相比） 2017 V.Boginad等人提出了一个RNN模型结合停留时间（用户花在某个推荐条目上的时间）提升基于会话的推荐系统在电子商务数据集（Yoochoose）上的推荐准确度 2018 Tran等人发表基于用户评级历史的深度学习技术，提出了一个注意力群体推荐模型来解决群体推荐问题，第一个将Attention机制应用到群体推荐中 2018 DK Chae等人提出CFGAN: 一种基于GAN的协同过滤推荐框架，提供了一种通过GAN来进行推荐任务的新思路 2018 Criteo人工智能实验室的Stephen等提出因果嵌入的推荐方法获得RecSys 2018最佳长论文 2019 RecSys 2019最佳论文《Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches》认为现有现有DNN-based推荐算法带来的基本上都是伪提升 2019 微软开源Recommenders：企业级可扩展推荐系统实践指南 2019 SIGIR 2019最佳论文《Variance Reduction in Gradient Exploration for Online Learning to Rank》介绍了一种降低Online Learning to Rank优化算法中方差的工作 2019 Eugene等人提出了一种名为SLATE-Q的Q-Learning算法，讲一个slate的推荐序列分解成多个items，计算长期收益LTV（Long-term Value），将长期收益加入排序多目标中进行建模优化推荐。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"发展趋势","slug":"发展趋势","permalink":"https://www.notlate.net/tags/%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/"}]},{"title":"智源研究院-2020年AI进展及2021年趋势关注重点笔记","slug":"Review/01.智源人工智能2021年技术趋势记录","date":"2021-02-24T13:36:22.000Z","updated":"2021-03-18T15:04:38.598Z","comments":true,"path":"posts/6cc588ae.html","link":"","permalink":"https://www.notlate.net/posts/6cc588ae.html","excerpt":"","text":"1. Google与Facebook全新无监督表征学习算法 2020年初，Google和Facebook分别提出SimCLR和MoCo两个算法，均能够在无标注的数据上学习图像数据表征。两个算法背后的框架都是对比学习（contrastive learning）。 对比学习的核心训练信号是图片的“可区分性”。模型需要区分两个输入是来自于同一图片的不同视角，还是来自完全不同的两张图片。这个任务不需要人类标注，因此可以使用大量无标签数据进行训练。尽管Google和Facebook的两个工作对很多训练的细节问题进行了不同的处理，但是他们都表明，无监督学习模型可以接近甚至达到有监督模型的效果。 MoCo论文链接：https://arxiv.org/pdf/1911.05722.pdf MoCo代码链接：https://github.com/facebookresearch/moco SimCLR论文链接：https://arxiv.org/abs/2002.05709 SimCLR代码链接：https://github.com/google-research/simclr SimCLR Google AI Blog地址：https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html 2. 深度学习理论迎来整合与突破 深度学习是当前人工智能领域最受关注的方法，在各类监督和非监督学习任务上获得了巨大成功，不仅大幅超越了传统机器学习算法，而且在某些特定的应用场景中，已经接近甚至达到了人类的水平。然而，研究者迄今对深度学习成功背后的根本原因仍然知之甚少，对其为何表现出比传统机器学习方法更优越的性能尚未建立完整的理论解释。这一重大理论缺陷背后蕴藏着深度学习科研科学性与可重复性的危机，制约着以深度学习为代表的主流人工智能技术的发展。 人工智能的发展历史，统计学习理论的建立和完善不仅为支持向量机（SVM）、核方法（Kernel Methods）等机器学习算法。与传统统计模型和机器学习算法相比，深度神经网络具有多个显著特点，如具有多层复合结构、过参数化导致的不可识别性、优化问题高维非凸等。此外，深度神经网络的训练通常采用随机梯度下降、随机失活、批标准化等具有隐士正则化效果的策略。这些与传统模型和算法截然不同的特点，使得对深度神经网络进行严格的理论分析异常困难，需要从数学、统计和计算的不同角度，以及表示能力、泛化能力、算法收敛性和稳定性等多个侧面，对现有的统计学习理论进行再思考、整合与重构。 近几年，深度学习理论研究的主要进展集中在浅层无穷宽网络的理论分析以及对**“双下降”**现象的理论解释两方面。对于两层无穷宽网络的，可以用近似网络参数分布的平均场理论和刻画参数演化动力学的神经正切核理论进行分析。这些理论较好地描述了浅层网络的性质，但对理解宽度受限的深度网络帮助不大，无法说明深度网络相对传统核方法的优越性。另一研究热点“双下降”现象则表明，深度网络等过参数化模型或许并不完全遵循经典的“偏差-方差权衡”，为理解正则化在深度学习泛化理论中扮演的关键角色提供了新的视角。 深度学习理论能否取得根本性突破，关键在于发现和建立联系各个理论的桥梁，融合与发展来自基础学、概率统计、数值计算等各个数学分支的理论工具。这些理论创新将有望提升对深度学习的理解，从浅层网络和局部性质向深度网络和全局性质不断深化和拓展，并整合与完善统计学、机器学习和数据科学的理论框架，为下一代人工智能提供强大的理论指引和方法学驱动。 3. 机器学习向分布式隐私保护方向演进 近年来，数据隐私保护逐渐成为各界关注的热点问题。在保护数据隐私的限制下，多机构联合进行机器学习模型训练需要密码学、分布式系统以及人工智能等多学科交叉的指导。当前热门的联邦学习能够解决一部分隐私保护的问题，但在计算性能、高可用性、可编程性、可证明安全性等领域，目前仍然存在诸多技术及工程难题，需要研究人员去攻破。 分布式隐私保护机器学习系统的计算性能非常重要，而常见系统的计算性能主要受限于（同态）密码学计算，包括加密、解密、密码加/乘等。而机器学习算法的特点是在张量（tensor）上操作，批次加密、并行加密会带来较大的性能提升；同时新硬件（FPGA、GPU），新（同态）加密算法的引入，也会带来极大的性能提升。 分布式隐私保护机器学习主要面向地理分布场景，在公网不稳定连接环境下如何进行高可用的机器学习模型训练，需要算法设计和系统设计两方面努力。在算法设计中，需要引入异步/半异步更新机制；在系统设计中需要探索更新备份与重发机制，保证高可用模型训练与预测。 4. 基于因果学习的信息检索模型与系统成为重要发展方向 信息检索领域目前亟待解决从海量检索数据中有效挖掘变量间因果关系的问题。探索兼具数据关联分析能力及因果推断能力的智能信息检索模型，实现可推理、可迁移、可解释的智能信息检索系统，已成为智能信息检索领域的重要发展方向。 因果学习通过反事实世界的模拟，推断变量间的因果关系，解决样本的有偏问题及结果的可解释性问题。一方面，可通过在抽象层面上对世界进行模拟来想象潜在行为会导致的结果；另一方面，可通过模拟当前世界存在的事件来推断出原因。同时，它也需要解决面向信息检索的预训练问题，帮助信息检索系统间的高效迁移能力。 对于用户数据纠偏、排序公平性、离线模型评价等信息检索中的挑战性问题，反事实框架下的因果学习将提供可靠的解决方案。这需要从因果关系的定义和表达、处理变量的设置与选择、反事实方法的建模与求解等方面深入探究。同时需要汲取经济学、统计学、公共管理等众多领域对因果推断的研究成果，并结合强化学习、无偏学习、持续学习等机器学习前沿研究的最新进展进行突破。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"发展趋势","slug":"发展趋势","permalink":"https://www.notlate.net/tags/%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/"}]},{"title":"GNN笔记01-基础原理与应用","slug":"PaperNotes/GNN/GNN学习路线","date":"2021-02-04T06:30:40.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/a08c2ad5.html","link":"","permalink":"https://www.notlate.net/posts/a08c2ad5.html","excerpt":"","text":"1. GNN技术发展时间轴 graph LR A[基于RNN/CNN技术演进]-.-A1>Graph Neural Network-2005]----->A2>Gated Graph Neural Network-2016]--->D B[基于图谱理论发展]-...-B1>Spectral Networks-2014]--->B2>ChebyNet-2016]-->B3>Graph Conv Network-2017]-->D C[基于图形学基础发展]-....-C1>Geodesic CNN-2015]-->C2>Anisotropic CNN/Diffusion CNN-2016]--->D D>Neural Message Passing-2017]-->E>FeaStNet/GraphSAGE-2018]-->F>至今] 1. GNN技术与传统Graph理论研究的融合 Weisfeiler-Lehman Test Longest/Shortest cycle diameter Coloring problem 2. 现有GNN框架的花式改进 Attention Strategy Heterogeneous（Node，edge） Hyper-Graph Dynamic Graph 3. GNN在各个领域的花式应用 Social Network Biology/Chemistry Image + NLP Embedding Representation EDA Program structure Relationship Infer 其他包含关系信息的应用 2. GNN基础原理 1. Deep learning based GNN 2. Spatial-based GNN 论文：Inductive representation learning on large graphs Message Passing Phase Message function Mt:mvt+1=∑u∈N(v)Mt(hvt,hut,euv)M_t: m^{t+1}_{v}=\\sum_{u\\in N(v)}M_t(h^{t}_v,h^{t}_u,e_{uv})Mt​:mvt+1​=∑u∈N(v)​Mt​(hvt​,hut​,euv​) MtM_tMt​: permutation invariant(sum, mean) euve_{uv}euv​: 可以拓展为复杂的edge update function Vertex update function Ut:hvt+1=Ut(hvt,mvt+1)U_t: h^{t+1}_v = U_t(h^{t}_v, m^{t+1}_v)Ut​:hvt+1​=Ut​(hvt​,mvt+1​) UtU_tUt​: 也称为集合方程，可省略或复杂化，eg: concat Message passing是迭代过程，迭代次数越多，则覆盖的graph局部信息越多，但是由于mesage function的定义，面临over smoothing的问题 为了解决over smoothing，研究出了多种edge drop, sampling, pooling等技术； GNN迭代次数的增加，计算量、内存成倍增长，因此解决超大graph是很有难度的。 An readout function R to make predictions on nodes or whole graph y^=R(hvT∣v∈G)\\hat{y}=R({h^{T}_v | v\\in G})y^​=R(hvT​∣v∈G) 对node/edge计算Embedding时，可省略readout function 3. Spectral-based GNN 论文：Semi-Supervised Classification with Graph Convolutional Networks Use spectral graph theory to design localized convolutional filters on graph. Generalize CNN filter concepts from low-dimensional regular grids to high-dimensional irregular domains. Localized convolutional filters graph LR A[Spectral Network]-->B[ChebNet]-->C[GCN] Spectral Network: gθ(L)=∑k=0K−1θkLk,L=IN−D−1/2AD−1/2=UΛUTg_\\theta(L)=\\sum^{K-1}_{k=0} \\theta_k L^k, L=I_N-D^{-1/2} A D^{-1/2} = U \\Lambda U^Tgθ​(L)=∑k=0K−1​θk​Lk,L=IN​−D−1/2AD−1/2=UΛUT ChebNet: gθ(L)=∑k=0K−1θkTk(L~)g_\\theta(L)=\\sum^{K-1}_{k=0} \\theta_k T_k (\\widetilde{L})gθ​(L)=∑k=0K−1​θk​Tk​(L) GCN: gθ(A)=θ(IN+D−1/2AD−1/2),K=2,λmax=2g_\\theta(A)=\\theta(I_N+D^{-1/2} A D^{-1/2}), K=2,\\lambda_{max}=2gθ​(A)=θ(IN​+D−1/2AD−1/2),K=2,λmax​=2 Layer Propagation model y=gθ(L)x=∑k=0K−1θkTk(L~)xy=g_\\theta(L) x=\\sum^{K-1}_{k=0} \\theta_k T_k (\\widetilde{L}) xy=gθ​(L)x=∑k=0K−1​θk​Tk​(L)x Drawback Spectral filter coefficients与对应的的graph是相互依赖的，无法在graph间迁移 无法保证spectral domain的local feature在spatial domain是一致的：gθ∗x=Ugθ(Λ)UTxg_\\theta * x = U g_\\theta (\\Lambda) U^{T} xgθ​∗x=Ugθ​(Λ)UTx 3. GNN 理论探索 Weisfeiler-Lehman Test(graph isomorphic test): [graph A,B permutation function π，使得Aπ=πB] 论文：Invariant and Equivariant Graph Networks 1 dimensional Weisfeiler-Lehman graph (GIN GNN) K dimensional Weisfeiler-Lehman graph (3WL GNN - rank2 tensor) Weisfeiler-Lehman GNN随order增加，计算量成指数增加O(n)−&gt;O(n2)O(n)-&gt;O(n^2)O(n)−&gt;O(n2)，难以batch training 多项式时间内无法求解（Babai算法可以做到quasi-polynomial time） 研究图的性质 论文：Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks Graph function: trace, diagonal, maximal singular vector 研究图结构学习的Invariance &amp; equivariance 论文：Provably Powerful Graph Networks CNNs: translation invariance(image label is invariance to a translation of the input image) Group Equivariant Neural Network(3D shape and spherical functions: rotation, reflection) 研究graph isomorphism testing与invariant/equivariance function approximation的统一 论文：On the equivalence between graph isomorphism testing and function approximation with GNNs 将graph特性的基础探究与Deep learning based GNN model技术结果 提高计算效率和学习能力 4. GNN应用 Relationship Prediction 论文：Learning Human-Object Interactions by Graph Parsing Neural Networks 给定一组人和物的候选目标，可能存在多种人-物，人-人，物-物的关系组合； 关系类型：粗略的可分为空间关系（状态）、时序关系（动态）、动作关系、社会关系等； 包含技术：目标检测/分割，自然语言处理，图像标注：行为预测/行动，图像-&gt;检测-&gt;关系预测-&gt;分类/自然语言描述/行动； 可根据实际应用调整GNN的模型设计：着重Link relation(edge information)的表达 应用场景：社会关系分类（相册分类）；预测人的行动（行人检测）；预测物体行动（车辆检测） DataSet: HICO-DET, V-COCO, MS-COCO, bAbl, CLEVR, PISC(people in Social Context), PIPA(people in Photo Album Relation) Netlist Feature Representation for Chip Placement GNN模型已经广泛的应用在抓取电路网表的内在结构特征的应用中。 论文：Chip Placement with Deep Reinforcement Learning 结合宏模块的物理特征参数，相关体系结构的特征参数，对网表功能模块的划分、数据流关系建立特征表达； 作为后续布局算法的输入信息，学习求解不同网表的物理布局方式； 泛化：实现不同配置之间的物理布局的高效迁移，降低布局实现的时间代价； 近年在DAC，ICCAD等EDA等会中，GNN已经广泛的出现在各种论文，workshop中； Program Structure Representation Designed for capturing the syntax and semantic information from the program abstract syntax tree and the control and data flow graph. 论文：Deep Program Structure Modeling Through Multi-Relational Graph-based Learning 论文：Program Graphs for Machine Learning Multi level program structure建模: heterogeneous device mapping, GPU thread coarsening, loop vectorization, code vulnerability detection Multi-relationship: data flow, control flow, call-flow and etc… Heterogeneous Mapping, Thread Coarsening, Loop Vectorization, Vulnerability Detection, Dead Code Elimination, Global Code Motion, etc… 基于Machine Learning的编译器优化 背景：给定计算图的情况下，需采用人工/机器学习的方式做出最优决策，编译生成高效代码，这个过程可以转化为图优化问题（graph optimization problems），现有的方法依赖硬件在环的方式，对编译结果评估。当计算图越大，评估约耗时。因此，学习一种可泛化的local advantage functions用于编译器优化，引导在未知计算图进行搜索最优编译选项； 思路： learn single node advantage functions from existing graphs and use the learned model to guide exploration on new graphs for improving sample efficiency single node advantage functions：（GNN model， GraphSAGE） f(yv=k;y⃗v−,G,X)=cost(yv=k,y⃗v−;G,X)−cost(yv=1,y⃗v−;G,X)f(y_v = k; \\vec{y}_{v^{-}},G,X)=cost(y_v=k,\\vec{y}_{v^{-}};G,X) - cost(y_v=1,\\vec{y}_{v^{-}};G,X)f(yv​=k;y​v−​,G,X)=cost(yv​=k,y​v−​;G,X)−cost(yv​=1,y​v−​;G,X) 实验：operation fusion task (Tensorflow-XLA compiler): binary decision(eg. fuse) 数据集：Gibbs sampling on given computation graph p(yv=k∣yv−^)∝e−cost(yv=k∣yv−^)τp(y_v = k | \\widehat{ y_{v^{-}} }) \\propto \\frac{e^{-cost(y_v = k | \\widehat{ y_{v^{-}} })}}{\\tau}p(yv​=k∣yv−​​)∝τe−cost(yv​=k∣yv−​​)​ 探索： 对于未知computation graph，无法做到精确的逐节点的性能cost估计，因此只能作为牵引； 在搜索过程中使用local advantage function的方式是很灵活的，论文中只是使用预测结果对待处理的节点做重要性排序，给出指导（使用的非常保守，主要是估计结果不一定准确）","categories":[{"name":"论文笔记","slug":"论文笔记","permalink":"https://www.notlate.net/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"GNN","slug":"GNN","permalink":"https://www.notlate.net/tags/GNN/"}]},{"title":"论文笔记001：Mixture of Experts","slug":"PaperNotes/RecSys/推荐系统论文笔记001-Mixture of Experts","date":"2021-01-05T16:24:40.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/405685a.html","link":"","permalink":"https://www.notlate.net/posts/405685a.html","excerpt":"","text":"","categories":[{"name":"论文笔记","slug":"论文笔记","permalink":"https://www.notlate.net/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"MMoE","slug":"MMoE","permalink":"https://www.notlate.net/tags/MMoE/"}]},{"title":"深度学习推荐系统-笔记13：探究业界主流的推荐系统解决方案","slug":"RecSys/6.前沿拓展/[深度学习推荐系统-笔记13]前沿拓展，业界经典的推荐实践","date":"2021-01-05T15:24:40.000Z","updated":"2021-03-18T15:04:38.598Z","comments":true,"path":"posts/2c4e9dc8.html","link":"","permalink":"https://www.notlate.net/posts/2c4e9dc8.html","excerpt":"","text":"1. Facebook的深度学习推荐系统 2014年，Facebook发表了广告推荐系统论文GBDT+LR。严格意义上讲，这并不属于深度学习的范畴，但在当时，这种方式进行特征的自动组合和筛选，开启了特征工程模型化、自动化的新阶段。并且其在2014年就采用的在线学习、在线数据整合、负样本降采样等技术至今仍具有极强的工程意义。 2019年Facebook又发布了最新的深度学习模型DLRM（Deep Learning Recommender Model），采用经典的深度学习模型架构，基于CPU+GPU平台完成模型训练，是业界经典的深度学习推荐系统尝试。 （1）应用 Facebook的广告推荐系统是一个标准的CTR预估场景，输入用户（User）、广告（Ad）、上下文（Context）相关特征，预测CTR，然后再使用CTR进行广告排序和推荐。 由于Facebook其他模块需要利用CTR计算广告出价、投资回报率（ROI）等预估值，因此CTR模型的预估值应该是一个具有物理意义的精准的CTR，而不是仅仅输出一个广告排序。同时Facebook也介绍了输出值与实际值的CTR矫正的方法。 （2）GBDT+LR预估模型 参考本系列的相关笔记-《08-传统推荐算法发展汇总》 ①实时数据流架构 为了实现模型的准实时训练和特征的准实时更新，Facebook在上述论文中提出了其基于Scribe（由Facebook开发并开源的日志收集系统）构建的实时数据流架构，被称为Online Data Joiner（在线数据整合）模块，该模块与其他模块的关系图如下： Online Data Joiner模块最重要的作用是：准实时的把来自不同数据流的数据整合起来，形成训练样本，并最终与点击数据（标签）进行整合，形成完整的有标签样本。在整个过程中，需要注意的点如下： Waiting window（数据等待窗口）的设定 waiting window就是在物品曝光后，要等多久才能够判定这个曝光行为产生了点击。如果窗口过大，则数据实时性受影响；如果过小，则会有一部分点击数据统计不全。这个设置是需要工程不断调优找到最佳的，少量点击数据丢失也是不可避免的。 分布式架构与全局统一的Action ID（行为ID） 为了实现分布式架构下曝光记录与点击记录的整合，Facebook除了为每个行为建立全局统一的Request ID，还建立了HashQueue用于缓存曝光记录。在缓存队列中的曝光记录，如果在Waiting Window过期后还没有匹配到点击记录，就会被作为负样本。 数据流保护机制 论文中，Facebook专门提到了Online Data Joiner的保护机制，因为该模块若因为某些异常而失效，比如点击数据流由于Action ID的bug无法与曝光数据流进行正确联结，那么所有的样本都会认定为负样本。同时，由于模型采用实时训练和服务，所以模型准确度会很快受到影响，后果非常严重。保护机制则是专门设立异常检测机制，一旦发现实时样本流的数据分布发生变化，会立即切断在线学习的过程，防止线上问题发生。 ② 降采样 为了控制数据规模，降低训练开销，Facebook时间了两种降采样方法：Uniform subsampling（均匀采样）和Negative down sampling（负样本降采样）。均匀采样是对所有样本进行无差别的随机抽样，其试验了1%、10%、50%、100%四个采样频率，从下图可以看出，当采样频率为10%的时候，相比全量数据训练的模型，模型损失仅仅上升了1%，因此是一个比较合适的平衡工程和理论最优的选择。 负采样方法则是保留全量正样本，对负样本进行降采样。除了提高训练效率，负样本还直接解决了正负样本不平衡的问题，Facebook经验性的选择了从0.0001到0.1的福采样频率，效果图如下： 可以看到当负采样频率为0.025时，模型损失最小，甚至小于0.1的频率，论文中未给出解释。在实际应用中，Facebook采用了0.025的负采样频率。 ③ 模型矫正 负采样带来的问题是CTR预估值得漂移：假设真实CTR是0.1%，进行0.01的负采样之后，CTR会攀升到10%左右。前文也提到了，Facebook的业务背景要求其计算出的CTR需要是具有物理意义的精准的CTR，为了进行准确的竞价和ROI估计，进行负采样后的CTR需要矫正，公式如下： q=pp+(1−p)/wq = \\frac{p}{p + (1-p) / w} q=p+(1−p)/wp​ 其中qqq是校正后的CTR，ppp是模型预估的CTR，www是负采样频率。 ④ 工程实践 特征工程模型化 模型复杂性和实效性的权衡 用数据验证Idea （3）DLRM预估模型 DLRM是一次彻底的应用深度学习方法的尝试。 ① 模型结构 如果所示，各层作用如下： 特征工程：所有的特征分为两类：一类是将类别、id类特征用one-hot编码生成稀疏特征（sparse features）；另外一类是数值型的连续特征（dense features）。 Embedding层：把稀疏特征转换成nnn维Embedding向量；连续性特征连接成一个特征向量之后输入黄色的MLP，被转换成同样nnn维向量。 神经网络层(NNs)：Embedding层上边的三角形，nnn维的Embedding向量有可能进一步通过神经网络层处理，但是这个过程是选择性的，根据实际场景评估决定的。 特征交互层(Interactions)：把之前的Embedding向量两两做内积操作，再和连续型特征对应的特征向量连接，输入后续MLP。目的是让特征之间做充分的交叉组合之后，再进入MLP做最终的目标拟合。 目标拟合层：最上层的三角形，表示一个全连接的MLP，最后一层使用softmax函数输出点击率。 综上分析，模型结构并不复杂，也没有注意力机制、序列模型、强化学习等思想，是一个非常标准的工业界深度学习推荐模型。简单的模型结构在海量的数据背景下也可以发挥出不俗的作用。 ② 并行训练方法 由于Facebook的数据量非常大，模型训练必然要并行化。DLRM融合使用了模型并行和数据并行的方案： 对Embedding部分采用了模型并行，可以减轻内存瓶颈问题。在一个设备上仅保存一部分Embedding层参数，每个设备进行并行mini batch梯度更新，仅更新自己节点上的部分Embedding参数。 对MLP部分（两个NNs层）采用了数据并行，可以并行前向和反向传播。每个设备已有全部模型参数，利用部分数据计算梯度，再利用全量规约（AllReduce）的方法汇总所有梯度进行参数更新。 （4）总结 无论是GBDT+LR还是DLRM，Facebook的技术选择总是偏工业化，简单直接，以解决问题为主。虽然从学术角度看模型创新性不足，但从业者可以从中借鉴到非常多的工程实践经验。如果公司刚开始从传统机器学习模型转到深度学习模型，则完全可以采用DLRM作为标准实现。 2. Airbnb基于Embedding的实时搜索推荐系统 2018年KDD的最佳论文《Real-time Personalization using Embeddings for Search Ranking at Airbnb》介绍了Airbnb把Embedding技术应用于搜索推荐的实践。 Embedding技术两大作用：1）将稀疏特征转换成稠密特征；2）能够对物品语义特征进行编码，通过计算相似度进行相似物品搜索。 （1）应用场景 Airbnb是一家短租网站，提供房主和短租客的中介平台。租客输入地点、价位、关键词等信息后，Airbnb给出房源的搜索推荐列表。对于列表，有以下集中交互方式： 租客点击（Click）房源 租客立即预定（Instant Book）房源 租客发出预定请求（Booking Request），房主有可能拒绝（Reject）、同意（Accept）或者不响应（No Response）租客的预定请求。 针对以上交互数据，Airbnb构建了实时搜索排序模型。为了捕捉用户兴趣，Airbnb没有将用户历史数据中的点击房源id序列（clicked listing ids）或者预定房源id序列（booked listing ids）直接输入排序模型，而是先对租客和房源分别进行Embedding，利用Embedding结果构建出诸多特征，作为排序模型的输入。 Airbnb分别对用户的“短期”和“长期”兴趣进行编码，生成了两种Embedding。短期兴趣目的是进行房源的相似推荐和session（会话）内的实时个性化推荐。长期兴趣目的是在最终的推荐结果中照顾到用户之前的预定偏好，推荐更容易被用户预订的个性化房源。 （2）基于短期兴趣的房源Embedding方法 Airbnb利用session内点击数据对房源进行Embedding，捕捉用户在一次搜索过程中的短期兴趣。那session内点击数据怎么获取呢？就是用户在一次搜索过程中点击的房源序列，这个序列需要满足2个条件：1）只有在房源详情页停留超过30秒才算序列中的一个数据点；2）如果用户超过30分钟没有动作，那么序列结束。 获取到由点击房源组成的序列后，就可以用Item2vec方法，把这些序列当成“句子”一样的样本，进行Embedding训练。Airbnb使用的是Word2vec的跳词（Skip-gram）模型，修改目标函数拟合Airbnb的业务目标。修改点如下： ① 在原始目标函数基础上，针对Airbnb的业务特点，希望把预定信息引入Embedding，目的是使得Airbnb的搜索列表和相似列表更倾向于推荐之前预定成功的Session中的房源。基于此想法，Airbnb把会话点击序列分成两类：1）最终产生预定行为的称为预定会话，会话只有最后一个房源是被预订房源；2）否则称为探索性会话。为了把预定行为引入目标函数，强假设这个被预定房源与滑动窗口的中心房源相关。 ② 为了更好的发现同一市场内部房源的差异性，Airbnb加入了另外一组负样本，在同一市场的房源集合中随机抽样，获得一组新的负样本，并加入目标函数。 训练过程：使用负采样方法。 冷启动方案：如果有新的房源确实Embedding向量，就找附近的3个同样类型、相似价格的房源向量进行平均得到。 （3）基于长期兴趣的用户Embedding和房源Embedding 基于短期兴趣的Embedding没有包含用户的长期兴趣信息。为了解决这种问题，Airbnb使用了预定会话序列。假设用户jjj在过去1年依次预定过5个房源，那么预定会话就是sj=(lj1,lj2,lj3,lj4,lj5)s_j = (l_{j1}, l_{j2}, l_{j3}, l_{j4}, l_{j5})sj​=(lj1​,lj2​,lj3​,lj4​,lj5​)。但是却不能和点击会话一样直接进行训练，因为这类预定数据是非常稀疏的，原因如下： ① 预定行为的总体数量本身远远小于点击行为 ② 单一用户的预定行为很少，大量用户在过去1年甚至只预定过1个房源，导致很多预定会话序列长度仅仅为1 ③ 大部分房源被预定的次数也很少，Word2vec训练需要物品至少出现5~10次，但是大量房源被预定的次数少于5词，根本无法训练出有效的Embedding。 Airbnb的解决方案是：基于某些属性规则做相似用户和相似房源的聚合。 首先定义用户和房源的各种属性，比如用户属性有设备类型、是否填写简介、是否有头像照片、历史订阅次数等；房源属性有国家、类型、价格等。根据这些属性就可以用聚合数据的方式生成新的预定序列（booking session sequence）。直接用用户属性代替原来的user id，生成一个由所有该用户属性预定历史组成的预定序列。 得到用户属性的预定序列后，如何得到用户属性和房源属性的Embedding呢？为了让user type Embedding和listing type Embedding在同一个向量空间中生成，Airbnb采用的方式如下： 对于某一user id按时间排序的booking session(l1,l2,...,lml_1, l_2,...,l_ml1​,l2​,...,lm​)，用（user_type, listing_type）组成的元组替换原来的listing item，原序列就变成了((utype1,ltype1),(utype2,ltype2),...,(utypeM,ltypeM))((u_{type1}, l_{type1}), (u_{type2}, l_{type2}), ..., (u_{typeM}, l_{typeM}))((utype1​,ltype1​),(utype2​,ltype2​),...,(utypeM​,ltypeM​))，这里的ltype1l_{type1}ltype1​指的是房源l1l_1l1​对应的房源属性，utype1u_{type1}utype1​指的是该用户在预订房源l1l_1l1​时的用户属性，用于用户的user_type会随着时间变化，所以utype1u_{type1}utype1​和utype2u_{type2}utype2​也不一定相同。 那如何训练呢？由于序列是元组组成的，如何确定“中心词（central item）”呢？Airbnb在论文中没有描述技术细节，《深度学习推荐系统》一书中给出了最接近论文原文的训练方式。 Airbnb分别给出了训练user type Embedding和listing type Embedding时，滑动窗口内“中心词”分别是user type(utu_tut​)和listing type(ltl_tlt​)时的目标函数如下： 其中DbookD_{book}Dbook​是中心词附近的用户属性和房源属性的集合，所以在训练过程中，用户属性和房源属性完全是被同等对待的，这两个目标函数也是完全一样的。可以认为训练时就是把元组扁平化了，把用户属性和房源属性当做完全相同的词去训练Embedding，这种方式自然保证了二者在同一个向量空间中生成。 （4）搜索词的Embedding Airbnb还在其搜索推荐系统中对搜索词进行了Embedding，方法与用户Embedding类似，把搜索词和房源置于同一向量空间进行Embedding，再通过二者之间的余弦相似度进行排序。 （5）实时搜索排序模型和特征工程 Airbnb没有直接把Embedding相似度排名当做搜索结果，而是基于Embedding得到了不同的用户房源相关特征（user-listing pair feature），然后输入搜索排序模型，得到最终的排序结果。Airbnb基于Embedding生成了如下特征： 最后一项UserTypeListingTypeSim指的是用户属性和房源属性的相似度，这一项特征是基于长期兴趣Embedding计算得到的，其余的都是短期兴趣Embedding。 实时性则主要体现在EmbClickSim（最近点击房源的相似度）和EmbLastLongClickSim（最后点击房源的相似度）这两项特征，使得在用户点击浏览的过程中就可以得到实时反馈，搜索结果也可以实时地根据用户的点击行为进行改变。 Airbnb采用的搜索排序模型是一个支持Pairwise Lambda Rank的GBDT模型。 3. YouTube深度学习视频推荐系统 2016年，YouTube发表了深度学习推荐系统论文《Deep Neural Networks for YouTube Recommendations》，虽然内容不算新颖，但是其方案却成为推荐系统业界最经典的深度学习架构之一。无论是经典的架构，还是技术细节等工程实践经验，都是收益颇多，值得多读几遍。 （1）应用场景 ① YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显； ② YouTube 的视频基数巨大，用户难以发现喜欢的内容 由于以上特点，YouTube非常适合使用深度学习推荐系统。 （2）推荐系统整体架构 为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 + 排序层的两级推荐系统架构。 第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选。把候选视频集合从百万降到几百量级，这就相当于经典架构中的召回层。 第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典架构中的排序层。 ① 候选集生成模型 最底层是输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。YouTube利用用户的观看序列和搜索序列，采用Item2vec方法预训练生成Embedding。当然也完全可以用 Embedding 跟模型在一起 End2End 训练的方式来训练模型。 除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。对于样本年龄这个特征，YouTube 不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型，这个操作是为了挖掘特性的非线性信息。这种对连续型特征的处理方式不仅限于平方，其他诸如开方、Log、指数等操作都可以用于挖掘特性的非线性信息。 所有特征使用concat连接起来后送入MLP进行训练，最后输出层使用softmax函数，预测的是用户会点击哪个视频。假设YouTube上有100万个视频，那么sofmax就有100万个输出，用于表示用户对所有视频的点击概率，因此模型的最终输出就是一个在所有候选视频上的概率分布。目的是为了更好、更快地进行线上服务。接下来看下线上服务是怎么做的。 ② 候选集生成模型的线上服务方法 候选集生成模型架构图左上角的模型服务方法与实际的训练方法完全不同。在线上服务过程中，YouTube 并没有直接采用训练时的模型进行预测，而是采用了一种最近邻搜索的方法。为什么呢？ 因为候选集生成模型可以得到全量的用户和视频的Embedding，服务时通过Embedding最近邻搜索的方法可以大大提高在线服务效率。因为只需要把所有的用户和视频Embedding存到特征数据库就行了。 视频Embedding是怎么得到的呢？架构图中从softmax向模型服务模块画了个箭头，用于代表视频 Embedding 向量的生成。此处的softmax层指的是激活函数是softmax的全连接层。全连接层的权重是m x n的矩阵，其中m指的是最后一层带ReLU激活函数的全连接层的权重维度，n指的是分类（YouTube所有视频）的总数。因此所有视频 Embedding就是这个m x n维矩阵的各列向量。这个思想参考的是Word2vec的词向量生成方法。 用户Embedding又是怎么得到的呢？因为输入的特征向量都是与用户相关的特征，一个物品和场景特征都没有，所以最后一层带ReLU层的输出向量就可以当作该用户u的Embedding向量。待模型训练完成后，逐个输入所有用户的特征向量，就得到了所有用户的Embedding向量，之后就可以预存到线上的特征数据库中了。 在预测某用户的视频候选集时，先从特征数据库中查询用户的 Embedding 向量，再在视频 Embedding 向量空间中，利用局部敏感哈希等方法搜索该用户 Embedding 向量的 K 近邻，这样就可以快速得到 K 个候选视频集合。这就是整个候选集生成模型的训练原理和服务过程。 ③ 排序模型 通过候选集生成模型，已经得到了几百个候选视频的集合了，下一步就是利用排序模型进行精排序。 排序模型的网络结构与候选集生成模型在结构上看起来没有太大区别，它们都遵循 Embedding+MLP 的模型架构。但是观察其细节，特别是输入层和输出层的部分，它们跟候选集生成模型区别很大。先看下输入特征： 序号 原文特征名称 含义 1 impression video ID embedding 当前候选视频的 Embedding 2 watched video IDs average embedding 用户观看过的最后 N 个视频 Embedding 的平均值 3 language embedding 用户语言的 Embedding 和当前候选视频语言的 Embedding 4 time since last watch 表示用户上次观看同频道视频距今的时间 5 #previous impressions 该视频已经被曝光给该用户的次数 前3个特征的含义很好理解，重点看下第4和第5个。 第 4 个特征是用户观看同类视频的间隔时间。如果从用户的角度出发，假如某用户刚看过“DOTA比赛经典回顾”这个频道的视频，那他很大概率会继续看这个频道的其他视频，该特征就可以很好地捕捉到这一用户行为。 第 5 个特征说的是这个视频已经曝光给该用户的次数。如果一个视频已经曝光给了用户 10 次，都没有被点击，那大概率用户对这个视频不感兴趣。所以previous impressions这个特征的引入就可以很好地捕捉到用户这样的行为习惯，避免让同一个视频对同一用户进行持续的无效曝光，尽量增加用户看到新视频的可能性。 把这 5 类特征连接起来之后，再经过MLP进行充分的特征交叉，最后经过Weighted LR输出。需要注意：排序模型的输出层与召回模型有所不同。主要是：召回模型输出层的激活函数是softmax，预测的是用户“会点击哪个模型”；而排序模型的输出层激活函数是weighted logistic regression（加权逻辑回归），预测的是用户“要不要点击当前视频”。 其实根本原因是YouTube想要更精确地预测用户的观看时长，因为观看时长才是 YouTube 最看中的商业指标，而使用加权逻辑回归可以实现这样的目标。就是在加权逻辑回归的训练中，为每个样本设置一个权重，代表这个样本的重要程度。为了预估观看时长，可以把正样本的权重设置为用户观看这个视频的时长。换个角度考虑就是观看时长长的样本被预测的为正样本的概率更高，这个概率与观看时长成正比，所以得到的预测结果可以认为是观看时长。 ④ 排序模型服务方法 服务时采用的函数形式是：eWx+be^{Wx + b}eWx+b，和训练的加权逻辑回归又不一样，这也能表示用户观看时长吗？逻辑回归如果还没有清晰的认识，可以看下《Logistic回归中的Logit函数和sigmoid函数》。 线性回归解决的是回归问题，也就是用直线拟合数据，因此其值域是(−∞，+∞)(-\\infty，+\\infty)(−∞，+∞)，形式如下： y=WT⋅xy = \\boldsymbol{W}^T \\cdot x y=WT⋅x 逻辑回归应用于分类问题时，因变量y的值只有0和1，连续的线性模型无法拟合，因此需要选择一个比较合适的激活函数：能够把值域(−∞，+∞)(-\\infty，+\\infty)(−∞，+∞)连续且单调地映射到(0,1)(0, 1)(0,1)。先来看个logit⁡\\operatorname{logit}logit函数的公式和图像： logit⁡(p)=log⁡(p1−p){\\operatorname {logit} (p)=\\log \\left({\\frac {p}{1-p}}\\right)} logit(p)=log(1−pp​) 其中p1−p\\frac{p}{1-p}1−pp​就是事件发生的几率Odds⁡(p)\\operatorname{Odds}(p)Odds(p)，也就是一个事件发生的概率（ppp）与不发生的概率（1−p1-p1−p）比值。可以看到logit⁡\\operatorname{logit}logit函数可以把值域(0,1)(0, 1)(0,1)连续且单调地映射到(−∞，+∞)(-\\infty，+\\infty)(−∞，+∞)。那么令logit⁡\\operatorname{logit}logit函数等于线性回归函数，近似得到如下等式（把log⁡\\loglog看成是ln⁡\\lnln）： log⁡(p1−p)=WT⋅x=&gt;p1−p=eWT⋅x=&gt;p=11+e−WT⋅x=&gt;p=sigmoid⁡(x)\\qquad \\qquad \\log \\left({\\frac {p}{1-p}}\\right) = \\boldsymbol{W}^T \\cdot x \\\\ =&gt; \\frac{p}{1-p} = e^{\\boldsymbol{W}^T \\cdot x} \\\\ =&gt; p = \\frac{1}{1 + e^{- \\boldsymbol{W}^T \\cdot x}} \\\\ =&gt; p = \\operatorname {sigmoid} (x) log(1−pp​)=WT⋅x=&gt;1−pp​=eWT⋅x=&gt;p=1+e−WT⋅x1​=&gt;p=sigmoid(x) 这就推导出了逻辑回归的函数式。再回过头来看下YouTube使用的服务函数式，可以发现计算的就是Odds⁡(p)\\operatorname{Odds}(p)Odds(p)，也就是某个视频被点击的几率。但是训练的时候，目标不是预测视频观看时长吗？这要从YouTube的训练方式寻找答案：加权逻辑回归在训练时，需要给样本指定权重wiw_iwi​，对于正样本来说，权重设置为观看时长，对于负样本来说，权重指定为1。因此样本发生的几率会变成原来的wiw_iwi​倍： Odds⁡(i)=wi p1−wi p\\operatorname{Odds}(i) = \\frac{w_i \\ p}{1 - w_i \\ p} Odds(i)=1−wi​ pwi​ p​ 由于在推荐场景中，用户点击率ppp往往是一个很小的值，因此上式通过不严谨的简化得到下式： Odds⁡(i)=wi p1−wi p≈wi p=E(wi)\\operatorname{Odds}(i) = \\frac{w_i \\ p}{1 - w_i \\ p} ≈ w_i \\ p = E(w_i) Odds(i)=1−wi​ pwi​ p​≈wi​ p=E(wi​) 其中wiw_iwi​是用户观看时长，ppp是打开的概率，因此乘积就是观看时长的期望。 （3）训练和测试样本的处理 ① 候选集生成模型训练时，输出是所有视频个数，数量非常庞大，因此YouTube采用了Word2vec中用的负采样训练方法。 ② 在训练集的预处理过程中，YouTube没有采用原始的用户日志，而是对每个用户提出等量的样本，是为了减少高度活跃的用户对模型损失的过度影响。因为这会使得模型更倾向于活跃用户，而忽略了长尾用户。 ③ 在处理测试集时，YouTube没有采用经典的随机留一法（random holdout），而是选择用户最近一次观看的行为，是为了避免引入未来信息（future information）。 ④ 如何处理用户对新视频的偏好呢？在特征中引入了Example Age，含义是训练样本产生的时刻距离当前时刻的时间，单位是小时。模型服务时，这个特征直接设置为0。YouTube通过实验验证了Example Age的重要性。 （4）问答经典 ① 请问召回模型中，输入层已经有了视频的预训练的Embedding向量，最后softmax 的参数也会作为视频的embedding向量。一开始不是都有了视频的Embedding向量了吗？最后ANN的为什么只用训练视频向量，而不用预训练的呢？ 因为只有最后的视频Embedding是跟用户Embedding在一个向量空间内。预训练和Embedding和最后relu层生成的user Embedding没有直接关系。 ② YouTube 的排序模型和候选集生成模型，都使用了平均池化这一操作，来把用户的历史观看视频整合起来。你能想到更好的方法来改进这个操作吗？ 在召回层，对用户历史观看的序列，按照时间衰减因子，对用户观看Embedding序列进行加权求平均，加强最近观看视频的影响力 在排序层，可以加入注意力机制，类似DIN模型中，计算候选Embedding与用户行为序列中视频Embedding的权重，然后在进行加权求平均，得到用户行为序列的Embedding ③ 之前讲Embedding近邻搜索，需要用户Embedding和物品Embedding在同一向量空间。那么在召回层relu中提取的用户Embedding和softmax提取的物品Embedding，是在同一向量空间的，为什么？ relu隐藏层的输出是用户向量，正好是softmax层的输入x，根据前向计算wi*x+b计算得到了物品i 节点值，这里的wi也就能代表物品向量了。也就是说由用户向量参与计算生成了最后的物品向量，跟前面利用电影向量 sum pooling出用户向量逻辑一致。所以他们在同一向量空间。 ④ 实际 Weighted LR 具体训练过程吗，比如 videoid1 labels=1 weights=15 , 实际中是把这个样本 重复抽样weights 次，放入训练样本吗，还是更改LR 的loss？ 两种方式都可以。有一些细微的差别，但我觉得无伤大雅，选一种就行。 ⑤ 为什么不一开始就使用item2vec训练视频embedding，平均为用户的embedding呢？是因为维度对不上还是因为不在同一个向量空间?而且这时候输入的视频embedding可以finetune的。 做法上当然是可以的，YouTube仅仅是把所有的模型结构都画在这里，至于怎么实现，说实话那是每个人自己的事情。个人而言，候选集生成模型的输入用户embedding完全可以像你说的一样预训练生成，没有一点问题。 4. Pinterest应用图神经网络 （1）简介 DeepWalk、Node2Vec 这些非常实用的 Graph Embedding 方法。但是技术的发展永无止境，最近两年，GNN（Graph Nerual Netwrok，图神经网络）毫无疑问是最火热、最流行的基于图结构数据的建模方法。严格一点来说，图神经网络指的就是可以直接处理图结构数据的神经网络模型。 在诸多 GNN 的解决方案中，著名的社交电商巨头 Pinterest 对于 GraphSAGE 的实现和落地又是最为成功的，在业界的影响力也最大 （2）搭桥还是平推？ ① Deep Walk、Node2Vec与GNN之间的关系。 Deep Walk、Node2Vec没有直接处理图结构的数据，而是走了一个取巧的方式，先把图结构数据通过随机游走采样，转换成了序列数据，然后再 用诸如 Word2vec 这类序列数据 Embedding 的方法生成最终的 Graph Embedding。就是面对一个复杂问题时，我们不直接解决它，而是“搭一座桥”，通过这座桥把这个复杂问题转换成一个简单问题，因为对于简单问题，我们有非常丰富的处理手段。这样一来，这个复杂问题也就能简单地解决了。显然，基于随机游走的 Graph Embedding 方法就是这样一种“搭桥”的解决方案。 GNN 是一种平推解决图结构数据问题的方法，它直接输入图结构的数据，产生节点的 Embedding 或者推荐结果。 （3）GraphSAGE的主要步骤 GraphSAGE 的全称叫做 Graph Sample and Aggregate，翻译过来叫“图采样和聚集方法”。其实这个名称就很好地解释了它运行的过程，就是先“采样”、再“聚集”。 GraphSAGE 的过程如上图所示，主要可以分为 3 步： 在整体的图数据上，从某一个中心节点开始采样，得到一个 k 阶的子图，示意图中给出的示例是一个二阶子图； 有了这个二阶子图，我们可以先利用 GNN 把二阶的邻接点聚合成一阶的邻接点（图 1-2 中绿色的部分），再把一阶的邻接点聚合成这个中心节点（图 1-2 中蓝色的部分）； 有了聚合好的这个中心节点的 Embedding，我们就可以去完成一个预测任务，比如这个中心节点的标签是被点击的电影，那我们就可以让这个 GNN 完成一个点击率预估任务。 （4）GraphSAGE是怎么工作的 首先，我们要利用 MovieLens 的数据得到电影间的关系图，这个关系图可以是用用户行为生成。它也可以是像生成知识图谱一样来生成。比如，两部电影拥有同一个演员就可以建立一条边，拥有相同的风格也可以建立一条边，规则我们可以自己定。 在这个由电影作为节点的关系图上，我们随机选择一个中心节点。比如，我们选择了玩具总动员（Toy Story）作为中心节点，这时再向外进行二阶的邻接点采样，就能生成一个树形的样本。 经过多次采样之后，我们会拥有一批这样的子图样本。这时，我们就可以把这些样本输入 GNN 中进行训练了。这个 GNN 既可以预测中心节点的标签，比如点击或未点击，也可以单纯训练中心节点的 Embedding 就够了。 （5）GraphSAGE模型结构 GraphSAGE 的模型结构到底怎么样？它到底是怎么把一个 k 阶的子图放到 GNN 中去训练，然后生成中心节点的 Embedding 的呢？以二阶GraphSAGE为例： 上图中处理的样本是一个以点 A 为中心节点的二阶子图，从左到右我们可以看到，点 A 的一阶邻接点包括点 B、点 C 和点 D，从点 B、C、D 再扩散一阶，可以看到点 B 的邻接点是点 A 和点 C，点 C 的邻接点是 A、B、E、F，而点 D 的邻接点是点 A。 清楚了样本的结构，我们再从右到左来看一看 GraphSAGE 的训练过程。这个 GNN 的输入是二阶邻接点的 Embedding，二阶邻接点的 Embedding 通过一个叫 CONVOLVE 的操作生成了一阶邻接点的 Embedding，然后一阶邻接点的 Embedding 再通过这个 CONVOLVE 的操作生成了目标中心节点的 Embedding，至此完成了整个训练。 这个 CONVOLVE 操作是由两个步骤组成的：第一步叫 Aggregate 操作，就是图 4 中 gamma 符号代表的操作，它把点 A 的三个邻接点 Embedding 进行了聚合，生成了一个 Embedding hN(A)；第二步，我们再把 hN(A) 与点 A 上一轮训练中的 Embedding hA 连接起来，然后通过一个全联接层生成点 A 新的 Embedding。 Aggregate 操作我们也不陌生，它其实就是把多个 Embedding 聚合成一个 Embedding 的操作。 （6）GraphSAGE的预测目标 要知道预测样本标签这个事情是一个典型的有监督学习任务，而生成节点的 Embedding 又是一个无监督学习任务。那 GraphSAGE 是怎么做到既可以进行有监督学习，又能进行无监督学习的呢？要想让 GraphSAGE 做到这一点，关键就看你怎么设计它的输出层了。 我们先来说说有监督的情况，为了预测中心节点附带的标签，比如这个标签是点击或未点击，我们就需要让 GraphSAGE 的输出层是一个 Logistic Regression 这样的二分类模型，这个输出层的输入，就是我们之前通过 GNN 学到的中心节点 Embedding，输出当然就是预测标签的概率了。这样，GraphSAGE 就可以完成有监督学习的任务了。 而对于无监督学习，那就更简单了。这是因为，我们的输出层就完全可以仿照Word2vec 输出层的设计，用一个 softmax 当作输出层，预测的是每个点的 ID。这样一来，每个点 ID 对应的 softmax 输出层向量就是这个点的 Embedding，这就和 word2vec 的原理完全一致了。 （7）GraphSAGE 在 Pinterest 推荐系统中的应用 Pinterest 这个网站的主要功能是为用户提供各种商品的浏览、推荐、收藏的服务，那么所谓的 Pin 这个动作，其实就是你收藏了一个商品到自己的收藏夹。因此，所有的 Pin 操作就连接起了用户、商品和收藏夹，共同构成了一个它们之间的关系图。PinSAGE 就是在这个图上训练并得到每个商品的 Embedding 的。 我们先看图 5 左边的例子，因为它给出的是一个种子发芽的图片，我们就推测它应该是一个卖绿植或者绿植种子的商家。接下来，我们再来判断左边通过四种不同算法找到的相似图片是不是合理。其中，PinSAGE 是 Pinterest 实际用于推荐系统中的算法，其他三个 Visual、Annot、Pixie 都是效果测试中的对比算法。 我们看到通过第一个算法 Visual 找到的图片，虽然看上去和原来的图片比较相似，但前两个图片居然都是食品照片，这显然不相关。第二个算法 Annot 中的树木，以及第三个算法 Pixie 中的辣椒和西兰花，显然都跟绿植种子有很遥远的差距。相比之下，PinSAGE 找到的图片就很合理了，它找到的全都是种子发芽或者培育绿植的图片，这就非常合乎用户的逻辑了。 要知道，在 PinSAGE 应用的构成中，它没有直接分析图片内容，而只是把图片当作一个节点，利用节点和周围节点的关系生成的图片 Embedding。因此，这个例子可以说明，PinSAGE 某种程度上理解了图片的语义信息，而这些语义信息正是埋藏在 Pinterest 的商品关系图中。可见，PinSAGE 起到了多么神奇的数据挖掘的作用。 5. Flink实现实时推荐 （1）为什么实时性是影响推荐系统效果的关键因素？ 推荐系统只有拥有实时抓住用户新兴趣点的能力，才能让你的用户“离不开你”。 （2）什么是批流一体的数据处理体系？ 无论是数据的预处理，还是特征工程，大部分是在 Spark 平台上完成的。Spark 平台的特点是，它处理的数据都是已经落盘的数据。也就是说，这些数据要么是在硬盘上，要么是在分布式的文件系统上，然后才会被批量地载入到 Spark 平台上进行运算处理，这种批量处理大数据的架构就叫做批处理大数据架构。批处理架构图： 但批处理架构的特点就是慢，数据从产生到落盘，再到被 Spark 平台重新读取处理，往往要经历几十分钟甚至几小时的延迟。我们能不能在数据产生之后就立马处理它，而不是等到它落盘后再重新处理它呢？当然是可以的，这种在数据产生后就直接对数据流进行处理的架构，就叫做流处理大数据架构。 从流处理架构示意图中可以看到，它和批处理大数据架构相比，不仅用流处理平台替换掉了分布式批处理 Map Reduce 计算平台，而且在数据源与计算平台之间，也不再有存储系统这一层。这就大大提高了数据处理的速度，让数据的延迟可以降低到几分钟级别，甚至一分钟以内，这也让实时推荐成为了可能。 但是，流处理平台也不是十全十美的。由于流处理平台是对数据流进行直接处理，它没有办法进行长时间段的历史数据的全量处理，这就让流处理平台无法应用在历史特征的提取，模型的训练样本生成这样非常重要的领域。 那是不是说，根本就没有能够同时具有批处理、流处理优势的解决方案吗？当然是有的，这就是我们在一开始说的，批流一体的大数据架构，其中最有代表性的就是 Flink。批流一体的大数据架构最重要的特点，就是在流处理架构的基础上添加了数据重播的功能。 数据重播指的是在数据落盘之后，还可以利用流处理平台同样的代码，进行落盘数据的处理，这就相当于进行了一遍重播。这样不就实现了离线环境下的数据批处理了吗？而且由于流处理和批处理使用的是一套代码，因此完美保证了代码维护的一致性，是近乎完美的数据流解决方案。 （3）Flink 是如何处理数据流的？ Flink 中两个最重要的概念，数据流（DataStream）和窗口（Window）。数据流其实就是消息队列，从网站、APP 这些客户端中产生的数据，被发送到服务器端的时候，就是一个数据消息队列，而流处理平台就是要对这个消息队列进行实时处理。 Flink 会怎么处理这个消息队列里的数据呢？答案很简单，就是随着时间的流失，按照时间窗口来依次处理每个时间窗口内的数据。除了固定窗口以外，Flink 还提供了多种不同的窗口类型，滑动窗口（Sliding Window）也是我们经常会用到的。 6. 阿里巴巴是如何迭代更新推荐模型的？ 阿里巴巴的业务场景大家非常熟悉，在传统算法部分，介绍了其LS-PLM的工作原理；在深度学习算法部分，介绍了DIN和DIEN。因此在本节主要介绍下其迭代更新思路。 graph LR A((推荐系统))-->B(LS-PLM,2012)-->C(基础深度学习模型,2016)-->D(DIN,2017)-->E(DIEN,2018)-->F(MIMN,2019)-->G((推荐系统主模型)) A-->X(多模态建模CMN,2018)-->Y(多目标建模ESMM,2018)-->Z((独立问题通用模型)) （1）基础深度学习模型 基于经典的Embedding+MLP深度学习模型架构，将用户历史Embedding简单的通过加和池化操作叠加，再与其他用户特征、广告特征、场景特征连接后输入上层神经网络进行训练。 （2）DIN模型 利用注意力机制替换基础模型的Sum Pooling操作，根据候选广告和用户历史行为之间的关系确定每个历史行为的权重。 （3）DIEN模型 在DIN基础上，进一步改进对用户行为历史的建模，使用序列模型在用户行为历史之上抽取用户兴趣并模拟用户兴趣的演化过程。 （4）MIMN模型 在DIEN模型基础上，将用户的兴趣细分为不同兴趣通道，进一步模拟用户在不同兴趣通道上的演化进程，生成不同兴趣通道的记忆向量，再利用注意力机制作用于多层神经网络，论文地址。 对于DIEN和MIMN这种带有序列结构的模型来说，推断过程要串行，因此无法被并行加速，使得模型服务的延迟率居高不下。阿里是如何解决这类问题的呢？在MIMN的论文中公开了解决方案。 上图展示了两种不同的模型服务架构，主要区别在于红框中对于实时用户行为事件的处理方法不同，具体如下： ① 用户兴趣表达模块 B架构把A架构的“用户行为特征（User Behavior Features）在线数据库”替换成了“用户兴趣表达（User Interest Representation）在线数据库”。无论是DIEN还是MIMN，它们表达用户兴趣的最终形式都是兴趣Embedding向量。如果在线获取的是用户行为特征序列，那么实时预测服务器（Real-Time Prediction Server）就需要执行复杂的序列模型推理以生成用户兴趣向量。若在线获取的直接就是用户兴趣向量，那么实时预测服务直接推理MLP部分就可以了，可以大大降低预测的延时（因为MLP的层数相比序列模型要少很多，而且可以并行计算）。 ② 用户兴趣中心模块 B架构增加了用户兴趣中心模块（User Interest Center，UIC）用于把用户行为序列生成用户兴趣向量，对DIEN和MIMN来说，UIC运行着生成用户兴趣向量的部分模型（在A架构中这部分模型是在实时预测服务器中的）。与此同时，实时用户行为事件（realtime user behavior event）的更新方式也发生了变化：在A架构中，一个新的用户行为事件发生时，该事件会被插入用户行为特征数据库中；在B架构中，新的用户行为事件则会出发UIC的更新逻辑，UIC利用该事件更新对应用户的兴趣向量。 除此之外的模块作用基本一致，接下来看下离线部分和在线部分的运行逻辑： ③ 离线运行逻辑 学习模块（Learner）定期利用系统日志（Logs）训练兵更新模型（Model），模型更新之后，新模型在A架构中被直接部署在实时预测服务器中，而B架构则对模型进行拆分，生成用户兴趣向量部分部署在UIC，其余部分部署在实时预测服务器。 ④ 在线运行逻辑 1）流量请求（traffic request）中带着用户ID(User ID)和待排序的候选商品ID(Ad ID)信息达到服务器。 2）实时预测服务器根据用户ID和候选商品ID获取用户和商品特征（Ad Features），用户特征具体包括用户的人口属性特征（User Demography Features）和用户行为特征（A架构）或用户兴趣表达向量（B架构）。 3）实时预测服务器利用用户和商品特征进行排序并返回结果。 B架构中对模型的拆解效果如何呢？阿里巴巴公开的数据是：每个服务节点在500QPS的压力下，DIEN模型的预测时间从200ms降至19ms，效果非常显著。 从本质上看，A架构采用了端到端的部署方案，B架构采用了Embedding+轻量级线上模型的部署方案。 （5）其他方向 阿里团队还在很多方向上都做了研究。比如说，同时处理 CTR 预估和 CVR 预估问题的多目标优化模型 ESMM（Entire Space Multi-Task Model），基于用户会话进行会话内推荐的 DSIN（Deep Session Interest Network ），可以在线快速排序的轻量级深度推荐模型 COLD(Computing power cost-aware Online and Lightweight Deep pre-ranking system) 等等。 7. 美团是如何在推荐系统中落地强化学习的? 强化学习在推荐系统中落地会有一个难点：因为强化学习涉及离线模型训练、线上服务、数据收集、实时模型更新等几乎推荐系统的所有工程环节，所以强化学习整个落地过程的工程量非常大，需要多团队紧密合作才能实现。大家可以学习一下美团的成功实践：强化学习在美团“猜你喜欢”的实践 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"业界主流","slug":"业界主流","permalink":"https://www.notlate.net/tags/%E4%B8%9A%E7%95%8C%E4%B8%BB%E6%B5%81/"}]},{"title":"深度学习推荐系统-笔记12：模型评估","slug":"RecSys/5.模型评估/[深度学习推荐系统-笔记12]模型评估","date":"2021-01-05T14:28:20.000Z","updated":"2021-03-18T15:04:38.586Z","comments":true,"path":"posts/9f710c67.html","link":"","permalink":"https://www.notlate.net/posts/9f710c67.html","excerpt":"","text":"1. 离线评估 （1）介绍 离线评估是最常用、最基本的。顾名思义就是：我们将模型部署于线上环境之前，在离线环境下进行的评估。由于不用部署到生产环境，“离线评估”没有线上部署的工程风险，也不会浪费宝贵的线上流量资源，而且具有测试时间短，可多组并行，以及能够利用丰富的线下计算资源等诸多优点。 （2）评估方法 ① Holdout 检验。 最基础，最常用的离线评估方法，它将原始的样本集合随机划分为训练集和测试集两部分，所以 Holdout 检验的关键词就是“随机”。 缺点：评估的结果有一定随机性。且样本比较少时，训练集会进一步减小，往往会影响模型的训练效果。 ② 交叉检验 将全部样本划分成 k 个大小相等的样本子集，然后依次遍历这 k 个子集，每次把当前遍历到的子集作为验证集，其余所有的子集作为训练集，这样依次进行 k 次模型的训练和评估。最后，我们再将所有 k 次评估指标的平均值作为最终的评估指标。在我们的实践中，k 经常取 10，也就是依次进行 10 次检验然后取指标均值。 缺点：当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这往往会影响模型的训练效果。 ③ 自助法（不太常用） 基于自助采样的检验方法，它的主要过程是：对于总数为 n 的样本集合，我们先进行 n 次有放回地随机抽样，得到大小为 n 的训练集。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，我们再将这些没有被抽出的样本作为验证集进行模型验证，这就是自助法的验证过程。 缺点：改变了原有数据的分布，有可能让模型产生一定程度的偏差。 ④ 时间切割 防止“信息穿越”导致的模型作弊现象发生，我们一般会使用时间切割的方案去划分训练集和测试集。比如，你一共处理了 30 天的样本，从第 25 天末开始切割，前 25 天的样本作为训练集，后 5 天的样本作为测试集，这样我们就从根源上切断了引入“未来信息”的可能。 缺点：整个评估过程是静态的，模型不会随着评估的进行而更新，这显然是不符合事实的 ⑤ 离线Replay 在离线状态下对线上更新过程进行仿真，让整个评估过程“动”起来。业界把这样离线仿真式的评估方式叫做离线 Replay。 Replay 评估的过程更接近于真实的线上环境，因为它在线下还原了模型在线上的更新、预估过程。这也让 Replay 方法的评估结果更加权威可信，毕竟，我们最终的目标是让模型在线上产生更好的效果。 （3）总结 （4）经典问答 ① 离线 Replay 和增强学习有什么相似之处吗？它们两个还有什么更深层次的关系吗？ 离线Replay和RL都是动态更新模型的，都需要不断的测试和再训练模型。增强学习(如DRN)是通过不断接受反馈，在线更新模型的，所以评估方法不能引入未来信息， 简单的时间切割评估方法又不能模拟模型的更新频率，所以离线Replay是增强学习的唯一离线评估方法。 2. 评估指标 为了更明白的描述各个指标，预定义如下标识： graph LR A[总样本:含正负样本]-->B[经过分类器计算得到概率] B-->|概率>=阈值| C1[判定为正] B-->|概率< 阈值| C2[判定为负] C1-->|实际正样本,预测正确| TP[TP:判定为正,分类正确] C1-->|实际负样本,预测错误| FP[FP:判定为正,分类错误] C2-->|实际负样本,预测正确| TN[TN:判定为负,分类正确] C2-->|实际正样本,预测错误| FN[FN:判定为负,分类错误] 关于以上TP、FP、FN、TN四组表示，大家应该可以总结出规律，其中第一个字母的T和F分别表示分类结果正确和错误；第二个字母P和N分别表示判定为正和负样本。 （1）低阶评估指标 对于低阶评估指标来说，准确率、精确率与召回率、对数损失、均方根误差，这四个指标在推荐模型评估中最常用。 ① 准确率： 准确率=(TP+TN)分类结果正确的样本(圆形图中绿色对角块)(TP+FP+FN+TN)总样本∗100%准确率 = \\frac{(TP + TN)_{分类结果正确的样本(圆形图中绿色对角块)}}{(TP + FP + FN + TN)_{总样本}} * 100 \\% 准确率=(TP+FP+FN+TN)总样本​(TP+TN)分类结果正确的样本(圆形图中绿色对角块)​​∗100% 缺点：当不同类别的样本比例非常不均衡的时候，占比大的类别往往成为影响准确率的最主要因素。比如，负样本占 99%，那么分类器把所有样本都预测为负样本也可以获得 99% 的准确率。 ② 精确率与召回率： 精确率=TP分类结果正确的正样本(圆形图中左上角)(TP+FP)分类为正的样本(圆形图中的左半球，纵轴方向)∗100%精确率 = \\frac{TP_{分类结果正确的正样本(圆形图中左上角)}}{(TP + FP)_{分类为正的样本(圆形图中的左半球，纵轴方向)}} * 100\\% 精确率=(TP+FP)分类为正的样本(圆形图中的左半球，纵轴方向)​TP分类结果正确的正样本(圆形图中左上角)​​∗100% 召回率=TP分类结果正确的正样本(圆形图中左上角)(TP+FN)真实的正样本(圆形图中的北半球，横轴方向)∗100%召回率=\\frac{TP_{分类结果正确的正样本(圆形图中左上角)}}{(TP + FN)_{真实的正样本(圆形图中的北半球，横轴方向)}} * 100 \\% 召回率=(TP+FN)真实的正样本(圆形图中的北半球，横轴方向)​TP分类结果正确的正样本(圆形图中左上角)​​∗100% 在推荐列表中，通常没有一个确定的阈值来把预测结果直接判定为正样本或负样本，而是采用 **Top N 排序结果的精确率（Precision@N）和召回率（Recall@N）**来衡量排序模型的性能。具体操作，就是认为模型排序的前 N 个结果就是模型判定的正样本，然后分别计算 Precision@N 和 Recall@N。 精确率和召回率其实是矛盾统一的一对指标，可以使用F1-score综合地反映精确率和召回率的高低。 F1-score的定义是精确率和召回率的调和平均值。 F1=2⋅Precision⋅RecallPrecision+RecallF1=\\frac{2 · Precision · Recall}{Precision + Recall} F1=Precision+Recall2⋅Precision⋅Recall​ ③ 对数损失 二分类和多分类模型的 Logloss 其实就是我们之前讲过的逻辑回归和 Softmax 模型的损失函数，而大量深度学习模型的输出层正是逻辑回归或 Softmax，因此，采用 Logloss 作为评估指标能够非常直观地反映模型损失函数的变化。所以在训练模型的过程中，我们在每一轮训练中都会输出 Logloss，来观察模型的收敛情况。 二分类对数损失函数：−1N∑i=1N[yilogpi+(1−yi)log(1−pi)]-\\frac{1}{N} \\sum^N_{i=1} [y_i log{p_i} + (1-y_i) log(1-p_i)]−N1​∑i=1N​[yi​logpi​+(1−yi​)log(1−pi​)] 多分类对数损失函数：−1N∑i=1N∑j=1M[yi,j log(pi,j)]-\\frac{1}{N} \\sum^N_{i=1} \\sum^M_{j=1} [y_{i,j} \\ log(p_{i,j})]−N1​∑i=1N​∑j=1M​[yi,j​ log(pi,j​)] ④ 均方根误差 准确率、精确率、召回率、LogLoss 都是针对分类模型指定的指标。除了这种分类模型外，还有回归模型。它是用来预测一个连续值，比如预测某个用户对某个电影会打多少分，这就是一个回归模型。最常用的评估指标就是均方根误差（RMSE，Root Mean Square Error）。公式如下： RMSE=∑i=1N(yi−yi^)2NRMSE=\\sqrt \\frac{\\sum^N_{i=1} (y_i - \\hat{y_i})^2}{N} RMSE=N∑i=1N​(yi​−yi​^​)2​​ yiy_iyi​是第iii个样本点的真实值，yi^\\hat{y_i}yi​^​是第iii个样本点的预测值，nnn是样本个数。那么均方根误差越小，当然就证明这个回归模型预测越精确。 （2）高阶评估指标 ① P-R曲线：P: 精确率Precision，R: 召回率Recall。 为了综合评价一个推荐模型的好坏，不仅要看模型在一个 Top n 值下的精确率和召回率，还要看到模型在不同 N 取值下的表现，甚至最好能绘制出一条 n 从 1 到 N，准确率和召回率变化的曲线，这条曲线就是 P-R 曲线。其中纵轴是精确率(注意看公式2的分母)，横轴是召回率(注意看公式3的分母)。 AUC(Area Under Curve)，曲线下面积。计算 AUC 值只需要沿着曲线横轴做积分。AUC 越大，就证明推荐模型的综合性能越好。其中可以是P-R曲线，也可以是接下来要介绍的ROC曲线。 ② ROC曲线：the Receiver Operating Characteristic 曲线（受试者工作特征曲线）。 纵轴是真阳性率TPR（True Positive Rate）: TPR=TP分类正确的正样本(圆形图中左上角)(TP+FN)真实的正样本(圆形图中北半球)=R召回率TPR=\\frac{TP_{分类正确的正样本(圆形图中左上角)}}{(TP + FN)_{真实的正样本(圆形图中北半球)}} = R_{召回率} TPR=(TP+FN)真实的正样本(圆形图中北半球)​TP分类正确的正样本(圆形图中左上角)​​=R召回率​ 横轴是假阳性率FPR（False Positive Rate）: FPR=FP分类错误的负样本(圆形图中左下角)(FP+TN)真实的负样本(圆形图中南半球)FPR=\\frac{FP_{分类错误的负样本(圆形图中左下角)}}{(FP+TN)_{真实的负样本(圆形图中南半球)}} FPR=(FP+TN)真实的负样本(圆形图中南半球)​FP分类错误的负样本(圆形图中左下角)​​ 绘制过程： 第一步，根据样本标签统计出正负样本的数量，假设正样本数量为P(圆形图中北半球)，负样本数量为N(圆形图中南半球)。 第二步，把纵轴的刻度间隔设置为1/P，横轴的刻度间隔设置为1/N。 第三步，再根据模型输出的预测概率对样本进行从高到低的排序。 第四步，依次遍历样本。从零点开始绘制 ROC 曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在 (1,1) 这个点，整个 ROC 曲线就绘制完成了。 第五步：在绘制完 ROC 曲线后，我们也可以像 P-R 曲线一样，计算出 ROC 曲线的 AUC，AUC 越高，推荐模型的综合性能就越好。 ③ 平均精度均值：mAP（mean average precision） mAP是对平均精度（AP，average precision）的再次平均。那么如何计算AP呢？假设推荐系统对某一用户测试集的排序结果如下（就是给这用户推荐了N个物品，用户实际点击的统计）： 推荐序列 N=1 N=2 N=3 N=4 N=5 N=6 真实标签 1 0 0 1 1 1 其中1表示正样本，0表示负样本（也就是说系统给这名用户推荐了N=6个物品，他/她分别点击了第1、4、5、6共计4个物品）。则每个位置上的Precision@N分别是多少呢？结果如下： 推荐序列 N=1 N=2 N=3 N=4 N=5 N=6 真实标签 1 0 0 1 1 1 Precision@N 1/1 1/2 1/3 2/4 3/5 4/6 计算时候，只取正样本处的 Precision 进行平均，所以AP=(1/1+2/4+3/5+4/6)4=0.6917AP = \\frac {(1/1 + 2/4 + 3/5 + 4/6)} {4} = 0.6917AP=4(1/1+2/4+3/5+4/6)​=0.6917。 mAP又是什么呢？对测试集中的每个用户都进行样本排序，那么每个用户都会计算出一个 AP 值，再对所有用户的 AP 值进行平均，就得到了 mAP。因为 mAP 需要对每个用户的样本进行分用户排序，而 P-R 曲线和 ROC 曲线均是对全量测试样本进行排序。 （3）问答经典 ① 在正负样本不平衡的场景下，P-R和ROC哪个稳定？ 当我们将负样本复制10倍时： ROC曲线（TPR=TP分类正确的正样本(圆形图中左上角)(TP+FN)真实的正样本(圆形图中北半球)TPR=\\frac{TP_{分类正确的正样本(圆形图中左上角)}}{(TP + FN)_{真实的正样本(圆形图中北半球)}}TPR=(TP+FN)真实的正样本(圆形图中北半球)​TP分类正确的正样本(圆形图中左上角)​​，FPR=FP分类错误的负样本(圆形图中左下角)(FP+TN)真实的负样本(圆形图中南半球)FPR=\\frac{FP_{分类错误的负样本(圆形图中左下角)}}{(FP+TN)_{真实的负样本(圆形图中南半球)}}FPR=(FP+TN)真实的负样本(圆形图中南半球)​FP分类错误的负样本(圆形图中左下角)​​）中，TPR显然不会变，FPR是负样本中被预测为正样本的比例，这其实也是不变的，那整个ROC曲线也就没有变。 PR曲线(精确率=TP分类结果正确的正样本(圆形图中左上角)(TP+FP)分类为正的样本(圆形图中的左半球，纵轴方向)精确率 = \\frac{TP_{分类结果正确的正样本(圆形图中左上角)}}{(TP + FP)_{分类为正的样本(圆形图中的左半球，纵轴方向)}}精确率=(TP+FP)分类为正的样本(圆形图中的左半球，纵轴方向)​TP分类结果正确的正样本(圆形图中左上角)​​，召回率=TP分类结果正确的正样本(圆形图中左上角)(TP+FN)真实的正样本(圆形图中的北半球，横轴方向)召回率=\\frac{TP_{分类结果正确的正样本(圆形图中左上角)}}{(TP + FN)_{真实的正样本(圆形图中的北半球，横轴方向)}}召回率=(TP+FN)真实的正样本(圆形图中的北半球，横轴方向)​TP分类结果正确的正样本(圆形图中左上角)​​），TP不变，FP增大，而召回率R没有变，显然ROC曲线更稳定一些。 ② 动图显示表现优秀、表现一般、表现很差的三种分类器对应的PR曲线和ROC曲线。 以下动图参考自：《P-R曲线和ROC曲线理解》，正样本对应下图中的红色曲线(病人分布)，负样本对应下图中的蓝色曲线(健康人群分布) 表现优秀的分类器，其中正负样本分布基本不重合，因此分类器可以很好地判定，此时PR和ROC曲线下的面积都很大。 表现很差的分类器，其中正负样本分布全部重合，因此分类器无法正确判定，此时PR和ROC曲线下面积都很小。 表现一般的分类器，其中正负样本重合一小部分，因此分类器在非重合场景下能够正确判定，重合场景下错误率较高。 3. 在线A/B测试 （1）A/B测试原理 A/B 测试又被称为“分流测试”或“分桶测试”，它通过把被测对象随机分成 A、B 两组，分别对它们进行对照测试的方法得出实验结论。具体到推荐模型测试的场景下，它的流程是这样的： 先将用户随机分成实验组和对照组，然后给实验组的用户施以新模型，给对照组的用户施以旧模型，再经过一定时间的测试后，计算出实验组和对照组各项线上评估指标，来比较新旧模型的效果差异，最后挑选出效果更好的推荐模型。 （2）选择A/B测试的原因 ① 离线评估无法完全还原线上的工程环境。一般来讲，离线评估往往不考虑线上环境的延迟、数据丢失、标签数据缺失等情况，或者说很难还原线上环境的这些细节。因此，离线评估环境只能说是理想状态下的工程环境，得出的评估结果存在一定的失真现象。 ② 线上系统的某些商业指标在离线评估中无法计算。 离线评估一般是针对模型本身进行评估的，无法直接获得与模型相关的其他指标，特别是商业指标。像我们上节课讲的，离线评估关注的往往是ROC曲线、PR曲线的改进，而线上评估却可以全面了解推荐模型带来的用户点击率、留存时长、PV访问量这些指标的变化。 ③ 离线评估无法完全消除数据有偏（Data Bias）现象的影响。 什么叫“数据有偏”呢？因为离线数据都是系统利用当前算法生成的数据，因此这些数据本身就不是完全客观中立的，它是用户在当前模型下的反馈。所以说，用户本身有可能已经被当前的模型“带跑偏了”，你再用这些有偏的数据来衡量你的新模型，得到的结果就可能不客观。 （3）A/B测试核心原则 ① 怎样才能对用户进行一个公平公正的分桶呢？如果有多组实验在同时做 A/B 测试，怎样做才能让它们互不干扰？ 分桶：我们需要注意的是样本的独立性和分桶过程的无偏性。这里的“独立性”指的是同一个用户在测试的全程只能被分到同一个桶中。“无偏性”指的是在分桶过程中用户被分到哪个实验桶中应该是一个纯随机的过程。 分层：层与层之间的流量“正交”，同层之间的流量“互斥”。 ② 层与层之间的流量“正交”：**层与层之间的独立实验的流量是正交的，一批实验用的流量穿越每层实验时，都会再次随机打散，然后再用于下一层的实验。**看下示意图： 假设，在一个 X 层的实验中，流量被随机平均分为 X1（蓝色）和 X2（白色）两部分。当它们穿越到 Y 层的实验之后，X1 和 X2 的流量会被随机且均匀地分配给 Y 层的两个桶 Y1 和 Y2。 如果 Y1 和 Y2 的 X 层流量分配不均匀，那么 Y 层的样本就是有偏的，Y 层的实验结果就会被 X 层的实验影响，也就无法客观地反映 Y 层实验组和对照组变量的影响。 ③ 同层之间的流量“互斥”： 如果同层之间进行多组 A/B 测试，不同测试之间的流量不可以重叠，这是第一个“互斥”； 一组 A/B 测试中实验组和对照组的流量是不重叠的，这是第二个“互斥”。 不同测试之间以及A/B测试的实验组和对照组之间的用户是不重叠的。特别是对推荐系统来说，用户体验的一致性是非常重要的。也就是说我们不可以让同一个用户在不同的实验组之间来回“跳跃”，这样会严重损害用户的实际体验，也会让不同组的实验结果相互影响。因此在 A/B 测试中，保证同一用户始终分配到同一个组是非常有必要的。 （4）A/B测试评估指标 A/B 测试的指标应该与线上业务的核心指标保持一致。 4. 推荐系统评估体系 （1）什么是推荐系统评估体系 由多种不同的评估方式组成的、兼顾效率和正确性的，一套用于评估推荐系统的解决方案。典型评估体系示意图如下： 处于最底层的是传统的离线评估方法，比如 Holdout 检验、交叉检验等，往上是离线 Replay 评估方法，再往上是一种叫 Interleaving 的线上测试方法，最后是线上 A/B 测试。 越是底层的方法就会承担越多筛选掉改进思路的任务，这时候“评估效率”就成了更关键的考虑因素； 随着候选模型被一层层筛选出来，越接近正式上线的阶段，评估方法对评估“正确性”的要求就越严格。 （2）实例讲解模型筛选过程 假设，现在有 30 个待筛选的模型，如果所有模型都直接进入线上 A/B 测试的阶段进行测试，所需的测试样本是海量的，由于线上流量有限，测试的时间会非常长。但如果我们把测试分成两个阶段，第一个阶段先进行初筛，把 30 个模型筛选出可能胜出的 5 个，再只对这 5 个模型做线上 A/B 测试，所需的测试流量规模和测试时间长度都会大大减少。这里的初筛方法，就是我们在评估体系中提到的离线评估、离线 Replay 和在线 Interleaving 等方法。 （3）Netflix 的 Replay 评估方法实践 Netflix 为了进行离线 Replay 的实验，建立了一整套从数据生成到数据处理再到数据存储的数据处理架构，并给它起了一个很漂亮的名字，叫做时光机（Time Machine）。 上图是时光机的架构，图中最主要的就是 Snapshot Jobs（数据快照）模块。它是一个每天执行的 Spark 程序，它做的主要任务就是把当天的各类日志、特征、数据整合起来，形成当天的、供模型训练和评估使用的样本数据。它还会以日期为目录名称，将样本快照数据保存在分布式文件系统 S3 中（Snapshots），再对外统一提供 API（Batch APIs），供其他模型在训练和评估的时候按照时间范围方便地获取。 Snapshot Jobs 主任务的源数据是它上方的 Context Set 模块和左边的 Prana 模块。 Context Set 模块负责保存所有的历史当天的环境信息。 环境信息主要包括两类：一类是存储在 Hive 中的场景信息，比如用户的资料、设备信息、物品信息等数据；另一类是每天都会发生改变的一些统计类信息，包括物品的曝光量、点击量、播放时长等信息。 Prana 模块负责处理每天的系统日志流。 系统日志流指的是系统实时产生的日志，它包括用户的观看历史（Viewing History）、用户的推荐列表（My List）和用户的评价（Ratings）等。这些日志从各自的服务（Service）中产生，由 Netflix 的统一数据接口 Prana 对外提供服务。 Snapshot Jobs 这个核心模块每天的任务就是，通过 Context Set 获取场景信息，通过 Prana 获取日志信息，再经过整合处理、生成特征之后，保存当天的数据快照到 S3。 在生成每天的数据快照后，使用 Replay 方法进行离线评估就不再是一件困难的事情了，因为我们没有必要在 Replay 过程中进行烦琐的特征计算，直接使用当天的数据快照就可以了。 （4）Interleaving 评估方法 ① 意义：首先，它是和 A/B 测试一样的在线评估方法，能够得到在线评估指标；其次，它提出的目的是为了比传统的 A/B 测试用更少的资源，更快的速度得到在线评估的结果。 ② 传统A/B过程：把用户随机分成两组。一组接受当前的推荐模型 A 的推荐结果，这一组被称为对照组 。另一组接受新的推荐模型 B 的推荐结果，这组被成为实验组。 ③ Interleaving过程：只需要一组用户，这些用户会收到模型 A 和模型 B 的混合结果。也就是说，用户会在一个推荐列表里同时看到模型 A 和模型 B 的推荐结果。在评估的过程中，Interleaving 方法通过分别累加模型 A 和模型 B 推荐物品的效果，来得到模型 A 和 B 最终的评估结果。 ④ 两者形象化的比较： ⑤ 以相等的概率让模型 A 和模型 B 产生的物品交替领先。这就像在野球场打球的时候，两个队长会先通过扔硬币的方式决定谁先选人，再交替来选择队员。 ⑥ 最后，我们要清楚推荐列表中的物品到底是由模型 A 生成的，还是由模型 B 生成的，然后统计出所有模型 A 物品的综合效果，以及模型 B 物品的综合效果，然后进行对比。这样，模型评估过程就完成了。 ⑦ 在测试一些用户级别而不是模型级别的在线指标时，我们就不能用 Interleaving 方法。比如用户的留存率，用户从试用到付费的转化率等，由于 Interleaving 方法同时使用了对照模型和实验模型的结果，我们就不清楚到底是哪个模型对这些结果产生了贡献。但是在测试 CTR、播放量、播放时长这些指标时，Interleaving 就可以通过累加物品效果得到它们。这个时候，它就能很好地替代传统的 A/B 测试了。 （5）Interleaving方法的优点与缺点 优点： ① Interleaving方法的灵敏度高于A/B测试。Netflix的实验表明，Interleaving方法利用10310^3103个样本就能判定算法A是否好于算法B，但是A/B测试则需要10510^5105个样本。 ② Netflix的实验同时表明Interleaving方法的指标与A/B测试指标之间的相关性是非常强的，也就意味着在Interleaving方法中胜出的算法也极有可能在A/B测试中胜出。 缺点： ① Interleaving的工程实现框架比A/B测试复杂，因为Interleaving方法的实验逻辑和业务逻辑耦合在一起，还需要将大量辅助性数据标识添加到整个数据流中。 ② Interleaving方法只是对“用户对算法推荐结果偏好程度”的相对测量，不能得出一个算法真实的表现。比如要想知道算法A能够将用户整体的观看时长提高多少？留存率提高多少？这些都是无法用Interleaving方法得到的。因此通常用的方法就是Interleaving+A/B测试两阶段实验结构。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 《P-R曲线和ROC曲线理解》 – 知乎 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"模型评估","slug":"模型评估","permalink":"https://www.notlate.net/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"}]},{"title":"深度学习推荐系统-笔记11：推荐模型融会贯通","slug":"RecSys/4.推荐模型/[深度学习推荐系统-笔记11]推荐模型融会贯通","date":"2021-01-05T14:24:40.000Z","updated":"2021-03-18T15:04:38.578Z","comments":true,"path":"posts/e1a19c9f.html","link":"","permalink":"https://www.notlate.net/posts/e1a19c9f.html","excerpt":"","text":"","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[]},{"title":"深度学习推荐系统-笔记10：经典的深度学习推荐模型","slug":"RecSys/4.推荐模型/[深度学习推荐系统-笔记10]经典深度学习推荐模型","date":"2021-01-05T13:44:40.000Z","updated":"2021-03-18T15:04:38.578Z","comments":true,"path":"posts/e066db0.html","link":"","permalink":"https://www.notlate.net/posts/e066db0.html","excerpt":"","text":"特征组合和特征交叉问题非常常见，特征的种类非常多，特征交叉的复杂程度也要大得多。解决这类问题的关键，就是模型对于特征组合和特征交叉的学习能力，因为它决定了模型对于未知特征组合样本的预测能力，而这对于复杂的推荐问题来说，是决定其推荐效果的关键点之一。 那特征交叉与深度学习模型的拟合能力有什么关系呢？以MLP为例： ① MLP具有拟合任意函数的能力，是建立在MLP有任意多层网络，以及任意多个神经元的前提下的。 ② 在训练资源有限，调参时间有限的现实情况下，MLP对于特征交叉的处理其实还比较低效。因为MLP是通过 concatenate层把所有特征连接在一起成为一个特征向量的，这里面没有特征交叉，两两特征之间没有发生任何关系。 ③ 在我们有先验知识的情况下，人为地加入一些负责特征交叉的模型结构，其实对提升模型效果会非常有帮助。这就是深度学习模型要加强特征交叉能力的原因。 接下来介绍下经典深度学习推荐模型的发展历程。 1. AutoRec - 基础结构 （1）介绍 2015年，澳大利亚国立大学提出的AutoRec，将自编码器（AutoEncoder）的思想与协同过滤结合，提出了一种单隐层神经网络推荐模型，结构简单明了，清晰易懂。 利用协同过滤（CF）中的共现矩阵，完成物品向量或者用户向量的自编码，再利用自编码的结果到用户对物品的预估评分，进行推荐排序。 自编码器的本质是起到降维作用，把稀疏的高维转成稠密的低维。 （2）模型结构 结构非常简单，就是一个单隐层的自编码器，也就是输入等于输出的结构。 ① 输入：共现矩阵中物品iii的向量r(i)⃗\\vec{r^{(i)}}r(i)，mmm表示物品向量维数。 ② 隐层：蓝色圆圈表示k(k&lt;&lt;m)k(k &lt;&lt; m)k(k&lt;&lt;m)维隐层神经元，向量V\\boldsymbol{V}V表示输入到隐层的权重参数矩阵。 ③ 输出：与输入相等，其中向量W\\boldsymbol{W}W表示隐层到输出的权重参数矩阵。 用数学形式表示如下： h(r;θ)=f(W⋅g(Vr⃗+μ)+b)h(r; \\theta) = f(\\boldsymbol{W} \\cdot g(\\boldsymbol{V} \\vec{r} + \\mu) + b) h(r;θ)=f(W⋅g(Vr+μ)+b) 其中f(⋅)，g(⋅)f(\\cdot)，g(\\cdot)f(⋅)，g(⋅)分别是输入层神经元和隐层神经元的激活函数。 （3）目标函数 目标函数形式如下，后边一项是L2正则化项，目的是为了防止过拟合： minθ∑i=1n∣∣r(i)⃗−h(r(i)⃗;θ)∣∣ο2+λ2⋅(∣∣W∣∣F2+∣∣V∣∣F2)\\underset{\\theta}{min} \\sum^{n}_{i=1} || \\vec{r^{(i)}} - h(\\vec{r^{(i)}}; \\theta) ||^{2}_{\\omicron} + \\frac{\\lambda}{2} \\cdot (||\\boldsymbol{W}||^{2}_{F} + ||\\boldsymbol{V}||^{2}_{F}) θmin​i=1∑n​∣∣r(i)−h(r(i);θ)∣∣ο2​+2λ​⋅(∣∣W∣∣F2​+∣∣V∣∣F2​) 其中∣∣⋅∣∣F2||\\cdot||^{2}_{F}∣∣⋅∣∣F2​表示是L2范数，训练方法用梯度反向传播即可。 （4）推荐过程 ① 当输入物品iii的向量r(i)⃗\\vec{r^{(i)}}r(i)时，模型的输出向量h(r(i)⃗;θ)h(\\vec{r^{(i)}}; \\theta)h(r(i);θ)就是所有用户对物品iii的评分。 ② 第uuu维就是用户uuu对物品iii的评分。 ③ 遍历输入物品向量，就可以得到用户uuu对所有物品的评分列表。 ④ 把评分列表排序得到推荐列表。 （5）特点 ① 优点：使用了单隐层神经元，具有一定的泛化性。 ② 缺点：因为结构比较简单，所以表达能力不足。 ③ 意义：拉开了使用深度学习的思想解决推荐问题的序幕，为复杂的深度学习网络构建提供了思路。 2. Deep Crossing - 经典结构 （1）介绍 2016年，微软提出了Deep Crossing，是一次深度学习架构在推荐系统中的完整应用。相比AutoRec模型过于简单的网络结构，Deep Crossing模型完整的解决了从特征工程、稀疏向量稠密化、深层神经网络目标优化等一系列问题。微软的应用场景是搜索引擎Bing中的搜索广告推荐场景。目标是在返回的搜索结果中带有相关广告，尽可能增加广告的点击率，准确地预测其点击率，并以此作为广告排序的指标之一。 （2）模型结构 端到端训练需要解决的问题如下： ① 解决离散类特征编码后过于稀疏问题 ② 解决特征自动交叉组合问题 ③ 解决在输出层中完成优化目标的问题。 模型主要由4层组成： ① Embedding层：作用是将稀疏的类别型特征转成稠密向量，结构上主要以全连接层（FC）为主。就Embedding技术本身而言，已经衍生除了Word2vec、Graph Embedding等多种不同的Embedding方法，请参考《深度学习推荐系统-笔记05：Embedding技术》。数值型特征不需要编码，直接进入Stacking层。 ② Stacking层：作用比较简单，把不同的Embedding特征和数值型特征拼接在一起，形成完整的特征向量。结构上通常是concat。 ③ Multiple Residual Units层：作用是对特征向量各个维度进行充分交叉组合，是模型能够挖掘到更多的非线性特征和组合特征的信息，提高模型的表达能力。结构上主要是引入了著名的残差结构（详细可以阅读《详解残差网络-知乎》）的多层感知机。 ④ Scoring层：输出层，作用是拟合优化目标，对于CTR预估这种二分类问题，结构往往使用LR结构；对于类似图像分类等多分类问题，结构往往采用Softmax结构。 （3）损失函数 logloss=−1N∑i=1N(yilog(pi)+(1−yi)log(1−pi))logloss=-\\frac{1}{N}\\sum_{i=1}^{N}(y_ilog(p_i)+(1-y_i)log(1-p_i)) logloss=−N1​i=1∑N​(yi​log(pi​)+(1−yi​)log(1−pi​)) 这个是损失函数是针对二分类的任务，实际任务中可以灵活替换。 （4）特点 ① 基于Embedding+MLP架构设计 ② 特征交叉层复杂度较高 3. NeuralCF - CF与深度学习结合 （1）介绍 CF（协同过滤）发展出了矩阵分解技术，就是把共现矩阵分解为用户向量矩阵和物品向量矩阵。在预测时，计算用户隐向量和物品隐向量的内积，作为用户对物品的评分预测。2017年，新加坡国立大学研究员提出了基于深度学习的协同过滤模型NeuralCF（[Neural Collaborative Filtering](http://184pc128.csie.ntnu.edu.tw/presentation/19-10-18/Neural Collaborative Filtering.pdf)）。 （2）模型结构 左图是矩阵分解技术的网络化表示，右图是NeuralCF结构，很显然区别就是用神经网络代替了矩阵分解中的内积操作（那么原来的内积操作是什么作用？上边已经介绍过了，就是用于计算用户对所有物品的评分列表，然后排序生成推荐列表）。 替换掉内积的好处是什么呢？① 让用户向量和物品向量做更充分的交叉，得到更多的有用价值的特征组合信息。② 引入更多的非线性特征，让模型的表达能力更强。 按照这种思路，用户和物品向量的互操作可以被替换成任意的互操作，这就是“广义矩阵分解”模型。 进一步分析，还可以把多种互操作得到的特征向量拼接起来，再送到输出层进行目标拟合。如上图，用户和物品向量分别经过GMF层和MLP层的互操作之后拼接起来，再送入输出层NeuralCF结构进行目标拟合。这样让模型具有了更强的特征组合和非线性能力。 （3）目标函数 输出层使用softmax激活函数输出多分类的概率分布，在分类问题中，softmax往往和交叉熵损失函数一起使用，因为交叉熵刻画了预测分类与真实结果之间的相似度。先定义似然函数（含义是表示物品iii和用户uuu的相似性）如下： p(Y,Y−∣P,Q,Θf)=∏(u,i)∈Yyui^∏(u,i)∈Y−(1−yuj^)p( \\mathcal{Y}, \\mathcal{Y^-} \\mid \\boldsymbol{P},\\boldsymbol{Q},\\Theta_f)=\\prod_{(u,i)\\in \\mathcal{Y}} \\hat{y_{ui}} \\prod_{(u,i)\\in \\mathcal{Y^-}}(1- \\hat{y_{uj}}) p(Y,Y−∣P,Q,Θf​)=(u,i)∈Y∏​yui​^​(u,i)∈Y−∏​(1−yuj​^​) 其中Y,Y−\\mathcal{Y}, \\mathcal{Y^-}Y,Y−分别表示观测样本集和负样本（未观测到的样本）集合； P,Q\\boldsymbol{P},\\boldsymbol{Q}P,Q分别表示用户和物品隐向量矩阵； Θf\\Theta_fΘf​表示交互操作函数的模型参数 ； yui^\\hat{y_{ui}}yui​^​表示预测输出，yuiy_{ui}yui​是标签值。 因此目标函数就是对上式相似性（概率值）进行等价变换，最小化其负对数，即最小化负对数似然函数，形式如下： L=−∑(u,i)∈Ylog yui^−∑(u,j)∈Y−log (1−yuj^)=−∑(u,i)∈Y ∪Y−yui log yui^+(1−yui) log(1−yui^)L = -\\sum_{(u,i)\\in \\mathcal{Y}}log \\: \\hat{y_{ui}}-\\sum_{(u,j)\\in \\mathcal{Y^-}}log\\:(1 - \\hat{y_{uj}})=-\\sum_{(u,i)\\in \\mathcal{Y}\\,\\cup \\mathcal{Y^-}} y_{ui}\\,log\\, \\hat{y_{ui}}+(1-y_{ui})\\, log(1-\\hat{y_{ui}}) L=−(u,i)∈Y∑​logyui​^​−(u,j)∈Y−∑​log(1−yuj​^​)=−(u,i)∈Y∪Y−∑​yui​logyui​^​+(1−yui​)log(1−yui​^​) （4）特点 ① NeuralCF实际提出了一种模型框架，基于用户和物品向量这个Embedding层，利用不同的互操作进行特征的交叉组合。 ② 实践中并不是模型结构越复杂、特征越多越好。因为过于复杂之后容易出现过拟合和难收敛的问题。 ③ 局限性：NeuralCF基于CF的思想，因此也没有引入更多其他类型的特征，比如用户和物品的属性信息等。 （5）泛化（双塔模型） NeuralCF 的模型结构之中，蕴含了一个非常有价值的思想，就是我们可以把模型分成用户侧模型和物品侧模型两部分，然后用互操作层把这两部分联合起来，产生最后的预测得分。 这里的用户侧模型结构和物品侧模型结构，可以是简单的 Embedding 层，也可以是复杂的神经网络结构，最后的互操作层可以是简单的点积操作，也可以是比较复杂的 MLP 结构。 但只要是这种用户侧模型 + 物品侧模型 + 互操作层的模型结构，我们把它统称为“双塔模型”结构。 2019年，YouTube发表了论文《Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations》，展示了其在构建用于召回层的双塔模型时，分别在用户侧和物品侧输入了多种不同的特征，结构图如下： ① 用户侧特征：包括了用户正在观看的视频 ID、频道 ID（图中的 seed features）、该视频的观看数、被喜欢的次数，以及用户历史观看过的视频 ID 等等 ② 物品侧特征：包括了候选视频的 ID、频道 ID、被观看次数、被喜欢次数等等。 ③ 输出：经过了多层 ReLU 神经网络的学习之后，双塔模型最终通过 softmax 输出层连接两部分，输出最终预测分数。 ④ 优点：易上线、易服务。物品塔和用户塔最顶端的那层神经元，那层神经元的输出其实就是一个全新的物品 Embedding 和用户 Embedding。不用把整个模型都部署上线，只需要预存物品塔和用户塔的输出，以及在线上实现互操作层即可。 （6）总结 ① 能否在用户塔和物品塔中加入context信息？不建议。把context特征放进user塔或者item塔，那么离线生成user embedding或者item embedding的数量就要翻好多倍。 ② 把context信息单独成塔？有点得不偿失，因为线上的context特征，比如时间，地点变化会非常快。所以理想的方式就是做实时inference。 4. PNN （1）介绍 NeuralCF模型的局限性就是只能使用用户和物品向量，如果加入更多组特征向量呢？如何设计特征交互方法？2016年，上海交通大学的研究人员提出的PNN（Product-based Neural Networks）模型，给出了几种设计特征交互方式的思路。 （2）模型结构 相比Deep Crossing模型，PNN模型在在整体结构上没有区别，唯一的不同是PNN模型用乘积层（Product Layer）替代了Deep Crossing的Stacking层。就是Deep Crossing的用最简单的concat拼接Embedding特征和数值型特征，而PNN的思路则是使用Product操作进行两两交互，更有针对性的获取特征之间的交叉信息。 另外，PNN模型的输入不仅包括用户和物品信息，还可以有更多不同形式、不同类别的特征，通过Embedding层统一编码成长度一致的稠密特征向量。 （3）损失函数 同样是二分类的逻辑回归函数。 （4）特征交叉方式 如PNN结构图中Product层分为两部分： ① z部分是直接把Embedding结果线性拼接起来； ② p部分是对各特征向量进行两两的内积操作或者外积操作，因为是两两操作，所以p部分总共有N(N−1)2\\frac{N(N-1)}{2}2N(N−1)​个圆圈（神经元）。 假如p部分是内积操作（称为IPNN模型） 每个圆圈（神经元）是一个标量，和z部分特征向量直接拼接起来输入下一层即可。 假如p部分是外积操作（称为OPNN模型） 每个圆圈（神经元）是一个方阵，因此每个神经元需要搭配一个可学习的参数方阵WWW，两个方阵对应元素相乘之后，再把整个方阵累加成一个标量，剩下的就和内积操作一样了。 但是这样的话，问题的复杂度提升巨大，于是论文介绍了一种降维方法，把所有Embedding向量叠加，然后叠加后的特征向量通过外积互操作得到矩阵p\\boldsymbol{p}p。然后再和L1层的D1D_1D1​个权重方阵对应元素相乘再累加成标量，就得到了一个D1D_1D1​维的输出向量。 但是，并不建议上述降维方法，因为把不同特征向量对应维度累加，容易丢失或模糊很多有价值的信息，比如“年龄”和“地域”两个特征向量不在同一个向量空间中，显然不具备任何可比性，因为不应该直接操作。 （5）特点 ① 强调了特征之间的交叉方式是多样化的，有效的交叉方式会更有利于捕获特征的交叉信息。 ② 局限性：外积操作为了简化操作，对特征进行了无差别的交叉、累加，忽略了原始特征中包含的高价值信息。 5. Wide&amp;Deep - 广泛应用的结构（记忆能力和泛化能力） （1）介绍 2016年，谷歌提出了Wide&amp;Deep模型，由单层的Wide部分和多层的Deep部分组成的混合模型。其影响力一直延续至今，并且衍生出了大量的同结构的混合模型。 （2）模型结构 ① Wide部分位于左侧，结构简单，就是把输入层直接连接到输出层，中间没有做任何处理。作用是：让模型具有较强的“记忆能力”（Memorization）。 ② Deep部分位于右侧，结构稍复杂，它就是 Embedding+MLP 的模型结构，作用是：让模型具有“泛化能力”（Generalization）。 正是这样的结构特点，使模型兼具逻辑回归和深度神经网络的优点：能够快速处理并记忆大量历史行为特征，同时具有强大的表示能力。 （3）记忆能力 ① 概念：模型直接学习历史数据中物品或者特征的“共现频率”，并且把它们直接作为推荐依据的能力。 ② 举例：在电影推荐中可以发现一系列的规则，比如，看了 A 电影的用户经常喜欢看电影 B，这种“因为 A 所以 B”式的规则，非常直接也非常有价值。 ③ 特点：1）数量非常多，一个“记性不好”的推荐模型很难把它们都记住；2）没办法推而广之，因为这类规则非常具体，没办法或者说也没必要跟其他特征做进一步的组合。 ④ 作用：让模型记住大量的直接且重要的规则，这正是单层的线性模型所擅长的。 （4）泛化能力 ① 概念：模型对于新鲜样本、以及从未出现过的特征组合的预测能力。 ② 举例1：假设我们知道 25 岁的男性用户喜欢看电影 A，35 岁的女性用户也喜欢看电影 A。问：“35 岁的男性喜不喜欢看电影 A”，那么： 只有记忆能力的模型回答：不知道，没有学过这个知识。 具有泛化能力的模型回答：从第1条可以学习到男性喜欢电影A，第2条可以学习到35岁喜欢电影A，那综合就是35岁男性喜欢电影A。 ③ 举例2：矩阵分解算法，就是为了解决协同过滤“泛化能力”不强而诞生的。协同过滤只会“死板”地使用用户的原始行为特征，而矩阵分解因为生成了用户和物品的隐向量，所以就可以计算任意两个用户和物品之间的相似度了。 （5）应用场景 ① 右侧Wide部分：这部分很简单，只利用了两个特征的交叉，这两个特征是“已安装应用”和“当前曝光应用”。这样一来，Wide 部分想学到的知识就非常直观啦，就是希望记忆好“如果 A 所以 B”这样的简单规则。在 Google Play 的场景下，就是希望记住“如果用户已经安装了应用 A，是否会安装 B”这样的规则。 ② 左侧Deep部分：是一个非常典型的 Embedding+MLP 结构。其中的输入特征很多，有用户年龄、属性特征、设备类型，还有已安装应用的 Embedding 等等，把这些特征一股脑地放进多层神经网络里面去学习之后，它们互相之间会发生多重的交叉组合，这最终会让模型具备很强的泛化能力。 （6）特点 Wide&amp;Deep成功的关键在于： ① 抓住了业务问题的本质特点，融合了传统模型记忆能力和深度学习模型泛化能力的优势。 ② 模型结构不复杂，易于工程实现、训练和线上服务，这加速了在业界的推广应用。 （7）问答经典 ① 如果把数值型特征放到wide部分，是否需要做normalization？ 做不做normalization还是看你自己的实践。作者一般推荐在实际工作中做normalization或者bucketize。会有助于模型收敛。 ② deep的输出是128，wide是10000，两个不在一个量纲，感觉直接这么concate，会不会削弱deep的效果，模型退化成LR？ 作者认为不会，因为wide部分一般来说都是非常稀疏的特征，虽然总的维度很大，但是在实际inference的过程中，wide部分往往只有几个维度是有值的，而deep部分一般都是稠密的特征向量，所以理论上两个部分对结果的影响不存在太大的bias 6. FM与深度学习结合 （1）介绍 2017年，由哈尔滨工业大学和华为公司联合提出的DeepFM，是将FM结构与Wide&amp;Deep模型进行了整合。用FM替换了原来的Wide部分，加强了浅层网络部分特征组合的能力。 同年，新加坡国立大学研究人员提出了NFM模型，就是把FM结构和深度神经网络结合起来的一种尝试。 （2）DeepFM模型结构 DeepFM 利用了 Wide&amp;Deep 组合模型的思想，用 FM 替换了 Wide&amp;Deep 左边的 Wide 部分，加强了浅层网络部分特征组合的能力，而右边的部分跟 Wide&amp;Deep 的 Deep 部分一样，主要利用多层神经网络进行所有特征的深层处理，最后的输出层是把 FM 部分的输出和 Deep 部分的输出综合起来，产生最后的预估结果。这就是 DeepFM 的结构。 （3）NFM模型结构 ① 特征交叉新方法：元素积操作 Bi-Interaction Pooling Layer 翻译成中文就是“两两特征交叉池化层”。具体操作为： 其中 ⊙ 运算代表两个向量的元素积（Element-wise Product）操作，即两个长度相同的向量对应维相乘得到元素积向量。在进行两两特征 Embedding 向量的元素积操作后，再求取所有交叉特征向量之和，我们就得到了池化层的输出向量。接着，我们再把该向量输入上层的多层全连接神经网络，就能得出最后的预测得分。 ② 特点：1）NFM 并没有使用内积操作来进行特征 Embedding 向量的交叉，而是使用元素积的操作。2）在得到交叉特征向量之后，也没有使用 concatenate 操作把它们连接起来，而是采用了求和的池化操作，把它们叠加起来。 （4）问答经典 ① 数值型特征是否可以用于特征交叉？ 按照DeepFM原论文，数值型特征是不参与特征交叉的，因为特征交叉的操作是在两个embedding向量间进行的。 如果可以把通过分桶操作把连续型特征处理成离散型特征，然后再加Embedding层，就可以让数值型特征也参与特征交叉。这是一个可行的方案。 ② FM与DeepFM交叉权重区别？ 原FM中内积作为权重，然后还要乘以特征本身的值。但在DeepFM中，所有的参与交叉的特征都先转换成了embedding，而且由于是one-hot，所以特征的值就是1，参不参与交叉都无所谓。所以直接使用embedding的内积作为交叉后的值就可以了 ③ FM与MF的区别： 结构上来说，因子分解机会引入除了user id和item id的其他特征，而且FM是有一阶部分的，不只是做特征交叉。 MF就是一个只利用user id和item Id的双塔模型。 MF有多种的求解方式，比如奇异值分解（SVD）、特征值分解（ED）、梯度下降法。 7. 注意力机制与深度学习结合 （1）介绍 “注意力机制”来源于人类天生的“选择性注意”的习惯。最典型的例子是用户在浏览网页时，会有选择性地注意页面的特定区域，而忽视其他区域。 近年来，注意力机制广泛应用在深度学习的各个领域，无论是自然语言处理、语音识别还是计算机视觉领域，注意力模型都取得了巨大的成功。从2017年开始，推荐领域也开始尝试把注意力机制引入推荐模型。 2017年，阿里巴巴提出了深度兴趣网络（Deep Interest Network，DIN）。 2019年，提出了其演进版本深度兴趣进化网络（Deep Interest Evolution Network，DIEN）。 这两个网络都是CTR预估网络。 （2）注意力机制在DIN上的应用 DIN的基础模型结构：是一个典型的 Embedding MLP 的结构。它的输入特征有用户属性特征（User Proflie Features）、用户行为特征（User Behaviors）、候选广告特征（Candidate Ad）和场景特征（Context Features）。 ① 用户行为特征是由一系列用户购买过的商品组成的，也就是图上的 Goods 1 到 Goods N，而每个商品又包含了三个子特征，也就是图中的三个彩色点，其中红色代表商品 ID，蓝色是商铺 ID，粉色是商品类别 ID。同时，候选广告特征也包含了这三个 ID 型的子特征，因为这里的候选广告也是一个阿里平台上的商品。 ② 把三个 ID 转换成了对应的 Embedding，然后把这些 Embedding 连接起来组成了当前商品的 Embedding。 ③ 用户的行为序列是一组商品的序列，这个序列可长可短，但是神经网络的输入向量的维度必须是固定的。SUM Pooling 层的结构直接把这些商品的 Embedding 叠加起来，然后再把叠加后的 Embedding 跟其他所有特征的连接结果输入 MLP。 ④ 问题来了：SUM Pooling 的 Embedding 叠加操作其实是把所有历史行为一视同仁，没有任何重点地加起来，这其实并不符合我们购物的习惯。 ⑤ DIN模型结构图 与 Base Model 相比，DIN 为每个用户的历史购买商品加上了一个激活单元（Activation Unit），这个激活单元生成了一个权重，这个权重就是用户对这个历史商品的注意力得分，权重的大小对应用户注意力的高低。 ⑥ 激活单元：它的输入是当前这个历史行为商品的 Embedding，以及候选广告商品的 Embedding。我们把这两个输入 Embedding，与它们的外积结果连接起来形成一个向量，再输入给激活单元的 MLP 层，最终会生成一个注意力权重，这就是激活单元的结构。简单来说，激活单元就相当于一个小的深度学习模型，它利用两个商品的 Embedding，生成了代表它们关联程度的注意力权重。 （3）兴趣进化序列模型 ① 无论是电商购买行为，还是视频网站的观看行为，或是新闻应用的阅读行为，特定用户的历史行为都是一个随时间排序的序列。既然是和时间相关的序列，就一定存在前后行为的依赖关系，这样的序列信息对于推荐过程是非常有价值的 ② 深度兴趣进化网络 DIEN（Deep Interest Evolution Network）模型正好弥补了 DIN 模型没有对行为序列进行建模的缺陷，它围绕兴趣进化这个点进一步对 DIN 模型做了改进。 ③ DIEN架构：整体上仍然是一个 Embedding MLP 的模型结构。与 DIN 不同的是，DIEN 用“兴趣进化网络”，也就是图中的彩色部分，替换掉了原来带有激活单元的用户历史行为部分。这部分虽然复杂，但它的输出只是一个 h’(T) 的 Embedding 向量，它代表了用户当前的兴趣向量。有了这个兴趣向量之后，再把它与其他特征连接在一起，DIEN 就能通过 MLP 作出最后的预测了 ④ 兴趣进化网络 最下面一层是行为序列层（Behavior Layer，灰绿色部分）。它的主要作用和一个普通的 Embedding 层是一样的，负责把原始的 ID 类行为序列转换成 Embedding 行为序列。 再上一层是兴趣抽取层（Interest Extractor Layer，浅黄色部分）。它的主要作用是利用 GRU 组成的序列模型，来模拟用户兴趣迁移过程，抽取出每个商品节点对应的用户兴趣。 最上面一层是兴趣进化层（Interest Evolving Layer，浅红色部分）。它的主要作用是利用 AUGRU(GRU with Attention Update Gate) 组成的序列模型，在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告（Target Ad）相关的兴趣进化过程，兴趣进化层的最后一个状态的输出就是用户当前的兴趣向量 h’(T)。 ⑤ 序列模型结构 兴趣抽取层和兴趣进化层都用到了序列模型的结构（如下图是RNN模型的典型结构）。 根据序列模型神经元结构的不同，最经典的有RNN、LSTM、GRU这 3 种，各神经元结构图如下： ⑥ 在 DIEN 模型中，神经元的输入就是商品 ID 或者前一层序列模型的 Embedding 向量，而输出就是商品的 Embedding 或者兴趣 Embedding，除此之外，每个神经元还会与后续神经元进行连接，用于预测下一个状态，放到 DIEN 里就是为了预测用户的下一个兴趣。这就是序列模型的结构和作用。 （4）总结 （5）问答经典 ① 有没有更实用的注意力权重计算方式？ 可以借鉴FM及DeepFM中特征交叉的计算方式，对两个向量直接计算内积 先通过Embedding层转换成维度相等的Embedding再求内积 可以像双塔结构一样，设计一个历史行为物品塔和广告物品塔，在塔的最后通过求内积或者拼接后用全连接层输出权重 ② DIN激活单元用到了外积，有什么意图？ 本质上内积和外积都是做特征交叉，计算相似性的方式，一般来说，外积因为输出是一个向量，所以表达能力更强一些 ③ DIN模型在工业界的排序阶段使用的多吗？ DIN比DIEN的使用场景要求低很多，很多团队在用，或者说很多团队在用DIN的思路来构建自己的模型。作者经验：attention机制是非常有价值的 ④ GRU这种提取序列信息的方式肯定会被Transformer取代？ 已经有不少的paper介绍Transformer替代GRU，最近提的比较多的是bert for rec。 ⑤ 广告的内容和商城的内容（包含用户行为）是不一样的，这时候还可以用DIN的注意力机制了吗？ 作者认为可以的，因为广告内容和商城内容从经验上来说也会有一定的相关性。以DeepFM的思路来说，其实可以做任意两个特征之间的交叉。对于注意力机制，当然也可以学任意两个特征间的注意力。但到底作用有多大，能不能提高效果，就看自己的实践了。 ⑥ 推荐模型的NN隐层数量都比较少（相对cv），业界常用的MLP隐层数量和神经元数量，有没有一个大致的取值范围？ 隐层数量大致在1到5之间吧，确实在推荐问题上再提高隐层数量效果上意义不大，inference延迟还大，得不偿失。 相比CV动辄上百的隐层数量，推荐模型是比较“浅”的了。CV的输入特征都是稠密的数值特征，相比推荐来说，特征维度就比较大，所以需要更深的网络来做特征提取。 同时，推荐模型大多数特征都是结构化的数据，因此只能用全连接来对特征进行提取，而过深的全连接层会带来大量的参数，使得模型训练效率低等问题；对于CV来说，其主要考虑的是三维数据的空间特征提取，CNN中卷积核是权重共享的，这就解决全连接层参数爆炸的问题。 ⑦ transformer中有position encoding，而在推荐的领域中，点击的序列中同样有时间间隔的因素，例如取用户最近若干次点击，可能每次间隔时间不等。这个间隔时间应该是有信息可以揭示用户兴趣的变迁速率等信息的吧？但是如何将其引入到推荐序列中呢？是类似于transformer中 position learned encoding这样么？ 这是一个很好的idea。但说实话我还没有见到非常成功的案例说能够很好的利用event interval然后取得很大的收益。也许是这个信号本身不够强，带来的收益有限。 ⑧ 查阅了关于外积的资料，向量外积一般只定义在3维空间，对于高于3维空间的向量如何计算其外积呢？我在DIN作者的开源项目中发现作者自己没有使用外积运算，而是使用了元素相减和元素相乘，不知道老师在DIN或相关实践中是否会使用外积操作来进行向量的融合呢？ 我也不喜欢用外积操作，我比较喜欢inner product和element wise操作。外积我始终没法得到更好的效果。DIN的paper中加入了外积操作，也许身体是诚实的，还是用了传统的交互操作。没必要纠结这些，自己实践中哪个好用用哪个。 8. 强化学习与深度学习结合 （1）背景 强化学习也被称为增强学习，它在模型实时更新、用户行为快速反馈等方向上拥有巨大的优势。自从 2018 年开始，它就被大量应用在了推荐系统中，短短几年时间内，微软、美团、阿里等多家一线公司都已经有了强化学习的成功应用案例。 （2）基本概念 基本原理：就是一个智能体通过与环境进行交互，不断学习强化自己的智力，来指导自己的下一步行动，以取得最大化的预期利益。 为了把强化学习技术落地，只清楚它的基本原理显然是不够的，我们需要清晰地定义出强化学习中的每个关键变量，形成一套通用的技术框架。对于一个通用的强化学习框架来说，有这么六个元素是必须要有的： 智能体（Agent）：强化学习的主体也就是作出决定的“大脑”； 环境（Environment）：智能体所在的环境，智能体交互的对象； 行动（Action）：由智能体做出的行动； 奖励（Reward）：智能体作出行动后，该行动带来的奖励； 状态（State）：智能体自身当前所处的状态； 目标（Objective）：指智能体希望达成的目标。 通用过程描述：一个智能体身处在不断变化的环境之中，为了达成某个目标，它需要不断作出行动，行动会带来好或者不好的奖励，智能体收集起这些奖励反馈进行自我学习，改变自己所处的状态，再进行下一步的行动，然后智能体会持续这个“行动 - 奖励 - 更新状态”的循环，不断优化自身，直到达成设定的目标。 （3）强化学习推荐系统框架 ① 深度强化学习网络DRN（Deep Reinforcement Learning Network），是微软在 2018 年提出的，它被应用在了新闻推荐的场景上。深度强化学习推荐系统框架图如下： ② DRN学习过程： 第一步是初始化推荐系统，主要初始化的是推荐模型，我们可以利用离线训练好的模型作为初始化模型，其他的还包括我们之前讲过的特征存储、推荐服务器等等。 接下来，推荐系统作为智能体会根据当前已收集的用户行为数据，也就是当前的状态，对新闻进行排序这样的行动，并在新闻网站或者 App 这些环境中推送给用户。 用户收到新闻推荐列表之后，可能会产生点击或者忽略推荐结果的反馈。这些反馈都会作为正向或者负向奖励再反馈给推荐系统。 推荐系统收到奖励之后，会根据它改变、更新当前的状态，并进行模型训练来更新模型。接着，就是推荐系统不断重复“排序 - 推送 - 反馈”的步骤，直到达成提高新闻的整体点击率或者用户留存等目的为止。 ③ 强化学习六要素与推荐系统的关系总结： ④ 强化学习推荐系统的特点：始终在强调持续学习和实时训练。它不断利用新学到的知识更新自己，做出最及时的调整，这也正是将强化学习应用于推荐系统的收益所在。 （4）深度强化学习推荐模型 DRN ① 智能体是强化学习框架的核心，作为推荐系统这一智能体来说，推荐模型就是推荐系统的“大脑”。在 DRN 框架中，扮演“大脑”角色的是 Deep Q-Network (深度 Q 网络，DQN)。其中，Q 是 Quality 的简称，指通过对行动进行质量评估，得到行动的效用得分，来进行行动决策。 ② DQN模型结构：如下图所示，它就是一个典型的双塔结构。用户塔的输入特征是用户特征和环境特征，物品塔的输入向量是所有的用户、环境、用户 - 新闻交叉特征和新闻特征。 ③ 在强化学习的框架下，用户塔特征向量因为代表了用户当前所处的状态，所以也可被视为状态向量。物品塔特征向量则代表了系统下一步要选择的新闻，这个选择新闻的过程就是智能体的“行动”，所以物品塔特征向量也被称为行动向量。 ④ 双塔模型通过对状态向量和行动向量分别进行 MLP 处理，再用互操作层生成了最终的行动质量得分 Q(s,a)，智能体正是通过这一得分的高低，来选择到底做出哪些行动，也就是推荐哪些新闻给用户的。 ⑤ DRN学习过程 离线部分初始化：DRN 根据历史数据训练好 DQN 模型，作为智能体的初始化模型。 在线t1-&gt;t2时间段：DRN 利用初始化模型进行一段时间的推送服务，积累反馈数据 t2时间点：DRN 利用 t1 到 t2 阶段积累的用户点击数据，进行模型微更新（Minor update），基于新的在线训练方法，Dueling Bandit Gradient Descent algorithm（竞争梯度下降算法） t4时间点：DRN 利用 t1 到 t4 阶段的用户点击数据及用户活跃度数据，进行模型的主更新（Major update），可以理解为：利用历史数据的重新训练，用训练好的模型来替代现有模型。 重复t1-&gt;t4阶段的操作 ⑥ 在线学习方法：竞争梯度下降算法 主要包括三步： 第一步：对于已经训练好的当前网络 Q，对其模型参数 W 添加一个较小的随机扰动，得到一个新的模型参数，这里我们称对应的网络为探索网络 Q~，产生随机扰动公式如下： ΔW=α⋅rand(−1,1)⋅WΔW=α·rand(-1, 1) · WΔW=α⋅rand(−1,1)⋅W $ α$ 是一个探索因子，决定探索力度的大小。$rand(-1,1) 产生的是一个产生的是一个产生的是一个[-1,1]$之间的随机数。 第二步：对于当前网络 Q 和探索网络 Q~，分别生成推荐列表 L 和 L~，再将两个推荐列表用间隔穿插（Interleaving）的方式融合，组合成一个推荐列表后推送给用户。 最后一步是实时收集用户反馈。如果探索网络 Q～生成内容的效果好于当前网络 Q，我们就用探索网络代替当前网络，进入下一轮迭代。反之，我们就保留当前网络。 （5）DRN的改进 最大的改进就是把模型推断、模型更新、推荐系统工程整个一体化了，让整个模型学习的过程变得更高效，能根据用户的实时奖励学到新知识，做出最实时的反馈。 但同时，也正是因为工程和模型紧紧地耦合在一起，让强化学习在推荐系统中的落地并不容易。 （6）问答经典 ① 竞争梯度下降算法的弊端是什么？ 每个参数的更新方向是随机的，而不是像随机下降算法一样，是沿着梯度更新的。随机更新可能导致的结果就是：1.收敛是缓慢的。2.很难收敛到全局最优值。 ② 如何判断竞争网络效果? 在实际实现中做一小段实践的数据收集，再根据这个batch的效果进行探索网络和当前网络的选择。这部分在实际的工程中一般是在flink等流计算平台上实现的。 ③ 历史数据重新训练指的是从零训练吗？使用竞争梯度下降算法吗？ 主更新不使用竞争梯度下降算法。实际工程中可视为一次正常的模型更新，使用的是全量历史样本，当然这其中包括了最近收集到的样本。 ④ 增加实时性上采用FTRL进行在线学习和强化学习在最终结果上有什么区别，如何判断选择？ online learning其实可以看作强化学习的一个子集，或者说是子类。所以FTRL做在线学习本质上就是强化学习的一种。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"推荐模型","slug":"推荐模型","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"深度学习推荐系统-笔记09：深度学习为推荐系统带来了革命性影响","slug":"RecSys/4.推荐模型/[深度学习推荐系统-笔记09]深度学习推荐模型发展脉络","date":"2021-01-05T13:33:40.000Z","updated":"2021-03-18T15:04:38.578Z","comments":true,"path":"posts/afc2d21f.html","link":"","permalink":"https://www.notlate.net/posts/afc2d21f.html","excerpt":"","text":"1. 深度学习模型拟合能力更强 特征交叉方式中，点积等方式过于简单，在样本数据比较复杂的情况下，容易欠拟合。而深度学习可以大大提高模型的拟合能力，比如在 NeuralCF（神经网络协同过滤）模型中，点积层被替换为多层神经网络，理论上多层神经网络具备拟合任意函数的能力，所以我们通过增加神经网络层的方式就可以解决欠拟合的问题了。 2. 深度学习模型结构更加灵活 （1）深度学习模型结构不尽相同，多数是可以通过堆叠不同作用的网络层，最简单的是串联结构，有的像网状结构，有的像金字塔结构等等。 （2）典型案例是阿里巴巴的 DIN（深度兴趣网络，下图左）和 DIEN（深度兴趣进化网络，下图右），通过在模型结构中引入注意力机制和模拟兴趣进化的序列模型，来更好地模拟用户行为。 其中，DIN 模型在神经网络中增加了一个“激活单元“结构，是为了模仿人类的注意力机制。其改进版 DIEN 模型不仅引入了注意力机制，还用AUGRU单元模拟了用户兴趣随时间的演化过程。 这些改进都是基于实际业务洞察分析的演进，所以需要正确、全面地掌握不同深度学习模型的特点以及发展关系非常重要。 3. 深度学习模型发展关系图 （1）核心结构：多层感知机（MultiLayer Perception，MLP）。 （2）基础结构：AutoRec，一种单隐层的神经网络模型，将自编码器（AutoEncoder）的思想与协同过滤结合。 （3）经典结构：Deep Crossing，在原始特征和MLP之间加入了Embedding层，把输入的稀疏特征先转换成稠密 Embedding 向量，再输入到MLP进行训练，这就解决了MLP不善于处理稀疏特征的问题。因此Embedding+MLP结构是最经典的深度学习推荐模型结构。 （4）广泛应用结构：Wide&amp;Deep，模型分为两部分：Wide部分是浅层的神经网络结构，让模型具备很好的记忆性；Deep部分是深层MLP，让模型具备良好的泛化性，最终把两者结合起来。凭借着易实现、易改造的特点，获得了业界广泛应用。同时还衍生出了诸多变种，比如通过改造 Wide 部分提出的Deep&amp;Cross和DeepFM，通过改造Deep部分提出的AFM、NFM等等。 （5）与其他机器学习子领域交叉： ① 深度学习和注意力机制结合，比如阿里的DIN，浙大和新加坡国立提出的AFM等； ② 把序列模型引入Embedding+MLP的经典结构，比如阿里的DIEN等； ③ 深度学习和强化学习结合，比如微软的DRN（深度强化学习网络），以及包括美团-猜你喜欢、阿里-强化学习在阿里的技术演进与业务创新在内的非常有价值的业界应用。 4. 演进规律 （1）改变神经网络的复杂程度 从最简单的单层神经网络模型 AutoRec，到经典的深度神经网络结构 Deep Crossing，它们主要的进化方式在于增加了深度神经网络的层数和结构复杂度。 （2）改变特征交叉方式 这种演进方式的要点在于大大提高了深度学习网络中特征交叉的能力。比如改变了用户向量和物品向量互操作方式的NeuralCF，定义了多种特征向量交叉操作的 PNN 等等。 （3）把多种模型组合应用 组合模型主要指的就是以 Wide&amp;Deep 模型为代表的一系列把不同结构组合在一起的改进思路。它通过组合两种甚至多种不同特点、优势互补的深度学习网络，来提升模型的综合能力。 （4）让深度推荐模型和其他领域进行交叉 我们从 DIN、DIEN、DRN 等模型中可以看出，深度推荐模型无时无刻不在从其他研究领域汲取新的知识。从今年的推荐系统顶会 Recsys2020 中可以看到，NLP 领域的著名模型 Bert 又与推荐模型结合起来，并且产生了非常好的效果。一般来说，自然语言处理、图像处理、强化学习这些领域都是推荐系统经常汲取新知识的地方。 下一篇详细记录深度学习模型。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://www.notlate.net/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"深度学习推荐系统-笔记08：传统推荐算法发展汇总","slug":"RecSys/4.推荐模型/[深度学习推荐系统-笔记08]传统推荐算法","date":"2021-01-05T13:23:00.000Z","updated":"2021-03-18T15:04:38.578Z","comments":true,"path":"posts/d6641ee8.html","link":"","permalink":"https://www.notlate.net/posts/d6641ee8.html","excerpt":"","text":"1. 协同过滤(CF) 1. 里程碑 2003年，Amazon发表论文《Amazon.com recommendations: item-to-item collaborative filtering》，不仅让Amazon的推荐系统广为人知，更让协同过滤成为今后很长时间的研究热点和业界主流的推荐模型。 2. 算法过程 （1）一种完全依赖用户和物品之间行为关系的推荐算法，利用所有用户与所有商品的关系创建共现矩阵。 （2）计算用户相似度：在共现矩阵中，每个用户对应的行向量其实就可以当作一个用户的 Embedding 向量，可以使用余弦相似度计算。 （3）用户评分预测：利用用户相似度与相似用户评价的加权平均值，来获得目标用户的评价预测 3. 缺点 共现矩阵往往非常稀疏 2. 矩阵分解 1. 里程碑 2006年，在Netflix Prize Challenge中，以矩阵分解为主的推荐算法大放异彩，拉开了矩阵分解在业界流行的序幕。 2. 协同过滤与矩阵分解区别 （1）协同过滤算法(图左)找到用户可能喜欢的视频的方式很直观，就是利用用户的观看历史，找到跟目标用户 Joe 看过同样视频的相似用户，然后找到这些相似用户喜欢看的其他视频，推荐给目标用户 Joe （2）矩阵分解算法(图右)是期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点接近，在推荐过程中，我们就应该把距离相近的视频推荐给目标用户 3. 矩阵分解过程 先分解协同过滤生成的共现矩阵，生成用户和物品的隐向量，再通过用户和物品隐向量间的相似性进行推荐。把一个M∗NM * NM∗N的共现矩阵，分解成一个M∗KM*KM∗K的用户矩阵和K∗NK*NK∗N 的物品矩阵相乘的形式。 4. 分解方法 （1）特征值分解（ED）：只能作用于方针，不适用于分解用户-物品矩阵 （2）奇异值分解（SVD）：要求原始的共现矩阵是稠密的，而且计算复杂度达到了O(mn2)O(mn^2)O(mn2)，因此不满足实时性要求 （3）梯度下降法（GD）：主要方法，为了减少过拟合现象，通常加入正则化项。 5. 矩阵分解优点： （1）泛化能力强，在一定程度上解决了数据稀疏的问题 （2）空间复杂度低，不需要存储大量的用户和物品相似性矩阵，只需要存储用户和物品隐向量即可。 （3）更好的扩展性和灵活性：矩阵分解结果是用户和物品隐向量，与Embedding思想不谋而合，因此其结果易于与其他特征交叉组合，也能够与深度学习网络无缝结合。 6. 矩阵分解局限性 不方便加入用户、物品和上下文相关的特征，丧失了利用很多有效信息的机会。同时在缺乏用户历史行为时无法进行有效推荐。 7. 问答经典 评分标准不一致，有人喜欢打高分，有人喜欢打低分，如何处理？ （1）在生成共现矩阵的时候对用户的评分进行用户级别的校正或者归一化，用当前得分减去用户平均得分作为共现矩阵里面的值 （2）可以尝试余弦相似度消除影响。 正负样本不平衡问题怎么处理？ （1）负样本欠采样，和正样本过采样，或者增大正样本学习的权重。 （2）SMOTE，大致意思是通过合成的方式过采样正样本，可以尝试但有一定风险。 3. 逻辑回归（LR） 1. 简介 LR模型可以综合利用用户、物品、上下文等多种不同的特征。另外LR的另外一种表现形式“感知机”是神经网络中最基础的单一神经元结构，是深度学习的基础结构。LR把推荐问题看作分类问题，通过预测正样本的概率对物品进行排序，因此转化成了点击率（CTR）预估问题。 2. 推荐流程 （1）将用户属性信息、物品属性信息、上下文属性信息等特征展开并转换成数值型特征向量。 （2）确定LR模型的优化目标，利用已有样本数据对LR模型进行训练。 （3）模型服务阶段，将特征向量输入LR模型，推断得到目标概率。 （4）利用概率对所有候选物品进行排序，得到推荐列表。 3. 具体LR模型数学表达及训练方法，请参考： 深度学习(DL)与卷积神经网络(CNN)学习笔记随笔-03-基于Python的LeNet之LR 4. LR模型的优势 （1）可解释性强。LR模型数学形式是各特征的加权和，在经过Sigmoid函数计算出0~1之间的概率，符合人类对预估过程的直觉人知。 （2）工程化需要。在互联网公司每天数据量是TB级别，模型的训练和在线推断效率非常重要。在尚未流行GPU训练（2012年）之前，LR模型凭借易于并行化、模型简单、训练开销小等优势占据着工程领域的主流。 5. LR模型局限性 表达能力不强，无法进行多特征交叉、特征筛选等复杂操作； 4. 因子分解机(FM) 1. 为什么需要多特征交叉？ 辛普森悖论 – 交叉与不交叉的结论完全相悖 2. 特征交叉发展里程碑 POLY2–&gt;FM–&gt;FFM 3. POLY2模型 原始的特征交叉方法是人工组合特征，再通过各种分析手段筛选有效的组合特征。因此首先发展出了POLY2（Degree-2 Polynomial Margin，论文链接）模型进行特征的“暴力”组合。数学形式如下： POLY2(w,x)=w0+∑i=1nwixi+∑j1=1n∑j2=j1+1nwh(j1,j2)xj1xj2POLY2(w, x) = w_0 + \\sum^{n}_{i=1} w_i x_i + \\sum^{n}_{j_1 = 1} \\sum^{n}_{j_2 = j_1 + 1} w_{h(j_1, j_2)} x_{j_1} x_{j_2} POLY2(w,x)=w0​+i=1∑n​wi​xi​+j1​=1∑n​j2​=j1​+1∑n​wh(j1​,j2​)​xj1​​xj2​​ 其中nnn表示特征数量，xix_ixi​表示one-hot编码中第iii个特征的值，www等是模型参数。 当xj1x_{j_1}xj1​​和xj2x_{j_2}xj2​​两者都非零时，交叉特征xj1xj2x_{j_1} x_{j_2}xj1​​xj2​​才有效。上式对特征进行了两两交叉（特征xj1x_{j_1}xj1​​和xj2x_{j_2}xj2​​），并对所有的交叉特征赋予权重wh(j1,j2)w_{h(j_1, j_2)}wh(j1​,j2​)​。 在一定程度上解决了特征交叉的问题，但本质上还是线性模型，其训练方法与LR无区别。同时存在2个较大缺陷： （1）one-hot编码处理类别型数据导致特征向量稀疏，POLY2进行无选择的特征交叉会导致特征向量更加稀疏。 （2）权重参数数量由nnn上升到了n2n^2n2，极大的增加了训练复杂度。 4. FM模型 2010年，Rendle提出了FM模型（Factorization Machines），FM为每个特征学习一个隐权重向量，在特征交叉的时候，使用两个特征隐向量的内积作为交叉特征的权重，而不是PLOY2中为每个交叉特征设置一个权重参数，数学形式如下： FM(w,x)=w0+∑i=1nwixi+∑j1=1n∑j2=j1+1n(wj1⃗⋅wj2⃗)xj1xj2FM(w, x) = w_0 + \\sum^{n}_{i=1} w_i x_i + \\sum^{n}_{j_1 = 1} \\sum^{n}_{j_2 = j_1 + 1} (\\vec{w_{j_1}} \\cdot \\vec{w_{j_2}}) x_{j_1} x_{j_2} FM(w,x)=w0​+i=1∑n​wi​xi​+j1​=1∑n​j2​=j1​+1∑n​(wj1​​​⋅wj2​​​)xj1​​xj2​​ 与POLY2的主要区别是：用两个向量的内积(wj1⃗⋅wj2⃗)(\\vec{w_{j_1}} \\cdot \\vec{w_{j_2}})(wj1​​​⋅wj2​​​)取代了单一的权重系数wh(j1,j2)w_{h(j_1, j_2)}wh(j1​,j2​)​。FM引入隐向量的思路与矩阵分解出用户隐向量和物品隐向量思路一致，但是做了进一步的拓展，即从单纯的用户、物品两特征拓展到了所有的特征。 优点： （1）把POLY2的n2n^2n2级别参数量降低到了nk(k是隐向量维度，n&gt;&gt;k)nk(k是隐向量维度，n&gt;&gt;k)nk(k是隐向量维度，n&gt;&gt;k)级别，大大降低了训练开销。 （2）引入隐向量，可以更好的解决稀疏数据问题。可以把两种特征的向量内积作为交叉特征的权重，而不需要样本中必须含有这种交叉特征。 （3）一定程度上丢失了某些具体交叉特征的精确记忆表示能力，但是大大提高了泛化能力。 由于FM同样可以用梯度下降方法进行训练，其二阶表达式经过化简之后，预测复杂度可从O(kn2)O(kn^2)O(kn2)优化到O(kn)O(kn)O(kn)，即可以在线性时间内完成线上预测。同时计算完梯度后的表达式复杂度是O(kn)O(kn)O(kn)，因此其训练也非常高效。所以凭借着易于训练和预测，所以在2012~2014年前后成为业界主流的推荐模型之一。 5. FFM 2015年，基于FM提出的FFM（Field-aware Factorization Machines for CTR Prediction）在多项CTR预估大赛中夺冠，并被Criteo、美团等公司深度应用在推荐系统、CTR预估等领域。FFM引入了特征域概念，使模型的表达能力更强。那怎么理解特征域呢？ 《深度学习推荐系统》中的示例： 用户的性别分为男、女、未知三类，那么对一个女性用户来说，采用one-hot方式编码的特征向量为[0, 1, 0]，这个三维的特征向量就是一个“性别”特征域。 《深入FFM原理与实践》中的解释： 以上面的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。 我的个人理解是：经过one-hot编码的某类特征的特征值的集合称为特征域。 FFM的数学形式如下： FFM(w,x)=w0+∑i=1nwixi+∑j1=1n∑j2=j1+1n(wj1,fj2⃗⋅wj2,fj1⃗)xj1xj2FFM(w, x) = w_0 + \\sum^{n}_{i=1} w_i x_i + \\sum^{n}_{j_1 = 1} \\sum^{n}_{j_2 = j_1 + 1} (\\vec{w_{j_1, f_{j_2}}} \\cdot \\vec{w_{j_2, f_{j_1}}}) x_{j_1} x_{j_2} FFM(w,x)=w0​+i=1∑n​wi​xi​+j1​=1∑n​j2​=j1​+1∑n​(wj1​,fj2​​​​⋅wj2​,fj1​​​​)xj1​​xj2​​ 其中： （1）fjf_jfj​表示第jjj个特征所属的特征域； （2）wj1,fj2⃗\\vec{w_{j_1, f_{j_2}}}wj1​,fj2​​​​表示特征xj1x_{j_1}xj1​​与特征域fj2f_{j_2}fj2​​对应的隐向量，同理wj2,fj1⃗\\vec{w_{j_2, f_{j_1}}}wj2​,fj1​​​​表示特征xj2x_{j_2}xj2​​与特征域fj1f_{j_1}fj1​​对应的隐向量。 因此FFM与FM的区别是隐特征向量由wj1⃗\\vec{w_{j_1}}wj1​​​变成了wj1,fj2⃗\\vec{w_{j_1, f_{j_2}}}wj1​,fj2​​​​，意味着每种特征都会针对其他特征域学习一个隐向量。当xj1x_{j_1}xj1​​和xj2x_{j_2}xj2​​两种特征进行交叉时，xj1x_{j_1}xj1​​挑出与xj2x_{j_2}xj2​​所属特征域fj2f_{j_2}fj2​​的隐向量wj1,fj2⃗\\vec{w_{j_1, f_{j_2}}}wj1​,fj2​​​​与xj2x_{j_2}xj2​​挑出与xj1x_{j_1}xj1​​所属特征域fj1f_{j_1}fj1​​的隐向量wj2,fj1⃗\\vec{w_{j_2, f_{j_1}}}wj2​,fj1​​​​进行交叉计算。 FFM模型在训练时，需要学习nnn个特征在fff个域上的kkk维隐向量，则二阶参数量有n⋅f⋅kn \\cdot f \\cdot kn⋅f⋅k个，且由于隐向量和域相关，FFM的二阶部分不能化简，所以预测复杂度是O(kn2)O(kn^2)O(kn2)。 FFM实际特征交叉举例： 5. GBDT+LR 1. 简介 FFM模型增加了特征交叉能力，但是只能做二阶特征交叉，若提高特征交叉的维度，则会产生组合爆炸和计算复杂度过高的问题。2014年，Facebook提出了基于GBDT+LR组合模型的解决方案，其中用GBDT自动进行特征筛选和组合，再把特征向量输入LR模型，这两步是独立训练的，所以不需要把LR的梯度回传到GBDT。 2. GBDT是什么 基本结构：决策树组成的树林，通过逐一生成决策子树生成整个树林。 生成新子树过程：利用样本标签值与当前树林预测值之间的残差。 学习方式：梯度提升。 预测方式：把所有子树结果加起来。 理论上，若可以无限生成决策树，则GBDT可以无限逼近目标拟合函数，从而达到减小预测误差的目的。 3. GBDT的特征转换过程 GBDT模型训练好之后，输入一个样本，根据每个节点的规则最终落入某一叶子节点，则该节点置为1，其他叶子节点置为0。所有叶子节点组成的向量即形成了该棵树的特征向量。把GBDT所有子树的特征向量连接起来，即形成了该样本的离散型特征向量。 以上图为例，若输入一个样本，分别落在了三颗子树的3，1，4叶节点，则最终的样本特征向量是：[0,0,1,0, 1,0,0,0, 0,0,0,1]。 4. 特点总结 （1）决策树的深度决定了特征交叉的阶数，比FM具有更强的特征交叉能力。 （2）GBDT容易产生过拟合，且容易丢失特征的数值信息。 （3）因此特征交叉能力强不意味着效果就一定会好。 （4）组合模型的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入是原始的特征向量，不需要投入过多人工筛选和模型设计的精力，真正的实现端到端训练。 6. 大规模分段线性模型(LS-PLM） 1. 简介 LS-PLM（Large Scale Piece-wise Linear Model），早在2012年就成为了阿里巴巴主流的推荐模型，主要应用于各类广告推荐场景，在2017年公之于众。 2. 模型结构 LS-PLM，结构与三层神经网络相似，在LR的基础上采用分而治之的思想，先对样本进行聚类分类，再对分类的样本使用LR进行CTR预估。因此本质上可以看成是对LR的推广，所以又被称为MLR（Mixed Logistic Regression，混合逻辑回归）模型。比如女装广告的CTR预估不希望把男性用户点击数码类产品的样本数据纳入训练集。 3. 原理 LS-PLM的数学形式如下： f(x)=∑i=1mπi(x)⋅ηi(x)=∑i=1meμi⋅x∑j=1meμj⋅x⋅11+e−wj⋅xf(x) = \\sum^{m}_{i=1} \\pi_{i}(x) \\cdot \\eta_{i}(x) = \\sum^{m}_{i=1}{\\frac{e^{\\mu_i \\cdot x}}{\\sum^{m}_{j=1} e^{\\mu_j \\cdot x}} \\cdot \\frac{1}{1 + e^{-w_j \\cdot x}}} f(x)=i=1∑m​πi​(x)⋅ηi​(x)=i=1∑m​∑j=1m​eμj​⋅xeμi​⋅x​⋅1+e−wj​⋅x1​ 其中π\\piπ为聚类函数，采用了softmaxsoftmaxsoftmax函数对样本进行多分类。mmm为超参数，表示分类数，可以较好的平衡模型的拟合与推广能力。当m=1m=1m=1时，LS-PLM退化为LR。mmm越大，表示能力越精确，拟合能力越强，但是参数量越大。 在每个分类内部构件LR模型，将每个样本的各个分类概率与LR的得分进行加权平均，得到最终的预估值。 4. 模型优点 （1）端到端的非线性学习能力。LS-PLM的分类能力使得模型可以提取样本中的非线性特征，省去了大量的人工样本处理和特征工程的过程，可以完成端到端的训练。 （2）模型的稀疏性强。LS-PLM建模时引入了L1和L2范数，使得最终训练完的模型具有较高的稀疏性，因此部署时更加轻量级。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 《深度学习推荐系统》 – 王喆 深入FFM原理与实践 – 美团技术团队 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"传统算法","slug":"传统算法","permalink":"https://www.notlate.net/tags/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"}]},{"title":"深度学习推荐系统-笔记07：推荐架构、特征提取、线上服务融会贯通","slug":"RecSys/3.线上服务/[深度学习推荐系统-笔记07]融会贯通","date":"2021-01-05T13:13:40.000Z","updated":"2021-03-18T15:04:38.506Z","comments":true,"path":"posts/4fddc052.html","link":"","permalink":"https://www.notlate.net/posts/4fddc052.html","excerpt":"","text":"基础架构、特征工程、线上服务融会贯通 一张脑图比较大，加载比较慢。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[]},{"title":"深度学习推荐系统-笔记06：推荐系统线上服务相关","slug":"RecSys/3.线上服务/[深度学习推荐系统-笔记06]推荐服务流程","date":"2021-01-05T13:12:40.000Z","updated":"2021-03-18T15:04:38.506Z","comments":true,"path":"posts/9eb245b5.html","link":"","permalink":"https://www.notlate.net/posts/9eb245b5.html","excerpt":"","text":"1. 高并发线上服务 1. 工业级推荐服务器功能 需要跟离线训练好的模型打交道，把离线模型进行上线，在线进行模型服务（Model Serving）， 需要跟数据库打交道，把候选物品和离线处理好的特征载入到服务器 召回层、排序层、业务逻辑(结果多样性、流行度) AB 测试 2. 高并发推荐服务整体架构 负载均衡：nginx或专门的硬件级负载均衡设备 负载均衡策略： 如果硬件配置基本一样且部署服务一样，就采用轮询或者随机的负载均衡策略 如果硬件配置不同，可以根据硬件的计算能力做加权的负载均衡策略 同样也可以利用源地址hash做策略 关于扩容和缩容：可以根据系统负载情况做动态调整 缓存 (1) 预先缓存好几类新用户的推荐列表 (2) 利用新用户有限的信息，比如ip，注册信息等做一个简单的聚类，为每个人群聚类产生合适的推荐列表提前缓存 (3) 缓存有TTL过期时间 推荐服务降级机制 (1) 抛弃原本的复杂逻辑，采用最保险、最简单、最不消耗资源的降级服务来渡过特殊时期 (2) 要有成熟的监控系统 2. 推荐特征存储 存储模块设计原则：分级存储，把越频繁访问的数据放到越快的数据库甚至缓存中，把海量的全量数据放到廉价但是查询速度较慢的数据库中。 存储示例： Redis使用经验： （1）redis keys命令不能用在生产环境中，如果数量过大效率十分低，导致redis长时间堵塞在keys上。生产环境我们一般选择提前载入一些warm up物品id的方式载入物品embedding （2）Redis value 可以用protobuf格式存储, 存储上节省空间. 解析起来相比string, cpu的效率也应该会更高 （3）把item embedding提前加载到内存里 （4）关于user embedding，指定一个内存区域的大小，用FIFO的方案来缓存，这样内存用完了，就自动把早进来的用户pop出去 （5）如果有条件可以判断活跃用户，可以尽量选择活跃用户进行缓存 3. 召回层 1. 作用 快速又准确地筛选掉不相关物品，从而节约排序时所消耗的计算资源。 2. 与排序层的比较 3. 指标 计算速度 召回率 4. 技术 单策略召回 （1）概念：通过制定一条规则或者利用一个简单模型来快速地召回可能的相关物品。规则其实就是用户可能感兴趣的物品的特点。 （2）优点：简单直观，计算速度非常快 （3）缺点：局限性很大 多路召回 （1）概念：采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用的策略 （2）优点：平衡计算速度与召回率 （3）缺点：在确定每一路的召回物品数量时，往往需要大量的人工参与和调整，具体的数值需要经过大量线上 AB 测试来决定。同时，策略之间的信息和数据是割裂的，所以我们很难综合考虑不同策略对一个物品的影响 基于Embedding的召回 （1）概念：利用物品和用户 Embedding 相似性来构建召回层 （2）优点： 多路召回中使用的“兴趣标签”“热门度”“流行趋势”“物品属性”等信息都可以作为 Embedding 方法中的附加信息（Side Information），融合进最终的 Embedding 向量中，相当于考虑到了多路召回的多种策略。 Embedding 召回的评分具有连续性，可以把 相似度作为唯一的判断标准，因此它可以随意限定召回的候选集大小。 在线上服务的过程中，Embedding 相似性的计算也相对简单和直接。通过简单的点积或余弦相似度的运算就能够得到相似度得分，便于线上的快速召回。 横向比较三种技术 5. 召回层经典问答 电商领域商品维度非常大，EGES训练过慢，怎么办？ EGES 指的是阿里提出的一种 Graph Embedidng 方法，全称是 Enhanced Graph Embedding with Side Information，补充信息增强图 Embedding。它是一种融合了经典的 Deep Walk Graph Embedding 结果和其他特征的 Embedding 方法，具体步骤如下： （1）把商品embedding进行预训练，再跟其他side information特征一起输入EGES。 （2）hash方法 （3）商品的聚类后输入，比如非常类似的商品，可以用一个商品id替代，当作一个商品来处理。这个方法airbnb embedding的论文讲的非常好。 用户Embedding怎么计算的？ 最简单的user embedding生成方法。之前我们说过embedding之间是可以进行运算的。所以用用户喜欢的物品的embedding平均去代表这个用户是非常直观且实用 多路召回中，topk除了根据经验值确定，业界通用的是怎么确定k得大小呢？ 在系统延迟允许的情况下，其实k取的越大越好。一般来说，如果最后的推荐结果需要n条，k取5-10n是比较合适的。 如果基于兴趣标签做召回，同一个物品有多个标签，用户也计算出了多个兴趣标签，如何做用户的多兴趣标签与物品的最优匹配呢？若物品标签有多层，怎么利用上一层的标签呢？ （1）简单做法：把兴趣标签转成MultiHot向量，然后计算用户和物品的相似度。 （2）复杂一点：计算每个兴趣标签的TF-IDF，为标签分配权重后，再转成MultiHot向量。 （3）若标签有多层，不妨把多层标签全部放到MultiHot向量中，高层标签的权重可以适当降低。 4. 局部敏感哈希(LSH) 1. 思想 召回与用户向量最相似的物品 Embedding 向量这一问题，其实就是在向量空间内搜索最近邻的过程。 2. 如何搜索最近邻？ 聚类 （1）常见方法：K-means等 （2）存在的问题： 聚类边缘的点的最近邻往往会包括相邻聚类的点，如果我们只在类别内搜索，就会遗漏这些近似点 中心点的数量 k 也不那么好确定，k 选得太大，离线迭代的过程就会非常慢，k 选得太小，在线搜索的范围还是很大，并没有减少太多搜索时间 索引 （1）实现方法：向量空间索引方法 Kd-tree（K-dimension tree） 先用红色的线把点云一分为二，再用深蓝色的线把各自片区的点云一分为二，以此类推，直到每个片区只剩下一个点，这就完成了空间索引的构建。 （2）存在的问题： 会遗漏掉最近邻点，它只能保证快速搜索到近似的最近邻点集合 Kd-tree 索引的结构并不简单，离线和在线维护的过程也相对复杂，这些都是它的弊端 局部敏感哈希和多桶 （1）基本思想：希望让相邻的点落入同一个“桶”，这样在进行最近邻搜索时，我们仅需要在一个桶内，或相邻几个桶内的元素中进行搜索即可 （2）定性结论：欧式空间中，将高维空间的点映射到低维空间，原本接近的点在低维空间中肯定依然接近，但原本远离的点则有一定概率变成接近的点 （3）构建单桶： 假设vvv是高维空间中的kkk维 Embedding 向量，xxx是随机生成的 kkk 维映射向量。那我们*利用内积操作可以将$ v $映射到一维空间，得到数值 h(v)=v⋅xh(v)=v⋅xh(v)=v⋅x。 使用哈希函数$ h(v) 进行分桶，公式为：进行分桶，公式为：进行分桶，公式为：h^{x,b}(v)=⌊\\frac{x⋅v+b}{w}⌋，，，w$ 是分桶宽度，bbb是$ 0 到到到 w 间的一个均匀分布随机变量，避免分桶边界固化。间的一个均匀分布随机变量，避免分桶边界固化。间的一个均匀分布随机变量，避免分桶边界固化。x和和和b的改变会生成不同的哈希函数的改变会生成不同的哈希函数的改变会生成不同的哈希函数 h(v) $。 随机调整b，生成多个hash函数，并且采用或的方式组合，就可以一定程度避免这些边界点的问题 （4）构建多桶：采用 m 个哈希函数同时进行分桶，如果两个点同时掉进了 m 个桶，那它们是相似点的概率将大大增加。 （5）如何处理多桶关系： 且(And)操作：最大程度地减少候选点数量，也增大了漏掉最近邻点的概率。 或(Or)操作：减少了漏掉最近邻点的可能性，也增大了后续计算的开销。 （6）多桶策略实际建议： 点数越多，我们越应该增加每个分桶函数中桶的个数；相反，点数越少，我们越应该减少桶的个数； Embedding 向量的维度越大，我们越应该增加哈希函数的数量，尽量采用且的方式作为多桶策略；相反，Embedding 向量维度越小，我们越应该减少哈希函数的数量，多采用或的方式作为分桶策略。 向量最近邻搜索库 FAISS，可以替代LSH Facebook 的开源向量最近邻搜索库 FAISS 5. 模型服务 1. 业界主流模型服务方法 预存推荐结果或 Embedding 结果 （1）原理：在离线环境下生成对每个用户的推荐结果，然后将结果预存到以 Redis 为代表的线上数据库中。这样，我们在线上环境直接取出预存数据推荐给用户即可。 （2）优缺点： （3）适用场景：用户规模较小，或者一些冷启动、热门榜单等特殊的应用场景中。 预训练 Embedding+轻量级线上模型 （1）原理：用复杂深度学习网络离线训练生成 Embedding，存入内存数据库，再在线上实现逻辑回归或浅层神经网络等轻量级模型来拟合优化目标 （2）案例：阿里的MIMN（Multi-channel user Interest Memory Network，多通道用户兴趣记忆网络） 左边的部分不管多复杂，它们其实是在线下训练生成的，而右边的部分是一个经典的多层神经网络，它才是真正在线上服务的部分。 S(1)-S(m) 和 M(1)-M(m)是在离线生成的 Embedding 向量，在 MIMN 模型中，它们被称为“多通道用户兴趣向量”，这些 Embedding 向量就是连接离线模型和线上模型部分的接口。 线上部分从 Redis 之类的模型数据库中拿到这些离线生成 Embedding 向量，然后跟其他特征的 Embedding 向量组合在一起，扔给一个标准的多层神经网络进行预估。 （3）优缺点： 优点：隔离了离线模型的复杂性和线上推断的效率要求。 缺点：割裂了模型，不是端到端训练+部署的完美方案 PMML 模型 （1）介绍：全称是“预测模型标记语言”(Predictive Model Markup Language, PMML)，它是一种通用的以 XML 的形式表示不同模型结构参数的标记语言。在模型上线的过程中，PMML 经常作为中间媒介连接离线训练平台和线上预测平台。 （2）优缺点： 优点：PMML 在 Java Server 部分只进行推断，不考虑模型训练、分布式部署等一系列问题，因此 library 比较轻，能够高效地完成推断过程。 缺点：对于具有复杂结构的深度学习模型来说，PMML 语言的表示能力还是比较有限的，还不足以支持复杂的深度学习模型结构。 TensorFlow Serving 原理：模型存储、模型载入还原以及提供服务 2. 几种方法横向比较 3. Embedding经验 Embedding层K值的选择 经验公式：K = Embedding维数开4次方，x初始的维度数，后续K的调参按照2的倍数进行调整。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"线上服务","slug":"线上服务","permalink":"https://www.notlate.net/tags/%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1/"}]},{"title":"深度学习推荐系统-笔记05：Embedding技术","slug":"RecSys/2.特征工程/[深度学习推荐系统-笔记05]Embedding技术","date":"2021-01-05T13:02:30.000Z","updated":"2021-03-18T15:04:38.506Z","comments":true,"path":"posts/ba149af9.html","link":"","permalink":"https://www.notlate.net/posts/ba149af9.html","excerpt":"","text":"1. Embedding是什么 Embedding 就是用一个数值向量“表示”一个对象（Object）的方法 解读1：左边例子，从 king 到 queen 的向量和从 man 到 woman 的向量，无论从方向还是尺度来说它们都非常接近。 解读2：右边例子也很典型，从 walking 到 walked 和从 swimming 到 swam 的向量基本一致，这说明词向量揭示了词之间的时态关系 2. Embedding技术的重要性 （1）处理稀疏特征的利器 1）大量使用 One-hot 编码会导致样本特征向量极度稀疏 2）深度学习的结构特点又不利于稀疏特征向量的处理，原因如下： ​ ① 特征过于稀疏会导致整个网络的收敛非常慢，因为每一个样本的学习只有极少数的权重会得到更新，这在样本数量有限的情况下会导致模型不收敛。 ​ ② One-hot 类稀疏特征的维度往往非常地大，可能会达到千万甚至亿的级别，如果直接连接进入深度学习网络，那整个模型的参数数量会非常庞大，这对于一般公司的算力开销来说都是吃不消的。 3） 因此由 Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量。 （2）可以融合大量有价值信息，本身就是极其重要的特征向量 1）相比由原始信息直接处理得来的特征向量，Embedding 的表达能力更强 2）Graph Embedding 技术被提出后，Embedding 几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息 3. Embedding的实现技术 （1）Word2Vec：首次成功应用 1）Word2Vec，2013年由谷歌提出。模型分为两种形式：CBOW(连续词袋模型：由相邻词预测中间词)和Skip-gram(跳词模型：由当前词预测前后相邻词)。 2）训练方法： ​ ① 准备语料 ​ ② 分词，去掉停用词等无实际含义词 ​ ③ 生成词序列 ​ ④ 选取滑动窗口N，通过截取词组的方式生成训练样本 ​ ⑤ 模型训练（可以基于开源项目） 3）模型结构：本质是一个三层神经网络 ​ ① 隐层激活函数：没有或者说输入即输出的恒等函数 ​ ② 输出激活函数： softmax 4）词向量： ​ ① 输入层到隐层的权重矩阵WV∗NW_{V*N}WV∗N​（输入向量矩阵） 的每一个行向量对应的就是我们要找的“词向量”。同理输出向量矩阵也可以表示，但是通常习惯使用输入向量矩阵表示“词向量”。 ​ ② 把输入向量矩阵转换成词向量查找表（Lookup table） 5）延伸：Word2vec还有非常多的知识点值得细细挖掘，比如：模型结构、目标函数、负采样方法、负采样中的目标函数等。建议看一下《动手学深度学习》的相关内容：10.1词嵌入和10.2近似计算。 （2）Item2Vec：万物皆Embedding 1）Item2Vec，2015年由微软提出，它是对 Word2vec 方法的推广，使 Embedding 方法适用于几乎所有的序列数据。 2）Item2Vec 模型的技术细节几乎和 Word2vec 完全一致，只要能够用序列数据的形式把要表达的对象表示出来，再把序列数据“喂”给 Word2vec 模型，就能够得到任意物品的 Embedding了。 （3）Graph Embedding 1）互联网的数据可不仅仅是序列数据那么简单，越来越多的数据被我们以图的形式展现出来。典型的图结构数据示意图： ​ ① 社交关系：从社交网络中，我们可以发现意见领袖，可以发现社区，再根据这些“社交”特性进行社交化的推荐。如果我们可以对社交网络中的节点进行 Embedding 编码，社交化推荐的过程将会非常方便。 ​ ② 知识图谱：知识图谱中包含了不同类型的知识主体（如人物、地点等），附着在知识主体上的属性（如人物描述，物品特点），以及主体和主体之间、主体和属性之间的关系。如果我们能够对知识图谱中的主体进行 Embedding 化，就可以发现主体之间的潜在关系，这对于基于内容和知识的推荐系统是非常有帮助的。 ​ ③ 行为关系：由用户和物品组成的“二部图”，借助这样的关系图，我们自然能够利用 Embedding 技术发掘出物品和物品之间、用户和用户之间，以及用户和物品之间的关系，从而应用于推荐系统的进一步推荐。 2）Deep Walk：基于随机游走的 Graph Embedding 方法 ​ ① Deep Walk，2014年由美国石溪大学的研究者提出。 ​ ② 主要思想：由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入 Word2vec 进行训练，最终得到物品的 Embedding。 ​ ③ 跳转概率：就是遍历 vi 的邻接点 vj 的概率。 &lt;1&gt; 有向有权图：N+(vi)N_+(v_i)N+​(vi​)是节点viv_ivi​所有的出边集合，MijM_{ij}Mij​是节点viv_ivi​到节点vjv_jvj​的边的权重，即跳转概率是跳转边的权重占所有相关出边权重之和的比例 &lt;2&gt; 无向无权图：是上述公式的特例，Mij=1M_{ij}=1Mij​=1，N+(vi)N_+(v_i)N+​(vi​)是节点viv_ivi​所有的边集合。 3） Node2Vec：在同质性和结构性间权衡的方法 ① Node2Vec，2016年由斯坦福大学的研究者提出。 ② 主要思想：基于Deep Walk，Node2vec 通过调整随机游走跳转概率的方法，让 Graph Embedding 的结果在网络的**同质性（Homophily）和结构性（Structural Equivalence）**中进行权衡，可以进一步把不同的 Embedding 输入推荐模型，让推荐系统学习到不同的网络结构特点。 ③ 同质性：距离相近节点的 Embedding 应该尽量近似，让游走的过程更倾向于 DFS。示例：节点 u 与其相连的节点 s1、s2、s3、s4的 Embedding 表达应该是接近的。 ④ 结构性：结构上相似的节点的 Embedding 应该尽量接近，让随机游走要更倾向于 BFS。示例：节点 u 和节点 s6都是各自局域网络的中心节点，它们在结构上相似，所以它们的 Embedding 表达也应该近似。 ⑤ 跳转概率：πvx=αpq(t,x)⋅wvx\\pi_{vx}=\\alpha_{pq}(t,x)·w_{vx}πvx​=αpq​(t,x)⋅wvx​ &lt;1&gt; wvxw_{vx}wvx​是vxvxvx的原始权重，αpq(t,x)\\alpha_{pq}(t,x)αpq​(t,x)如上图所示，dtxd_{tx}dtx​表示节点ttt和距离节点xxx(节点vvv的下一个节点)的距离。 &lt;2&gt; 参数 p 被称为返回参数（Return Parameter），p 越小，随机游走回节点 t 的可能性越大，Node2vec 就更注重表达网络的结构性 &lt;3&gt; 参数 q 被称为进出参数（In-out Parameter），q 越小，随机游走到远方节点的可能性越大，Node2vec 更注重表达网络的同质性。 &lt;4&gt; 计算出的概率需要做归一化，使节点vvv到所有下一个节点的概率和为1。 4. Embedding的应用方法 1）直接应用 ① 利用物品 Embedding 间的相似性实现相似物品推荐 ② 利用物品 Embedding 和用户 Embedding 的相似性实现“猜你喜欢”等经典推荐功能 ③ 利用物品 Embedding 实现推荐系统中的召回层 2）预训练应用 把这些 Embedding 向量作为特征向量的一部分，跟其余的特征向量拼接起来，作为推荐模型的输入参与训练 3）End2End应用：端到端训练 ① 概念：不预先训练 Embedding，而是把 Embedding 的训练与深度学习推荐模型结合起来，采用统一的、端到端的方式一起训练，直接得到包含 Embedding 层的推荐模型 ② 案例：微软的Deep Crossing，UCL 提出的 FNN 和 Google 的 Wide&amp;Deep 4）常用的向量相似度计算法方法 请参考《计算向量间相似度的常用方法》。 5. 经典问答 1. 比较： 预训练与端到端训练区别 Embedding预训练的优点： ① 更快。因为对于End2End的方式，Embedding层的优化还受推荐算法的影响，这会增加计算量。 ② 难收敛。推荐算法是以Embedding为前提的，在端到端的方式中，训练初期由于Embedding层的结果没有意义，所以推荐模块的优化也可能不太有意义，可能无法有效收敛。 Embedding端到端的优点： ① 能够找到Embedding层在这个模型结构下的最优解。因为端到端将Embedding训练和推荐算法连接起来训练，那么Embedding层可以学习到最有利于推荐目标的Embedding结果。 2. Deep walk的优点和特点 ① 去掉多余噪音信息，关注主要矛盾，所以一般要生成比原样本更少的样本量 ② deep walk的抽样过程保留了转移矩阵的“主要框架”，但同时当抽样次数不太高的时候，item embedding的覆盖率反而没有item2vec好 3. AutoEncoder和Word2vec的关系是什么？ 没找到特别好的材料，欢迎留言，参考 SVD分解(三)：连Word2Vec都只不过是个SVD？中的说法： 结构上：Word2vec与AutoEncoder和SVD是一样的； 实现上：Word2Vec最后接的是softmax来预测概率，也就是说实现了一个非线性变换，而自编码器或者SVD并没有。 6. 扩展阅读 强烈建议大家阅读下王喆推荐的Embedding从入门到专家必读的十篇论文。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Embedding","slug":"Embedding","permalink":"https://www.notlate.net/tags/Embedding/"}]},{"title":"深度学习推荐系统-笔记04：特征工程介绍","slug":"RecSys/2.特征工程/[深度学习推荐系统-笔记04]推荐系统有哪些可以利用的特征","date":"2021-01-05T12:54:29.000Z","updated":"2021-03-18T15:04:38.506Z","comments":true,"path":"posts/75f170bd.html","link":"","permalink":"https://www.notlate.net/posts/75f170bd.html","excerpt":"","text":"1. 特征与工程 （1）特征就是对具体行为的抽象，但是抽象过程会造成信息的损失 ① 因为具体的推荐行为和场景中包含大量原始的场景、图片和状态信息，保存所有信息的存储空间过大，我们根本无法实现。 ② 因为具体的推荐场景中包含大量冗余的、无用的信息，把它们都考虑进来甚至会损害模型的泛化能力。 （2）特征工程原则 ① 尽可能地让特征工程抽取出的一组特征，能够保留推荐环境及用户行为过程中的所有“有用“信息，并且尽量摒弃冗余信息 ② 在已有的、可获得的数据基础上，“尽量”保留有用信息是现实中构建特征工程的原则 （3）特征处理：没有标准答案 类别、ID类 ① one-hot ② multi-hot 数值类 ① 归一化：解决特征取值范围不统一的问题，但无法改变特征值的分布 ② 分桶：解决特征值分布极不均匀的问题。将样本按照某特征的值从高到低排序，然后按照桶的数量找到分位数，将样本分到各自的桶中，再用桶 ID 作为特征值。 ③ 特征交叉：参考YouTube深度推荐模型中的平方、开方等等改变特征分布。 ④ Spark MLlib：分别提供了两个转换器 MinMaxScaler 和 QuantileDiscretizer，来进行归一化和分桶的特征处理 （3）常用特征 用户行为数据。 又分为显性反馈行为和隐性反馈行为。在当前的推荐系统特征工程中，隐性反馈行为越来越重要，主要原因是显性反馈行为的收集难度过大，数据量小。 用户关系数据，分为“强关系”和“弱关系”。 用途： ① 作为召回层的一种物品召回方式 ② 建立关系图，使用 Graph Embedding 的方法生成用户和物品的 Embedding ③ 直接利用关系数据，通过“好友”的特征为用户添加新的属性特征 ④ 直接建立社会化推荐系统 属性、标签类数据。 特征处理方法： ① 通过 Multi-hot 编码的方式将其转换成特征向量 ② 重要的属性标签类特征也可以先转换成 Embedding，比如业界最新的做法是将标签属性类数据与其描述主体一起构建成知识图谱（Knowledge Graph），在其上施以 Graph Embedding 或者 GNN（Graph Neural Network，图神经网络）生成各节点的 Embedding 内容类数据，可以看作属性标签型特征的延伸。 ① 形式：往往是大段的描述型文字、图片，甚至视频。 ② 使用：通过自然语言处理、计算机视觉等技术手段提取关键内容特征转成标签类数据。 场景信息（上下文信息）。 ① 形式：时间、地点、当前页面信息、季节、月份、节假日、天气、社会大事件等等 ② 任何影响用户决定的因素都可以当作是场景特征的一部分，但实际中更多利用易获取的场景特征 （4）经典问答 1. 问：以音乐APP为例，用户挑选歌曲时的关键信息有哪些？ ① 听歌的目的。比如是为了放松，冥想，学习还是运动。目的决定了歌曲是安静还是激昂，舒缓还是节奏感强烈。 ② 歌曲或歌单是否受欢迎。定下基调后，我一般会选择收藏或播放量较多的歌曲。这样一般不容易采坑。 ③ 歌曲的旋律与当下状态的匹配度。当下的状态可能是心情，情绪或身体的疲劳程度，而旋律与状态的匹配也很重要。 2. 问：音乐APP工程师应该提取哪些关键信息作为特征？ ① 用户听歌的目的很难准确预测，但是可以通过“隐性”数据去推测，比如搜索关键词等。 ② 歌曲或歌单是否受欢迎，则可以通过歌曲或歌单的播放量、收藏量去建立特征，而具体到人和歌曲的关系时，还可以进一步具体到单曲循环的次数等来细化特定用户对特定歌曲的喜好程度。 ③ 当下的状态也很难显性的获得，则可以根据历史听歌记录去推测用户的生理节律，例如夜晚会愿意听舒缓的歌曲，运动会愿意听节奏感强烈的歌曲等等。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"https://www.notlate.net/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"深度学习推荐系统-笔记01：推荐系统基础","slug":"RecSys/1.基础架构/[深度学习推荐系统-笔记01]推荐系统概要","date":"2021-01-05T12:15:34.000Z","updated":"2021-03-18T15:04:38.506Z","comments":true,"path":"posts/ffff5cb7.html","link":"","permalink":"https://www.notlate.net/posts/ffff5cb7.html","excerpt":"","text":"1. 深度学习推荐系统基础概念 2. 从0到1搭建深度学习推荐系统 – 开源项目Sparrow RecSys实操（以Mac为例） （1）安装Scala 2.11（务必是2.11大版本，否则与开源项目设置的版本号不匹配，会有执行失败的问题） 12brew updatebrew install scala@2.11 （2）下载开源项目Sparrow RecSys git clone https://github.com/wzhe06/SparrowRecSys.git 项目地址：https://github.com/wzhe06/SparrowRecSys （3）安装IDEA和JDK ① 下载IDEA（https://www.jetbrains.com/idea/download/#section=mac） ② 下载JDK（https://www.oracle.com/java/technologies/javase-jdk15-downloads.html） ③ 安装IDEA和JDK（JDK的路径~/Library/Java/JavaVirtualMachines/openjdk-15.0.1-1） （4）导入工程&amp;运行 ① 打开IDEA，打开File-&gt;Project Strucure-&gt;Project-&gt;Project JDK(我的好像会自动识别)。若没有识别（显示jdk15.1）,点击三角号，自己添加，步骤Add SDK-&gt;JDK-&gt;选择上面提到的JDK路径选择。 ② 在pom.xml点击右键，设置为maven project-&gt;‘Reload project’。耐心等待，这个很费时间。 ③ 然后找到SparrowRecSys/src/main/java/com/SparrowRecSys/online/RecSysServer,右击选择&quot;Run ‘RecSysServer.main()’&quot;,程序就执行起来了. ④ 浏览器中输入http://localhost:6010/即可打开首页 （5）SparrowRecsys涵盖的技术 3. 推荐系统相关知识扩充 （1）书籍推荐 ① 深度学习推荐系统 ② 西瓜书 ③ 蒲公英书 ④ 百面机器学习 ⑤ 数学之美（吴军） （2）实践工具相关 Spark ① 形象理解Hadoop、Hive、Spark ② 根据官网写一个Spark Hello World 程序 ③ 初步了解Spark MLlib Tensorflow ① 介绍 TensorFlow 和 Keras 的基本概念的文章 ② Keras 写一个 Hello World ③ 官方教程 Redis ① Redis基本介绍 ② Redis基本操作 经典问答 1. 问：对于电影推荐系统来讲，哪些数据对生成用户个性化推荐结果最有帮助？ 答：（1）内容相关特征：电影种类，演员，电影内容，电影质量等；（2）用户行为特征：用户历史浏览记录、观看记录等； 2. 问：召回层单独优化新增特征，在排序层没有，如何处理？ 答：在设计召回层和排序层的时候一般要联合设计，召回层要特别关注召回率指标。如果新增特征对结果影响比较大，排序层模型训练的时候同步引入这两个特征。 参考资料 《深度学习推荐系统实战》 – 极客时间，王喆 本文首发于个人小站：NotLate.net，欢迎关注。","categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}]},{"title":"Python操作MongoDB","slug":"Distribution/mongodb/3. Python操作MongoDB","date":"2020-12-26T16:55:51.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/fcb39fd4.html","link":"","permalink":"https://www.notlate.net/posts/fcb39fd4.html","excerpt":"","text":"1. python基础操作 pip install pymongo 连接数据库，ip：localhost 端口号：27017 12import pymongomyclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;) 创建或切换到名为notlate的数据库： 1mydb = myclient[&quot;notlate&quot;] 数据库是否已经存在： 123dblist = myclient.list_database_names()if &quot;notlate&quot; in dblist: print(&quot;数据库已存在！&quot;) 删除名为notlate的数据库： 1myclient.drop_database(mydb) 创建或切换名为notlate2的集合Collection： 1my_collection = mydb[&#x27;notlate2&#x27;] 删除集合： 1my_collection.drop() 更新文档，若不存在，则创建： 1my_collection.update_one(&#123;&#x27;name&#x27;: &#x27;not&#x27;&#125;, &#123;&#x27;$set&#x27;: &#123;&#x27;name&#x27;: &#x27;not&#x27;, &#x27;age&#x27;: 15&#125;&#125;, upsert=True) 查找所有文档： 12for doc in my_collection.find(): print(doc) 查找指定条件的文档： 12for doc in my_collection.find(&#123;&#x27;name&#x27;: &#x27;not&#x27;&#125;): print(doc)","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"https://www.notlate.net/tags/mongodb/"},{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Mongodb增删改查","slug":"Distribution/mongodb/2. Mongodb增删改查","date":"2020-12-25T16:55:51.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/c2f4b902.html","link":"","permalink":"https://www.notlate.net/posts/c2f4b902.html","excerpt":"","text":"1. 创建数据库 显示所有数据库：show dbs 切换/创建数据：use NotLate 再次show dbs发现新创建的NotLate数据库并不在列表中，因为数据库是空的 插入一条数据：db.NotLate.insert(&#123;&quot;name&quot;: &quot;not too late yet&quot;&#125;) 再次show dbs发现列表中有NotLate了 2. 删除数据库 切换到要删除的数据库：use NotLate 删除：db.dropDatabase() 查看所在数据库：db，依然会显示所在数据库是：NotLate 使用show dbs查看列表：发现列表中已经没有NotLate了 3. 创建集合 切换数据库：use NotLate 创建集合语法格式：db.createCollection(name, options)，例如：db.createCollection(&quot;notlate&quot;)，其中options为可选参数： 字段 类型 描述 capped 布尔 （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 当该值为 true 时，必须指定 size 参数。 autoIndexId 布尔 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size 数值 （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段。 max 数值 （可选）指定固定集合中包含文档的最大数量。 查看集合列表：show tables 或者 show collections 创建固定集合 notlatetest，整个集合空间大小 1024 KB, 文档最大个数为 10 个： db.createCollection(&quot;notlatetest&quot;, &#123; capped : true, autoIndexId : true, size : 1024, max : 10 &#125; ) 如果没有集合，可以在插入文档时，自动创建集合：db.not1.insert(&#123;&quot;name&quot;: &quot;not1&quot;&#125;) 4. 删除集合 语法格式：db.collection.drop() 切换到NotLate数据库：use NotLate 查看集合列表：show collections 删除集合not1：db.not1.drop() 再次查看集合列表：show collections 5. 插入文档 语法格式：db.collection_name.insert(document)，若_id主键不存在，则插入；否则抛出异常 3.2之后新增db.collection_name.insertOne(doc)和db.collection_name.insertMany([doc1, doc2, ...]) 插入一份文档：db.not1.insert(&#123;&quot;name&quot;: &quot;not1&quot;&#125;) 查看文档列表：db.not1.find() 插入多份文档：db.not1.insertMany(&#123;&quot;name&quot;: &quot;not2&quot;&#125;, &#123;&quot;name&quot;: &quot;not3&quot;&#125;) 查看文档列表：db.not1.find() 6. 更新文档 语法格式： 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) 参数含义： ① query : update的查询条件，类似sql update查询内where后面的。 ② update : update的对象和一些更新的操作符（如,,,inc…）等，也可以理解为sql update查询内set后面的 ③ upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 ④ multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 ⑤ writeConcern :可选，抛出异常的级别。 把name=not1 更新为name=not111 : db.not1.update(&#123;&quot;name&quot;: &quot;not1&quot;&#125;, &#123;\\$set: &#123;&quot;name&quot;: &quot;not111&quot;&#125;&#125;, &#123;multi: true&#125;) , 其中，&#123;multi:true&#125;表示把所有name=not1的文档都修改成name=not111 查看文档列表：db.not1.find() 其他操作，比如inc等后续补充完善。 7. 删除文档 deleteOne()和deleteMany() 删除name=not111的文档: db.not1.deleteOne(&#123;&quot;name&quot;: &quot;not111&quot;&#125;) 查看文档列表：db.not1.find() 删除name=not2的全部文档：db.not1.deleteMany(&#123;&quot;name&quot;: &quot;not2&quot;&#125;) 查看文档列表：db.not1.find() 删除全部文档：db.not1.deleteMany({}) 参考文档 菜鸟-MongoDB 教程 MongoDB官方文档","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"https://www.notlate.net/tags/mongodb/"},{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MongoDB基础","slug":"Distribution/mongodb/1. MongoDB基础","date":"2020-12-25T16:55:34.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/f20a5c88.html","link":"","permalink":"https://www.notlate.net/posts/f20a5c88.html","excerpt":"","text":"1. Mac brew安装命令 12brew tap mongodb/brewbrew install mongodb-community@4.4 4.4表示最新版本 2. 安装配置信息 配置文件：/usr/local/etc/mongod.conf 日志文件路径：/usr/local/var/log/mongodb 数据存放路径：/usr/local/var/mongodb 3. 运行命令 1234brew启动：brew services start mongodb-communitybrew停止：brew services stop mongodb-communitymongod命令后台进程方式：mongod --config /usr/local/etc/mongod.conf --fork后台进程关闭方式：先进入mongo shell，然后执行db.adminCommand(&#123; &quot;shutdown&quot; : 1 &#125;) 4. 数据库相关基础命令 进入mongo shell：先用brew命令启动mongod服务，然后执行客户端mongo命令连接到服务。 显示所有数据库：show dbs 查看当前所在数据库：db，默认所在数据库是：test 切换数据库： use xxx，其中xxx是新的数据库名字，例如：use local，然后可以用db确认。 保留数据库名： ① admin：从权限的角度来看，这是&quot;root&quot;数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 ② local：这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 ③ config：当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 5. RDBMS和MongoDB数据库术语对比 RDBMS MongoDB 数据库 数据库 表格 集合(Collection) 行 文档(document)，一组键值对(即BSON) 列 字段 表联合 嵌入文档 主键 主键(MongoDB 提供了 key 为 _id ) 6. MongoDB数据类型 数据类型 描述 String 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean 布尔值。用于存储布尔值（真/假）。 Double 双精度浮点值。用于存储浮点值。 Min/Max keys 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 Array 用于将数组或列表或多个值存储为一个键。 Timestamp 时间戳。记录文档修改或添加的具体时间。 Object 用于内嵌文档。 Null 用于创建空值。 Symbol 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID 对象 ID。用于创建文档的 ID。 Binary Data 二进制数据。用于存储二进制数据。 Code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 ObjectId：类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是： ​ ① 前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 ​ ② 接下来的 3 个字节是机器标识码 ​ ③ 紧接的两个字节由进程 id 组成 PID ​ ④ 最后三个字节是随机数 123&gt; var objId = ObjectId()&gt; objId.getTimestamp() # ISODate(&quot;2021-01-07T12:57:38Z&quot;)&gt; objId.str # 5ff70542f9b544d4a909f49b 字符串：BSON字符串都是UTF-8编码 时间戳：BSON 有一个特殊的时间戳类型用于 MongoDB 内部使用，与普通的 日期 类型不相关。 时间戳值是一个 64 位的值。其中： ​ ① 前32位是一个 time_t 值（与Unix新纪元相差的秒数） ​ ② 后32位是在某秒中操作的一个递增的序数 日期：表示当前距离 Unix新纪元（1970年1月1日）的毫秒数。日期类型是有符号的, 负数表示 1970 年之前的日期。 123456&gt; var date1 = new Date() # 格林尼治时间&gt; date1 # ISODate(&quot;2021-01-07T13:07:15.195Z&quot;)&gt; typeof date1 # object&gt; var date1Str = date1.toString() # Thu Jan 07 2021 21:07:15 GMT+0800 (CST)&gt; typeof date1Str # string&gt; Date() # Thu Jan 07 2021 21:09:44 GMT+0800 (CST) 7. MongoDB连接 标准连接URI： mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]] ​ ① mongodb:// 这是固定的格式，必须要指定。 ​ ② username:password@ 可选项，如果设置，在连接数据库服务器之后，驱动都会尝试登录这个数据库 ​ ③ host1 必须的指定至少一个host, host1 是这个URI唯一要填写的。它指定了要连接服务器的地址。如果要连接复制集，请指定多个主机地址。 ​ ④ portX 可选的指定端口，如果不填，默认为27017 ​ ⑤ /database 如果指定username:password@，连接并验证登录指定数据库。若不指定，默认打开 test 数据库。 ​ ⑥ ?options 是连接选项。如果不使用/database，则前面需要加上/。所有连接选项都是键值对name=value，键值对之间通过&amp;或;（分号）隔开 连接示例 mongodb://admin:123456@localhost/ 参考文档 菜鸟-MongoDB 教程","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"https://www.notlate.net/tags/mongodb/"},{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Redis分布式锁","slug":"Distribution/redis/3. Redis分布式锁","date":"2020-12-24T16:56:24.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/cc4d73b2.html","link":"","permalink":"https://www.notlate.net/posts/cc4d73b2.html","excerpt":"","text":"1. RedLock python安装： 1pip install redlock 单机常规用法： 1234567from redlock import RedLock# By default, if no redis connection details are# provided, RedLock uses redis://127.0.0.1:6379/0lock = RedLock(&quot;distributed_lock&quot;)lock.acquire()do_something()lock.release() 单机with语句： 123from redlock import RedLockwith RedLock(&quot;distributed_lock&quot;): do_something() 集群with语句： 12345678910from redlock import RedLockwith RedLock(&quot;distributed_lock&quot;, connection_details=[ &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;, &#x27;port&#x27;: 6379, &#x27;db&#x27;: 0&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;, &#x27;port&#x27;: 6379, &#x27;db&#x27;: 0&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;, &#x27;port&#x27;: 6379, &#x27;db&#x27;: 0&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;, &#x27;port&#x27;: 6379, &#x27;db&#x27;: 0&#125;, ] ): do_something() 复用redis客户端： 1234567891011121314from redlock import RedLockFactoryfactory = RedLockFactory( connection_details=[ &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;&#125;, &#123;&#x27;host&#x27;: &#x27;xxx.xxx.xxx.xxx&#x27;&#125;, ])with factory.create_lock(&quot;distributed_lock&quot;): do_something()with factory.create_lock(&quot;another_lock&quot;): do_something()","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.notlate.net/tags/redis/"}]},{"title":"Redis实现发布订阅模式消息队列","slug":"Distribution/redis/2. Redis实现发布订阅模式消息队列","date":"2020-12-23T16:56:24.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/a62d9a52.html","link":"","permalink":"https://www.notlate.net/posts/a62d9a52.html","excerpt":"","text":"1. Redis实现阻塞消息队列 介绍：Redis的列表类型键可以用来实现队列，并且支持阻塞式读取，可以很容易的实现一个高性能的优先队列。同时在更高层面上，Redis还支持&quot;发布/订阅&quot;的消息模式。 方法：brpop和blpop实现阻塞读取 brpop命令的完整的格式是：BRPOP key [key ...] timeout，表示同时检测多个key值，若所有key都没有数据，则阻塞；任意一个key有元素，则按照key的顺序进行读取，因此可以实现优先级队列。 测试：brpop阻塞读取 ① 打开第1个客户端，执行brpop list1 list2 0，表示读取list1和list2对应的key值。回车后，命令会阻塞住，不会返回任何值，因为当前redis中没有这两个key对应的值。 ② 打开第2个客户端，执行lpush list1 a b，表示往list1中添加a和b`，返回2，表示添加了2个元素。 ③ 此时第1个客户端会立即返回list1 和 a，再执行一次会立即返回list1和b。 2. Redis实现发布/订阅模式 订阅者命令： ① subscribe channel1 [channel2 ...] 可以同时订阅多个频道，订阅后不会受到之前已经发布的消息 ② psubscribe pattern [pattern ...] 可以订阅指定规则的频道，支持通配符：?表示1个占位符，*表示任意个占位符，?*表示至少1个占位符 ③ 订阅状态客户端只能收到3种类型的回复，每种类型包含3个值，详细如下； ​ subscribe/psubscribe：第1个值是表示成功订阅了频道；第2个是频道名称；第3个是成功订阅频道的个数。 ​ message/pmessage：第1个值是表示发布的消息；第2个是频道名称；第3个是消息内容。 ​ unsubscribe/punsubscribe：第1个值表示成功退订了频道；第2个是频道名称；第3个是当前客户端剩余成功订阅频道的个数。若此值为0，则客户端会推出订阅状态。 ④ 多个客户端订阅了同一频道时，均可收到发布的消息； ⑤ punsubscribe与unsubscribe不会互相影响对应命令订阅的频道 发布者命令： ① publish channel message 向频道channel发布消息内容为message ② 发布的消息不会持久化，即不会把消息发给之后才订阅本频道的客户端","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.notlate.net/tags/redis/"}]},{"title":"Redis基础","slug":"Distribution/redis/1. Redis基础","date":"2020-12-23T16:56:07.000Z","updated":"2021-03-18T15:04:38.498Z","comments":true,"path":"posts/fe29e710.html","link":"","permalink":"https://www.notlate.net/posts/fe29e710.html","excerpt":"","text":"1. Mac安装Redis 123456安装：brew install redis启动：brew services start redis停止：brew services stop redis重启：brew services restart redis配置文件路径：/usr/local/etc/redis.conf进入客户端：redis-cli 2. Python安装客户端 1pip install redis 3. 基础操作 3.1 Redis的数据类型 3.1.1 String(字符串) set命令：设置一个键和值，键存在则只覆盖，返回ok。示例：set [key] [value] get命令：获取一个键的值，返回值。示例：get [key] setnx命令：设置一个不存在的键和值（防止覆盖，若键已存在则返回0表示失败）。示例：setnx [key] [value] setex命令：设置一个指定有效期的键和值（单位秒），不写有效时间则表示永久有效，等价于set。示例：setex [key] [time] [value] setrange命令：替换子字符串 (替换长度由子子串长度决定)，将key的第[4]个位置开始替换。示例：setrange [key] [位置] [value] mset命令：批量设置键和值，成功则返回ok。示例：mset [key1] [value1] [key2] [value2] [key3] [value3] msetnx命令：批量设置不存在的键和值，成功则返回ok。示例：msetex [key1] [value1] [key2] [value2] [key3] [value3] ... getset命令：获取原值，并设置新值。 getrange命令：获取指定范围的值。示例：getrange [key] 0 4 mget命令： 批量获取值。示例：mget [key1] [key2] [key3] ... incr命令： 指定键的值做加加操作，返回加后的结果。示例：incr [key] incrby命令： 设置某个键加上指定值。示例：incrby [key] m decr命令： 指定键的值做减减操作，返回减后的结果。示例：decr [key] decrby命令： 设置某个键减上指定值。示例：decrby [key] m append命令：给指定key的字符串追加value，返回新字符串值的长度。示例：append [key] 追加子串 strlen命令：求长度。示例：strlen [key] 3.1.2 hash(哈希表) 4. Python使用Redis 4.1 python操作字符串 12345678910111213141516171819202122import redis # host是redis主机，需要redis服务端和客户端都启动 redis默认端口是6379r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)# 字串操作r.set(&#x27;name&#x27;, &#x27;junxi&#x27;) # key是&quot;foo&quot; value是&quot;bar&quot; 将键值对存入redis缓存print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;)) # 取出键name对应的值print(type(r.get(&#x27;name&#x27;)))# 如果键fruit不存在，那么输出是True；如果键fruit已经存在，输出是Noneprint(r.set(&#x27;fruit&#x27;, &#x27;watermelon&#x27;, nx=True)) # True--不存在print(r.setnx(&#x27;fruit1&#x27;, &#x27;banana&#x27;)) # fruit1不存在，输出为True#设置过期时间r.setex(&quot;fruit2&quot;, &quot;orange&quot;, 5)time.sleep(5)print(r.get(&#x27;fruit2&#x27;)) # 5秒后，取值就从orange变成Noneprint(r.mget(&quot;fruit&quot;, &quot;fruit1&quot;, &quot;fruit2&quot;, &quot;k1&quot;, &quot;k2&quot;)) # 将目前redis缓存中的键对应的值批量取出来 4.2 python操作hash 1234567891011121314r.hset(&quot;hash1&quot;, &quot;k1&quot;, &quot;v1&quot;)r.hset(&quot;hash1&quot;, &quot;k2&quot;, &quot;v2&quot;)print(r.hkeys(&quot;hash1&quot;)) # 取hash中所有的keyprint(r.hget(&quot;hash1&quot;, &quot;k1&quot;)) # 单个取hash的key对应的值print(r.hmget(&quot;hash1&quot;, &quot;k1&quot;, &quot;k2&quot;)) # 多个取hash的key对应的值r.hsetnx(&quot;hash1&quot;, &quot;k2&quot;, &quot;v3&quot;) # 只能新建print(r.hget(&quot;hash1&quot;, &quot;k2&quot;))#hash的批量操作r.hmset(&quot;hash2&quot;, &#123;&quot;k2&quot;: &quot;v2&quot;, &quot;k3&quot;: &quot;v3&quot;&#125;)print(r.hget(&quot;hash2&quot;, &quot;k2&quot;)) # 单个取出&quot;hash2&quot;的key-k2对应的valueprint(r.hmget(&quot;hash2&quot;, &quot;k2&quot;, &quot;k3&quot;)) # 批量取出&quot;hash2&quot;的key-k2 k3对应的value --方式1print(r.hmget(&quot;hash2&quot;, [&quot;k2&quot;, &quot;k3&quot;])) # 批量取出&quot;hash2&quot;的key-k2 k3对应的value --方式2print(r.hgetall(&quot;hash1&quot;)) #取出所有的键值对 4.3 python操作list 123456789101112131415161718192021222324252627r.lpush(&quot;list1&quot;, 11, 22, 33)print(r.lrange(&#x27;list1&#x27;, 0, -1))r.rpush(&quot;list2&quot;, 11, 22, 33) # 表示从右向左操作print(r.llen(&quot;list2&quot;)) # 列表长度print(r.lrange(&quot;list2&quot;, 0, 3)) # 切片取出值，范围是索引号0-3r.rpush(&quot;list2&quot;, 44, 55, 66) # 在列表的右边，依次添加44,55,66print(r.llen(&quot;list2&quot;)) # 列表长度print(r.lrange(&quot;list2&quot;, 0, -1)) # 切片取出值，范围是索引号0到-1(最后一个元素)r.lset(&quot;list2&quot;, 0, -11) # 把索引号是0的元素修改成-11print(r.lrange(&quot;list2&quot;, 0, -1))r.lrem(&quot;list2&quot;, &quot;11&quot;, 1) # 将列表中左边第一次出现的&quot;11&quot;删除print(r.lrange(&quot;list2&quot;, 0, -1))r.lrem(&quot;list2&quot;, &quot;99&quot;, -1) # 将列表中右边第一次出现的&quot;99&quot;删除print(r.lrange(&quot;list2&quot;, 0, -1))r.lrem(&quot;list2&quot;, &quot;22&quot;, 0) # 将列表中所有的&quot;22&quot;删除print(r.lrange(&quot;list2&quot;, 0, -1))r.lpop(&quot;list2&quot;) # 删除列表最左边的元素，并且返回删除的元素print(r.lrange(&quot;list2&quot;, 0, -1))r.rpop(&quot;list2&quot;) # 删除列表最右边的元素，并且返回删除的元素print(r.lrange(&quot;list2&quot;, 0, -1))print(r.lindex(&quot;list2&quot;, 0)) # 取出索引号是0的值 4.4 python操作set 12345678910111213141516#新增r.sadd(&quot;set1&quot;, 33, 44, 55, 66) # 往集合中添加元素print(r.scard(&quot;set1&quot;)) # 集合的长度是4print(r.smembers(&quot;set1&quot;)) # 获取集合中所有的成员print(r.sscan(&quot;set1&quot;)) #获取集合中所有的成员--元组形式for i in r.sscan_iter(&quot;set1&quot;): print(i)#差集r.sadd(&quot;set2&quot;, 11, 22, 33)print(r.smembers(&quot;set1&quot;)) # 获取集合中所有的成员print(r.smembers(&quot;set2&quot;))print(r.sdiff(&quot;set1&quot;, &quot;set2&quot;)) # 在集合set1但是不在集合set2中print(r.sdiff(&quot;set2&quot;, &quot;set1&quot;)) # 在集合set2但是不在集合set1中","categories":[{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.notlate.net/tags/redis/"}]},{"title":"计算广告-笔记16：其他广告相关技术","slug":"AdComputation/16.其他广告相关技术","date":"2020-10-04T04:17:26.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/2e46f1ce.html","link":"","permalink":"https://www.notlate.net/posts/2e46f1ce.html","excerpt":"","text":"创意优化 创意需要将向用户推送广告的关键原因明确表达出来 程序化创意 地域性创意 搜索重定向创意 个性化重定向创意 电机热力图 电机热力图是将某一个创意各位置被点击的密度用热力图方式呈现，帮助创意优化者直观的发现和解决其中的问题 创意的发展趋势 视频化 激励视频形式 交互化 实验框架 一个实用的实验框架需要尽可能的同时容纳多组试验，以提高流量利用效率和产品进化速度 设计实验系统的关键是利用系统模块之间的相对独立性，用分层的结构来扩展实验流量 广告监测与归因 广告监测 主要需求存在于按CPT或CPM结算的合约广告，需要委托第三方监测公司对实际发生的展示或点击数目进行核对 广告安全 广告投放验证：重点在于阻止不恰当展示的发生 可视性验证：重点在于广告展示的曝光程度 广告归因 按CPS/CPA/ROI方式结算的广告，转化行为不发生在媒体上，往往需要第三方机构对这些效果数据进行追踪，确定某次转化来自哪个渠道，将广告展示和点击数据对应起来 常见方法 直接访问归因 用户ID碰撞归因 作弊与反作弊 广告作弊：制造虚假流量，或用技术手段骗过广告监测和归因 作弊方法分类 从主体来看，分媒体作弊、广告平台作弊、广告主竞争对手作弊 从原理看，分为虚假流量作弊、归因作弊 从作弊手段看，分机器作弊、人工作弊 常见作弊方法 服务器刷监测代码 客户端刷监测代码 频繁换用户身份 流量劫持 损害媒体利益，但是有真是流量 信道弹窗 创意替换 搜索结果重定向 损害广告主利益 落地页来源劫持 cookie填充 IP遮盖 点击滥用与点击注入 产品技术选型实战 媒体实战 广告主实战 数据提供方实战","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"测试框架","slug":"测试框架","permalink":"https://www.notlate.net/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"}]},{"title":"计算广告-笔记15：程序化交易核心技术","slug":"AdComputation/15.程序化交易核心技术","date":"2020-10-04T03:00:10.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/cf81ffb6.html","link":"","permalink":"https://www.notlate.net/posts/cf81ffb6.html","excerpt":"","text":"广告交易平台ADX cookie映射 解决供给方和需求方身份对应问题 重点关注三个问题：由谁发起，在哪里发起，谁保存映射 询价优化 对每次展示中询价的DSP数进行精简，尽可能只向那些可能赢得拍卖的DSP询价 两种思路：工程规则思路和带约束的优化问题 需求方平台DSP 实时竞价环境下：提供定制化用户分化功能 最常用的是重定向：把访问广告主网站的某特定用户集合传送给DSP 1.在广告主网站上布设DSP域名的js代码 优点：实时获取到访客信息，但是需要一段时间的数据积累 缺点：js代码过多影响用户体验 2.采用线下数据接口，定期将广告主或DMP收集到的访客集合批量传送给DSP，需要建立cookie映射机制 优点：避免js代码多导致页面重 缺点：数据更新有延时 另一种注意的是新客推荐look-alike 完全面向广告主，在量的约束下投放：出价策略 在有预算的情况下，尽可能提升每次展示的利润率 像广告网络一样(CPC)：点击率预测 除关注AUC等指标外，CoPC(真是点击数/预估点击数)也要重点关注，它表征某部分流量上是否存在明显的点击率高估或者低估 面对效果类广告主时(CPS)：点击价值预估 可拆解为：到达率转化率转化单价 到达率和转化单价可采用统计的方法 转化率预估：数据不充分情况下可采用简单统计与运营经验相结合，数据充分下可采用模型方法 供给方平台SSP 网络优化问题 指SSP在接入多个广告网络以后，在线动态决定将广告请求发给哪个广告网络，从而优化整体收入的问题","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"ADX","slug":"ADX","permalink":"https://www.notlate.net/tags/ADX/"},{"name":"DSP","slug":"DSP","permalink":"https://www.notlate.net/tags/DSP/"},{"name":"SSP","slug":"SSP","permalink":"https://www.notlate.net/tags/SSP/"}]},{"title":"计算广告-笔记14：点击率预测模型","slug":"AdComputation/14.点击率预测模型","date":"2020-10-03T10:30:30.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/f123b21a.html","link":"","permalink":"https://www.notlate.net/posts/f123b21a.html","excerpt":"","text":"点击率预测模型 点击率：把点击事件h看成一个二元取值的随机变量，取值为真(h=1)的概率就是点击率 点击事件分布：表示成以点击率μ为参数的二项分布 点击率基础模型：逻辑回归(LR)，在(a,u,c)组合与点击率μ之间建立函数关系，表示成对μ(a,u,c)=p(h=1|a,u,c)的概率建模问题 LR正是当目标值的分布服从伯努利分布时广义线性模型的特例，映射函数是logit(t)=log{p/(1-p)} L2-norm避免过拟合 LR模型优化 梯度下降法 L-BFGS 置信域法 工业界常用模型训练思路 1.降低模型训练次数，通过特征侧的方法捕捉信号的快速变化 2.增量求解，降低模型收敛所需的迭代次数 3.精心设计最优化算法如ADMM，降低模型收敛所需的迭代次数 点击率模型的矫正：正负样本不平衡可能带来预估模型的偏差，原因如下: 高斯分布方差的最大似然估计是有偏的（为了得到方差的无偏估计，需要将样本数目-1来计算方差） 偏差的方向是对方差有所低估，且样本数目越少，低估越严重。 由于正样本(h=1)远远小于负样本(h=0)的数据量，对前者的低估更严重。 其他点击率预估模型 因子分解机FM GBDT 深度学习点击率预估模型 探索与利用 α贪婪法 决策过程 总是用比例为α的小部分流量做探索，在探索流量上随机选择A个广告中的1个； 在剩余1-α比例的流量上，总是选择经验收益最高的那个。 置信上界(UCB)方法 概念：每次投放时，不但简单的选择经验最优广告，而且考虑到经验估计的不确定性，进而选择估计值有可能达到的上界最大的那个广告 UCB决策过程 1.根据过去的观测值，利用某概率模型计算出每个a的期望回报的UCB 2.选择UCB最大的a UCB计算方法 考虑上下文的bandit LinUCB方法 点击率模型特征 基本特征：广告侧特征、用户侧特征、上下文特征 特征的非线性化 特征离散化：将连续特征切成一组分区，当特征值落在某个区间时，对应区间离散特征值标1，否则标0 引入特征的非线性变化，如平方、平方根、log等 特征组合 静态特征组合：用户侧、广告侧、上下文侧的标签组合，如性别和地点、广告主题和性别等 动态特征组合：如用户对某个广告的历史点击率，即当某个组合特征被触发时，不再标为1，而是使用历史点击率作为特征值 偏差特征 常见的偏差特征包括广告位位置、广告位尺寸、广告投放延迟、日期和时间、浏览器 如何消除？CoEC 期望点击 概念：将某广告位相当长一段时间内的平均点击数作为其关注程度的近似估计 评估对象：在广告质量完全随机的情况下，广告位或其他属性对应的平均点击率 偏差模型：从数据中近似学习除期望点击的方法是只用那些偏差因素作为特征，训练一个点击率模型，称为偏差模型 CoEC：归一化点击率指标，即点击与期望点击的比值 点击反馈的平滑 当使用点击率或CoEC特征时，若展示数较小，可在分子和分母上各加上一个常数进行平滑 点击率模型评价 准确率Precision / 召回率Recall（PR）：PR曲线下方面积无意义 ROC / AUC：把ROC曲线下的面积称为曲线下方面积(AUC)，有明确的物理意义，在一定程度上能表征了对h=0和h=1事件估计值排序的正确性，是常用的量化指标","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"点击率预测","slug":"点击率预测","permalink":"https://www.notlate.net/tags/%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E6%B5%8B/"},{"name":"探索与利用","slug":"探索与利用","permalink":"https://www.notlate.net/tags/%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8/"}]},{"title":"计算广告-笔记13：竞价广告核心技术","slug":"AdComputation/13.竞价广告核心技术","date":"2020-10-03T08:41:51.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/f3b32133.html","link":"","permalink":"https://www.notlate.net/posts/f3b32133.html","excerpt":"","text":"竞价广告计算法 常用的定价策略 广义第二高价(GSP) 市场保留价(MRP) 价格挤压 最关键的两个计算问题 广告检索 广告排序 搜索广告系统 优化目标 关键技术 查询扩展 意义：需求方通过扩展关键词获得更多的流量，供给方借此来变现更多流量和提高竞价的激烈程度 扩展方法 第五章介绍了精确匹配、短语匹配、广泛匹配、否定匹配，本章主要介绍广泛匹配 基于推荐的方法：构建会话和关键词的交互强度矩阵，可通过协同过滤(CF)方法进行推荐 CF具体划分 基于内存的非参数化方法 基于模型的参数化方法 空间降维与主题模型区别 推荐问题中：把未观测到的交互单元视为未知 主题模型中：认为未在某文档中出现的词交互强度为0 基于主题模型的方法 计算两个词的主题向量相似度，主要考虑的是语义上的相关性，非用户意图上的相关性，因此效果略差 基于历史效果的方法 从历史数据中发现某些关键词对某些特定广告主的eCPM较高，那么将这些查询记录下来。 对营收效果往往好于上两种 检索技术：倒排索引是搜索引擎的关键技术 点击率预测：14章重点介绍 广告放置：搜索引擎广告中确定各区放置广告条数的问题，是一个典型的带约束优化问题 广告网络 特点：除搜索广告以外最重要的非实时竞价类广告产品 优化目标 广告网络的成本是分成或包断媒体资源，因此成本项去掉 系统架构 广告投放的决策流程 服务器接收前端用户访问出发的广告请求 根据上下文信息和用户身份标识，从页面标签库和用户标签库查出标签 用这些标签以及其他广告请求条件从广告索引中找到符合要求的广告候选集 利用CRT预估模型计算所有候选广告的eCPM 根据eCPM排序选出赢得竞价的广告，返回给前端 离线计算流程 广告网络需要根据广告投放的历史展示和点击数据，对点击率进行建模 受众定向功能计算 流式计算流程 准实时的计费和点击反作弊功能必不可少 将用户行为尽快反馈到广告决策中对于点击率预估和受众定向效果提升也很关键 关键技术 点击率预测 受众定向 短时行为反馈与计算 一些需要快速对在线日志进行处理的场景，催生了计算平台 具体场景 实时反作弊：过滤爬虫流量、突发的作弊流量等 实时计费：将预算耗完的广告及时下线 短时用户标签：利用用户分钟级别的行为数据加工用户短时兴趣的标签 短时动态特征：CTR中的动态特征 流计算编程接口：Storm等 广告检索 基本的倒排索引在广告索引中遇到的新问题 广告的定向条件组合，可以看成是一个由与或关系连接的布尔表达式 在上下文关键词和用户标签比较丰富时，查询条件可能非常多 广告检索技术 布尔表达式的检索 表达形式：DNF 每个DNF都可以分解成1个或多个交集的并 每个交集又可以分解为1个或多个赋值集的交 特点 1.某次请求的定向标签满足某个交集(conjunction)时，一定满足包含该交集的所有广告 2.在交集的倒排索引中，可以通过请求标签的个数过滤掉比其数目多的交集(因为其条件更多更精准) 相关性检索：WAND算法 1.合理性：与最终排序使用的评价函数近似 2.高效性：在检索阶段实现快速评价 基于DNN的语义建模 DSSM模型 Youtube个性化推荐模型 线上应用DNN模型时，往往使用用户和广告的嵌入向量进行最近邻检索 哈希算法 数据无关 局部敏感哈希(LSH)：原始数据空间中距离近的样本点比距离远的样本点在哈希后更容易碰撞 数据相关/学习哈希 语义哈希 深度学习哈希 向量量化算法 概念：对向量x做整体量化，将其映射为K个离散向量中的一个，从而压缩数据的算法 常见K均值算法 乘积量化 层次K均值树HKM 基于图的算法 相对于HKM的优势 1.检索时可以从任意节点开始访问，因此可以做高并发检索 2.小世界网络可以通过相对较少的长度连接使大多数节点之间的联通路径有较短的长度（能更快的找到与查询距离更近的相似数据） 常见算法 NSW 1.构建索引时，通过逐步插入节点去构造小世界网络，在构造网络的初始阶段建立的连接即成为相对较少的长程连接 2.查询索引时，从多个节点并发检索，直到检索出的top节点收敛","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"广告网络","slug":"广告网络","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E7%BD%91%E7%BB%9C/"}]},{"title":"计算广告-笔记12：受众定向核心技术","slug":"AdComputation/12.受众定向核心技术","date":"2020-10-03T04:11:11.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/d44b943f.html","link":"","permalink":"https://www.notlate.net/posts/d44b943f.html","excerpt":"","text":"定义 对广告(a)、用户(u)、上下文©三个维度提取有意义的标签的过程 受众定向是在线广告，特别是展示广告最核心的驱动力之一 技术分类 用户标签t(u) 特点：以用户历史日志数据为依据 分类 人口属性定向：规模化获取人口属性比较困难，需要数据驱动的模型，以用户行为为基础自动预测其人口属性 行为定向 建模问题 建模目标 找出某个类型的广告上eCPM较高的人群 若点击价值近似一致，则等价于：找出在该类型广告上点击率较高的人群 建模对象：某个用户在某类广告上的点击量 建模方法： 广义线性模型 1.根据目标值的特性选择合适的指数族分布描述 由于点击行为是离散到达的随机变量，对其数量最自然的概率描述是泊松分布。 h：某个用户在某个定向类别广告上的点击量(单位有效展示的点击数) t：某个受众标签 λt：控制点击行为到达频繁性参数 含义解读：根据某用户在不同类别广告的点击量和控制参数(用户行为加权和)计算他在某类广告（不同受众标签）上的点击率 - 2.用线性模型将多个自变量和指数族分布的参数联系起来 把用户行为与频繁性参数λt联系起来 wt：标签t对应的行为定向模型需要优化的参数 n：不同的行为类型，比如搜索、浏览、购买等 x(b)：原始行为b经过特征选择函数后，作为特征输入(看下文) 含义解读：不同行为的加权和作为参数λt 特征生成 x(b)函数如何选择？ 最常用函数：将一段时间内的原始用户行为映射到确定的标签体系上，同时计算出各行为在对应标签上的累积强度作为模型的特征输入 - 累计控制在一段时间内方法 - 1.滑动窗法 - 设定时间窗D，把D内的全部行为强度累加求和 ![](https://gitee.com/acdance/img/raw/master/blogs/image-20210203214821267.png) - 2.时间衰减法 - 设定衰减因子α，用上一个时间片的累计特征与本时间片的行为强度递归得到本时间片的累计特征 ![](https://gitee.com/acdance/img/raw/master/blogs/image-20210203214832090.png) - 优点：只需要保存前一时间片的累计特征和当前时间片的行为强度，时间和空间复杂度都很低 x(b)函数计算方法： 即把不同行为映射到一个或者多个定向标签上 - 1.网页浏览、分享等与内容相关的行为 - 用有监督文本主题模型方法，映射到预先定义好的标签体系上 - 提取内容中的关键字 - 2.广告点击等与广告活动相关行为 - 转化为对落地页内容的分析，然后类似于1 3.搜索、搜索点击等与查询相关行为 - 利用搜索引擎做内容扩展 - 对查询进行某垂直领域分类时，直接利用相应垂直媒体的标签体系和搜索引擎 4.转化等需求行为 对应到一个单品，利用单品分类信息，可以将其映射到某个标签上。 w的计算方法： 即模型训练集的组织和生成方式 - 训练集长度：为了消除工作日的周期性影响，训练集的天数一般选择7的倍数 - 时间片大小：反映了对定向的时效性的要求，若想更快的利用行为数据对标签做出调整，则缩小时间片 决策过程 不需要λ到h的泊松分布，只需要计算线性函数λ的值，然后根据预先确定的阈值来确定某个用户是否应该被打上某个定向标签 计算过程比训练过程的数据准备要简单，只需要根据时间衰减法和特征映射函数计算出λ(d)即可。 reach/CRT曲线进行半定量评测 reach：标签接触到的人群规模 曲线含义：标签覆盖的人群规模越小，则应该越准确，也即对该类型的广告的CRT较高；随着人群规模覆盖的扩大，CRT逐渐走低 生成此曲线只需要遍历一遍数据 上下文标签t© 特点：以当前环境信息为依据 分类： 地域定向 - 频道定向 - 上下文定向 - 方法 - 根据广告请求中的参数信息，经过简单运算就可以得到。比如：地域定向、频道/URL定向、操作系统定向等 - 基于上下文页面的一些特征标签进行定向，比如：关键词、主题、分类等。如何获取页面信息呢？ - 半在线抓取系统 - 工作原理 - 1.在线广告请求到来 - 2.如果该请求的上下文URL在缓存中，则直接返回对应的标签 - 3.如果URL在缓存中不存在，为了广告请求能够及时响应，则先返回空标签集合(做随机或默认推荐)；同时立刻向后台抓取队列加入此URL，然后打标签入缓存 - 4.考虑到页面内容可能会不定期更新，则可以设置合适的TTL自动失效缓存 - 优点 - 1.在线缓存处理效率极高，仅仅抓取有广告请求的URL，节省爬虫资源 - 2.只抓取需要的页面，且第一次广告请求后很快得到页面标签，使得页面的信息覆盖率较高 - 主要思路 - 思路1：\b用规则将页面进行归类(为频道或主题分类) - 思路2：提取页面中的关键词 - 思路3：提取页面入链锚文本中的关键词，技术较难 - 思路4：提取页面流量来源中的搜索关键词，更接近行为定向 - 思路5：主题模型将页面内容映射到语义空间中的一组主题上，泛化广告主需求，提高市场流动性和竞价水平 - 类别 - 1.监督学习方法：预先定义好主题集合，用监督学习方法将文档映射到某一个元素上 - 2.无监督学习方法：不预先定义主题集合，控制主题的总个数，用无监督学习方法自动学习出主题集合以及文档到这些主题的映射函数 - 用途 - 广告效果优化的特征提取：两者都可以 - 对广告主售卖的标签体系：优先考虑监督学习方法 - 方法 - 潜在语义分析LSA模型：对文档和词组成的矩阵X进行奇异值分解(SVD)，找到这一矩阵的主要模式。 - 概率潜在语义索引PLSA方法：通过对文档生成过程进行概率建模来进行主题分析 - 文档主题模型LDA方法：在文档信息不足或噪声较大时，利用贝叶斯框架对结果做有效的平滑 - 词嵌入word2vec方法：不能够针对具体问题学习语义表示 - 基础技术：关键词提取 - 方法1：按照信息检索中的一般方法，选取页面内容中TF-IDF较高的词作为关键词 - 方法2：需求方驱动，从广告商相关描述中得到商业价值高的关键词表和IDF，再与页面内容中关键词的TF一起计算TF-IDF。 广告主定制化标签t(a,u) 特点：完全开放的标签体系，与广告主数目成正比，适合于程序化交易中由需求方提供，以便与上下文或用户标签进行匹配 分类 重定向 新客推荐","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"受众定向","slug":"受众定向","permalink":"https://www.notlate.net/tags/%E5%8F%97%E4%BC%97%E5%AE%9A%E5%90%91/"}]},{"title":"计算广告-笔记11：合约广告核心技术","slug":"AdComputation/11.合约广告核心技术","date":"2020-10-03T02:09:31.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/5df7f5ff.html","link":"","permalink":"https://www.notlate.net/posts/5df7f5ff.html","excerpt":"","text":"广告位合约(CPT) 一般采用广告排期系统来管理和执行 将广告素材按预先确定的排期插入媒体页面，\b并通过内容分发系网络CDN)加速访问 广告未能返回时，需设定防天窗广告补位 展示量合约(CPM) 对应的广告系统称为担保式投送(GD)系统 系统架构 在线投放引擎接收用户触发的广告请求 根据用户标签和上下文标签找到可以匹配的广告合约 在线分配模块决定本次展示投放那个广告（最重要） 决策完成后，将展示和点击日志送入数据高速公路。 一部分进入离线分布式平台，通过日志整理确定在线分配算法的参数，再将分配方案送给在线投放引擎使用。 另一部分日志送到流式计算平台，在反作弊和计价基础上，再对索引进行快速调整。 核心技术 流量预测 给定一组受众标签组合，估算将来某个时间段内符合该受众标签组合的广告展示量 主要方法包括基于历史统计来拟合未来流量和时间序列分析方法 基于反向索引的流量预测方法 频次控制 一般来说，随着某个用户看到同一个创意频次的提升，点击率会呈下降趋势，因此控制某个广告曝光给同一用户的次数，可以提升广告主的ROI 有客户端和服务端两种解决方案 客户端：将频次记录在cookie中 优点：简单易行，服务成本低 缺点：扩展性差，若跟踪多个广告频次时，cookie变重，影响广告响应时间。 移动端利用SDK控制方案非常好 服务端：在后台设置一个专门用来频次记录和更新的缓存 缓存特点 高并发读 高并发写 轻量级缓存方案 频次存储的规模是有上限的，比如在某个周期内统计。 当用(a,u)的组合生成缓存中对应的key时，可以接受不处理冲突 在线分配 两个主要挑战 在量的约束下优化效果 实时对每一次展示做出决策 一般将问题转化为一个二部图匹配的问题 供给节点：广告库存 需求节点：广告合约量 如果某个供给节点的受众标签能够满足某个需求节点的要求，就在两个节点间建立一条连接边 可将问题表示为带约束的优化问题 需求约束：分配给某广告合约a的收益要至少等于其约定的量d qia：供给节点i连接到需求节点a的单位流量惩罚 si：供给节点i的总供给量 xia：si分配给合约a的比例 供给约束：每个供给节点被分配出去的量小于等于总流量 引入供给约束和需求约束后的在线分配优化问题框架 ![](https://gitee.com/acdance/img/raw/master/blogs/image-20210203215006126.png) 问题举例 GD问题(按CPM售卖) 不考虑合约未完成惩罚：目标函数是常数，此时优化目标主要在于更好的满足所有的合约，而不是优化eCPM 考虑合约未完成惩罚：目标函数不是常数，可以增加一项惩罚项约束即可。 AdWords问题(有预算约束的出价问题，CPC) - 目标函数和需求约束增加参数qia ![](https://gitee.com/acdance/img/raw/master/blogs/image-20210203215021460.png) - 优化目标：整个市场收入最大化 - 供给约束：与GD一致，总分配比例≤1 - 需求约束：每个广告主的花费总和应该小于该广告主的预算 简单理解 - 把供给节点i理解为搜索广告中的1个关键词 qia：广告a对关键词i的出价（关键词i的一次点击分配给广告a的期望收益） si：关键词i的总点击量 xia：关键词i分配给广告a的流量比例 极限性能 - 研究指标：某在线分配策略的有效性 - 最优解：如果能够完全确定所有的流量分布，可以根据全局信息求得分配的最优解 - e-competitive：在线分配策略最差情况下能够达到最优解目标函数的e倍，称分配方案是e-competitive的 - 带约束优化求解： - 最好的极限性能上限：1-1/e 实用优化算法 - 分配方案要求 - 紧凑性：离线分配的复杂度和线上响应时间 - 无状态：投放策略与前面的投放历史无关 - 常见优化方法 - 直接求解的原始分配方案：利用历史流量拟合未来流量，把在线分配转化成离线问题 - 基于对偶算法的紧凑分配方案 - 综合分配方案SHALE - 启发式分配方案HWM（最常用） - 根据历史流量确定每个广告合约资源的紧缺程度，得到分配优先级 - 根据优先级确定各个广告合约的分配比例","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"核心技术","slug":"核心技术","permalink":"https://www.notlate.net/tags/%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/"}]},{"title":"计算广告-笔记10：基础知识储备","slug":"AdComputation/10.基础知识储备","date":"2020-10-02T10:09:45.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/e7a52cc7.html","link":"","permalink":"https://www.notlate.net/posts/e7a52cc7.html","excerpt":"","text":"信息检索 倒排索引：从大量文档中查找包含某些词的文档集合 向量空间模型(Vector Space Model, VSM)：最基础最重要的相似度度量方法之一 文档表示方法：用各个关键词在文档中的强度(如TF-IDF)组成的矢量来表示文档 词频(Term Frequency, TF)：某文档中，该词出现的频率 倒数文档频率(Inverse document frequency, IDF)：该词在所有文档中出现的频繁程度的倒数 DF(m)：出现词m的文档总数目 N：总的文档数目 出现m的文档越多，则DF(m)越大，N不变，则IDF越小，表示该词重要性越低 相似度度量方法：一般采用余弦相似度 两个矢量在尺度上没有归一化时，仍然可以得到比较稳健的结果 最优化 给定某个确定的目标函数，及该函数自变量的一些约束条件，求解该函数的最大或最小值的问题 有约束的优化问题转化为无约束问题 拉格朗日乘子法 引入拉格朗日对偶函数，对偶问题的最优值是原问题最优值的下界 原问题是凸函数时，两者完全一致，称为强对偶。但是不是只有凸函数才是强对偶。 KKT条件 无约束问题求解方法 不可导或求导代价大 下降单纯形法 容易求导：线搜索方法 先确定方向，再计算步长 梯度下降法 牛顿法 拟牛顿法 置信域法：每次迭代，将搜索范围限制到x的置信域内，然后同时决定下次迭代的方向和步长；如果当前置信域内找不到可行解，则缩小置信域范围。 统计机器学习 最大似然估计： 把模型的参数看成是固定的，找到使得训练数据上似然值最大的参数 最大熵(ME) 原理：当在某些约束条件下选择统计模型时，需要尽可能选择满足这些条件的模型中不确定性最大的那个。 最大熵解&lt;==&gt;对应指数型分布的最大似然解 指数族分布(单模态) 指数族分布参数的最大似然估计，可以完全由其充分统计量u(x)得到。 重要的指数族分布 高斯分布 γ分布 β分布 多项式分布 混合模型(多模态) 高斯混合模型(Mixture of Gaussians, MoG) 概率潜在语义索引(Probabilistic Latent Semantic Index, PLSI) EM算法 贝叶斯估计： 模型参数服从一定分布的随机变量 共轭先验 经验贝叶斯 深度学习 神经网络优化方法：梯度下降法 卷积神经网络CNN 循环神经网络RNN 生成对抗网络GAN","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.notlate.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","permalink":"https://www.notlate.net/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算广告-笔记09：计算广告技术概览","slug":"AdComputation/09.计算广告技术概览","date":"2020-10-02T08:49:27.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/e90e8794.html","link":"","permalink":"https://www.notlate.net/posts/e90e8794.html","excerpt":"","text":"个性化系统框架 计算广告是根据个体用户信息投送个性化内容的典型系统之一 个性化系统的构成部分 用于实时响应请求，完成决策的『在线投放引擎』 离线的『分布式计算』数据处理平台 用于在线实时反馈的『流计算平台』 连接和转运上面三部分数据流的『数据高速公路』 协作流程 『在线投放系统』的日志接入『数据高速公路』 『数据高速公路』把日志转运到『离线分布式平台』和『流计算平台』 『离线分布式平台』周期性的批处理历史数据，得到人群标签和其他模型参数，放到缓存中，供『在线投放系统』决策使用； 『流计算平台』准实时的处理最近一小段时间内的数据，得到准实时的用户标签和其他模型参数，放到缓存中，供『在线投放系统』决策使用。 web规模问题比较 主要准则 搜索：相关性 搜索广告&amp;展示广告：利润 个性化推荐：用户兴趣 索引规模 搜索：十亿级 搜索广告：千万/百万级 展示广告：百万级 个性化推荐：亿级/百万级 各类广告系统优化目标 展示量合约：满足各合约带来的约束 ADN：预估和优化点击率，与广告主出的点击单价相乘得到期望收入 ADX：直接用广告主的展示单价作为期望收入 DSP：预估和优化点击率、点击价值和成本 计算广告系统架构 广告投放引擎 广告投放机：接受广告前端web服务器发来的请求，完成广告投放决策并返回最后页面片段的主逻辑 广告检索：根据用户标签和页面标签，从广告索引中查找广告候选 广告排序：高效计算eCPM，并进行排序 若依赖点击率估计：则需要用到离线计算得到的CTR模型和特征；以及流计算得到的实时点击率特征 若依赖点击价值估计：则需要点击价值估计模型 收益管理：统一代表那些在各种广告系统中将局部广告排序结果进一步调整，以全局收益最优为目的做调整的功能，需要用到离线计算好的某种分配计划 GD：在线分配 DSP：出价策略 广告请求接口 定制化用户划分：从广告主处收集用户信息的产品接口 数据高速公路 将在线投放的数据准实时传输到离线分布式计算与流式计算平台上 由于受众定向建模时需要用到广告系统以外的日志或者第三方数据，因此也需要支持收集这些数据源 离线数据处理 主要目标 统计日志得到报表、仪表盘数据等，供人工决策 利用数据挖掘、机器学习技术进行手中定向、点击率预估、分配策略规划等，为在线决策提供支持 主要模块 用户会话日志生成：从各个渠道收集来的日志，需要整理成以用户ID为key的统一存储格式 行为定向：根据日志行为给用户打结构化标签库中的某些标签 上下文定向：半在线的页面抓取和页面标签的缓存，给上下文页面打标签 点击率建模：在分布式计算平台上训练得到点击率模型参数和特征 分配规划：利用离线日志进行规划，得到适合线上执行的分配方案 商业智能系统：包括ETL过程、仪表板和Cube。以人为最终接口的数据处理和分析流程的总结 广告管理系统：广告操作者与广告系统的接口 在线数据处理 在线反作弊：实时判断作弊流量并去除 计费：通知广告索引系统下线预算耗尽的广告 在线行为反馈：包括实时受众定向和实施点击反馈，将短时间内发生的用户行为和广告日志加工成实时用户标签和实时点击率模型特征。 实施索引：实时接收广告投放数据，建立倒排索引 计算广告系统的主要技术 算法优化角度 特征提取：对a,u,c打标签 点击率预测：估计eCPM 在线分配：量的约束、投放时即时决策 机制设计：在多方博弈的市场中达到动态平衡时的收益最大化 探索与利用(E&amp;E)：更全面的采样整个(a,u,c)空间以便更准确的估计点击率 个性化推荐：用于实时竞价中效果类DSP的个性化重定向中 系统架构角度 广告主的预算、定向条件等信息设置后，线上需要快速生效 ===&gt; 实时索引技术服务于广告候选的检索 需要用NoSQL数据库为投放时提供用户、上下文标签和其他特征 MapReduce分布式计算平台进行大规模数据挖掘和建模；流计算平台实现短时用户行为和点击反馈 实现高并发、快速响应的实时竞价接口 用开源工具搭建计算广告系统 web服务器Nginx：高性能、高并发、低内存 分布式配置和集群管理工具ZooKeeper：基础原理Paxos算法 全文搜索引擎Lucene，基于Lucene用于云计算的Elasticsearch 跨语言通信接口Thrift 数据高速公路Flume 分布式数据处理平台Hadoop 特征在线缓存Redis 流计算平台Storm 高效的迭代计算框架Spark","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"广告投放","slug":"广告投放","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E6%8A%95%E6%94%BE/"},{"name":"数据处理","slug":"数据处理","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"计算广告-笔记08：信息流与原生广告","slug":"AdComputation/08.信息流与原生广告","date":"2020-10-02T05:28:16.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/ccab977a.html","link":"","permalink":"https://www.notlate.net/posts/ccab977a.html","excerpt":"","text":"移动广告 移动广告：移动互联网上的广告产品 特点 场景广告的可能性：移动设备可以对用户行为模式进行全天候的监测和分析 大量潜在的本地化广告主：移动环境下，GPS等多种精确定位的手段，使基于精确地理位置的本地化广告变得可行。 传统创意形式 横幅与插屏 误点击率高，打断用户任务，用户体验差 后续转换率差 开屏与锁屏 推荐墙与积分墙 点击和激活数据很好，但是后续活跃程度比较差 挑战 应用生态造成的行为数据割裂 许多PC时代的广告主移动化程度还不够(\b网站没有移动端)，无法充分消化移动广告带来的流量 移动广告的产品形态需要一次革命：原生化才是移动广告最重要的方向 原生广告产品 所有将商业化内容和非商业化内容统一生产或者混合排序的产品，都可以认为与原生广告产品有关系。原生广告产品方向，也经常被称为“内容即广告”，最主要的形式是信息流广告。 原生概念的两种诉求 表现原生：将广告的展示风格和样式变得和内容一致，从而做到产品形式上的原生 场景原生：将广告的投放决策略逻辑和内容生产相一致，从而做到用户场景上的原生 信息流广告 定义：信息流广告指的是这样一种产品形式，首先，广告以与内容联动的方式进行交互；其次，被广告区隔开的各部分内容之间没有直接的关联 产品关键 广告位适配 广告竞价与放置：在平均广告条数的约束下，通过调整每个用户的S和K，优化总体广告的点击率。 视频激励广告 近年来原生广告领域的重要创新形式 从游戏场景自然带入、与游戏的积分体系无缝对接，为用户带来沉浸式的广告体验 植入式原\b生广告：如在某拉萨游记的网页上插入拉萨酒店的广告 其他原生广告产品形态 搜索广告：展示形式与自然搜索结果基本一致 软文广告 联盟","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"信息流","slug":"信息流","permalink":"https://www.notlate.net/tags/%E4%BF%A1%E6%81%AF%E6%B5%81/"},{"name":"原生广告","slug":"原生广告","permalink":"https://www.notlate.net/tags/%E5%8E%9F%E7%94%9F%E5%B9%BF%E5%91%8A/"}]},{"title":"计算广告-笔记07：数据加工与交易","slug":"AdComputation/07.数据加工与交易","date":"2020-10-02T03:19:55.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/e28268fb.html","link":"","permalink":"https://www.notlate.net/posts/e28268fb.html","excerpt":"","text":"有价值的数据来源 数据类别 用户标识：cookie、IDFA、IMEI 用户行为 决策行为：转化、预转化 主动行为：广告点击、搜索、搜索点击 半主动性为：分享和网页浏览 被动行为：广告浏览 人口属性 地理位置 设备信息 社交关系 基本规律 用户主动意图的提升，相应的行为数据价值越大 越接近转化的行为，对效果广告的精准指导作用越强 数据管理平台DMP 三方数据划分 第一方数据：广告主的数据 第二方数据：广告平台的数据(广告网络主要使用) 第三方数据：不直接参与广告交易的其他数据提供方的数据 第一方DMP 广告主自建DMP 委托其他DMP进行数据托管和加工 第三方DMP(又称数据交易平台) 主要功能：聚合各种来源的在线的用户行为数据，并加工成有价值的用户标签，然后在广告市场上通过售卖标签变现 交易方式：以广告为载体交易 交易过程通过ADX或者SSP作为中转 DMP–&gt;(批量)–&gt;ADX ADX–&gt;(部分)–&gt;(多个)DSP 优势 由于数据量级比较大，如果DMP直接与各DSP交易，则总体数据传输成本非常高。 所有的DSP、数据提供方只需要与ADX进行cookie映射，以ADX为中心的星形拓扑结构更加方便 可以实现部分数据交易(DSP从ADX购买需要的部分数据) ADX在数据的买卖双方之前起到数据使用量的监测和计费作用 存在的问题 数据的重复售卖会引起数据价格向流量价格的转移 系统架构图： 见12.6节图 1.通过部署在媒体上的代码或SDK收集第一方访问日志，送入数据高速公路 2.通过数据高速公路收集自由的第二方数据，然后将这些原始行为映射到结构化或者非结构化的受众标签体系上 3.第三方提供的加工好的标签数据直接进入用户标签集，再通过统一接口对外提供 隐私保护和数据安全 隐私保护问题 隐私保护基本原则 要严格避免使用个人可辨识信息 用户有权要求系统停止跟踪和使用自己的行为数据 不应长期保留和使用用户行为数据 工程上需要特别注意权限的严格分配和最小数据访问原则 准标识符与K匿名 一些数据组合在一起可以确定一个人，这样的信息称为准标识符 将准标识符作一定程度的泛化，如果泛化的结果能使每一组准标识符的实例都能找到K条与其相同的，就是K匿名 稀疏行为的数据挑战 通过稀疏的行为数据，可以比较容易定位自己熟悉的人 差分隐私：对数据集进行一定程度的随机扰动，尽可能少损失查询准确率的情况下，使隐私泄露的风险最低。 程序化交易中的数据安全 供给方的数据安全 询价优化技术，尽可能阻止以恶意收集数据的DSP参与竞价 需求方的数据安全 避免DSP在竞争对手之间倒卖访客数据 欧盟通用数据保护条例(General Data Protection Regulations, GDPR)","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"数据加工","slug":"数据加工","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%B7%A5/"},{"name":"数据交易","slug":"数据交易","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E4%BA%A4%E6%98%93/"}]},{"title":"计算广告-笔记06：程序化交易广告","slug":"AdComputation/06.程序化交易广告","date":"2020-10-02T01:19:34.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/cc17c06f.html","link":"","permalink":"https://www.notlate.net/posts/cc17c06f.html","excerpt":"","text":"程序化交易市场产生背景 在线广告发展到竞价阶段，基本的计算格局已经建立 需求方进一步加强优化效果，广告网络无法完全满足诉求 市场的发展方向是向需求方彻底开放 允许广告主按照已经定义好的用户划分购买 提供广告主自行选择流量和每次展示独立出价功能 因此：广告展示时集询价、出价和竞价于一体 实时竞价(RTB) 实时竞价产生的原因 广告网络这样封闭式竞价方案，无法规模化和精细化地针对定制化标签做投放。 将竞价过程开放，在广告展示时由需求方判断是否需要并出价，就产生了实时竞价 实时竞价的关键产品目标：用定制化标签指导产品投放 实时竞价的影响 解放了效果类广告需求 为品牌广告带来了全新的机会 合约广告&amp;竞价广告中的人群定义方式由广告平台决定，需求方基本没有加工的自由 实时竞价交易中，DSP可以根据市场上采买的各种数据，为某个特定的广告主加工特有人群，完成更加符合其市场策略的人群触及 实时竞价的流程 用户标识映射 从广告主网站向DSP服务器发起cookie映射请求 DSP和ADX服务器之间通信完成cookie映射(详见15章) 广告请求 用户接触到媒体广告位，前端向ADX发送广告请求 ADX向DSP传送URL和用户ID，发起询价请求 DSP根据数据决定是否竞价，参与则返回出价 固定时间片后，ADX选择出价最高的DSP返回给媒体 媒体从胜出的DSP拿到广告创意并展示 实时竞价存在的问题 每次展示都有ADX服务器与多个DSP服务器交互，使得带宽成本大大增加 询价过程中，ADX要等待一个约定好的时间片(一般100ms)，这使得用户看到广告的延迟增大，对CTR有负面影响。若移动客户端有缓存，则影响较小 原理上DSP每次都是以极低的价格参数竞价，从而极低成本获得媒体中的用户行为，存在信息泄露风险 程序化交易 概念：广告交易依赖机器间的在线通信，而非事先约定或由人工操作完成，实时竞价(RTB)是最主要的程序化交易形式 核心目的：让需求方可以自由的选择流量和出价 程序化相关交易方式分类 库存类型 价格模式 参与方式 担保投送优化 预留 定价 1 V 1 非预留定价交易 非预留 定价 1 V 1 邀请制竞价交易 非预留 竞价 1 V 少量 公开竞价交易 非预留 竞价 1 V 多个 其他程序化交易方式 优选(Preferred Deal, PD) 只有一个需求方的程序化交易 早于实时竞价产生 允许需求方按照自己的意愿来挑选流量，避免了复杂的竞价过程 出价会高于市场价格的CPM单价 私有市场 为了保证广告主的质量，将拍卖限制在一些被邀请的需求方的小范围内 兼顾优选与实时竞价的优点 程序化直投 特点 属于直接购买与实时竞价之间过渡的一种方式 交易本身仍然以定价、保量或半保量的方式完成 需求方可以自行对采买的库存做广告投放决策 应用场景 跨媒体频次控制 多个子产品流量分配 一定比例的还量 广告交易平台ADX 功能 将媒体流量以拍卖的方式售卖给DSP 类比于证券交易所 产品策略 无需广告库 无需广告检索 排序也较简单 核心问题：询价优化，即如何解决与多个DSP发广告请求带来的带宽和机器成本的上升 结算方式 竞价广告网络(非实时)一般为CPC结算，因为广告位复杂。 实时竞价一般为CPM结算，因为每次广告展示时，DSP可以获取到广告位信息，所以DSP可以精确估计点击率和合理的eCPM。 需求方平台DSP 两个核心特征 RTB、优选等程序化的流量购买方式 支持需求方定制化用户划分的能力 具备的能力 点击率预测 点击价值估计 流量预测 站外推荐 产品策略问题 与广告网络相似 决策过程 检索 排序 定价 出价(关键策略) 基础 竞价广告网络中，eCPM只是为了结果排序，因此不需要太精确 DSP中，每次展示都需要按CPM向ADX报价，因此eCPM需要准确 策略 首先通过历史的观察和预测，得到一天内ADX成交价随时间变化的曲线 然后将预算分配到eCPM与市价比例较大的流量上去 受众定向方式 数据来源：以第一方数据为核心，结合第二、三方数据定制化标签 定制化标签 重定向 网站重定向 个性化重定向，3个关键点 动态创意 推荐引擎，可以看作是站外推荐 广告主商品库存实时接口，避免商品已经售完或下架带来的形象伤害 搜索重定向：搜索过于广告主直接相关的关键词的用户群作为重定向集合 新客推荐(look-alike) 由广告主第一方数据提供一份种子用户 再由拥有更丰富数据的第二方数据的广告平台分析种子用户行为特征，并根据这些特征找到具有相似特征的拓展人群 供给方平台SSP 动态分配广告请求：CPT&amp;CPM合约 &gt; 自运营广告库 or 广告网络ADN or RTB方式对接DSP 产品策略关键环节 聚合：集成多个ADN，动态决定向谁请求广告 广告投放：支持投放媒体自行销售的(内部)广告，但是内部广告和外部ADN一起排序 市场：以RTB方式向DSP请求广告，与ADX同质化趋势越来越强 一般决策过程 先估算被聚合的ADN和内部广告的eCPM，排序选出最优作为低价 再向DSP发送询价请求 催生数据加工和交易市场(详见第7章) 数据交易平台 数据管理平台(DMP)","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"程序化交易","slug":"程序化交易","permalink":"https://www.notlate.net/tags/%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93/"}]},{"title":"计算广告-笔记05：搜索广告&竞价广告","slug":"AdComputation/05.搜索广告&竞价广告","date":"2020-10-01T07:05:54.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/8a804683.html","link":"","permalink":"https://www.notlate.net/posts/8a804683.html","excerpt":"","text":"搜索广告 产品形态 定义：搜索广告是以查询词为粒度进行受众定向，并按照竞价方式售卖，按CPC结算的广告产品 竞价标的物：竞价关键词 特点 变现能力，即eCPM远远高于一般的展示广告，因为用户主动输入的查询直接反映了用户的意图 受众定向标签就是上下文的搜索查询 展示形式与自然结果非常接近 从搜索广告发展起来的竞价交易模式，逐渐发展成为互联网广告最主流的交易模式。 探索趋势 丰富文字链创意的展示形式 在通用广告链接上增加更多有表现力的信息点 增加Logo 增加联系方式 直接展示结构化的广告内容摘要或直接访问功能 订票酒店等快捷入口 拓展弱相关广告形式 同类产品推荐功能 原生化探索 如商品直达式广告 搜索广告产品策略 搜索广告决策过程 查询扩展 介绍：搜索广告独有的策略 目的：为广告主自动拓展相关查询词，扩大匹配流量 匹配方式 精确匹配：不对广告主的关键词进行扩展 短语匹配：用户查询完全包括广告主关键词 广泛匹配：通过数据挖掘算法进行匹配 否定匹配：黑名单方式灵活关停低效流量 检索 主要依靠关键词-广告的倒排索引 排序 主要依据eCPM=点击率*出价 放置 介绍：确定各个区域放置的广告条数 定价 产品案例 Google AdWords 创造性的引入了点击率的概念 CPM–&gt;CPC 淘宝直通车 淘宝与广告主之间是共生关系 由于淘宝对广告主的全部转换流程了解，极大促进利用后续数据优化广告系统 位置拍卖与机制设计 竞价广告可以看作是一种位置拍卖问题，即将前S个高出价的广告依次放到前面排序好的S个广告位上 机制设计问题 市场保留价(MRP) 设置一个赢得拍卖的最低价格 最优起价与竞价和质量度的分布有关 定价机制 概念：如何对获得某个位置的广告商收取合适的费用 目标：市场稳定，纳什均衡 定价策略 多广告位：广义第二高价GSP， 单广告位：第二高价(SP) VCG 基本思想：对于赢得某位置的广告主，其所付出的成本应该等于他占据这个位置给其他市场参与者带来的价值损害 竞价系统均衡状态下最优的定价策略 稳定状态下，理性广告主是忠实出价的 相对于其他策略，这种定价向广告主收取的费用是最少的 单广告位时退化为第二高价 价格挤压 公式 价格挤压因子作用 k–&gt;∞时，只通过点击率排序 k–&gt;0时，只通过出价进行排序 k的增大，相当于挤压出价的作用 收入最优 Myerson最优拍卖 影响定价和排序结果，难以向广告主解释其公平性，采用较少 竞价广告网络 产品形态 既有竞价售卖，也有合约售卖 特点 根据变现能力(eCPM)决定每次展示分配给哪个广告主，不做量的约定 按人群售卖，极力淡化媒体和广告位概念 不需要再满足品牌广告主独占的要求，提高市场流动性 根据实际消耗结算，因此广告主要先充值，改善运营方的现金流状况 CPC合理性 广告位聚合售卖，广告主无法知道具体位置，因此无法评估每次展示的出价。而CPC没有这个问题 广告网络更容易估计点击率，需求方根据点击价值出价，市场分工更为合理。 决策过程：检索、排序、定价 广告检索：可以描述成一个相关性检索问题，与搜索广告不同，竞价广告网络中用户意图不明确，需要更多的关键字、兴趣标签 广告排序：主要是点击率预估相对较难 点击数据更加稀疏，使得新广告、新策略的冷启动问题非常突出 广告位差别大，点击率变动范围较大 产品案例 Google Display Network, GDN：世界上最大的展示广告网络 需求方产品 搜索营销引擎(SEM) 帮助需求方选择合适的关键词 对不同关键词给予合适的出价 交易终端(TD)：面向ADN、DSP等的一站式采买平台 与合约广告比较 供给方角度 合约广告 vs 竞价广告 ≈ 计划经济 vs 市场经济 合约广告：供给方完成 量的保证和质的优化 竞价广告：市场只负责制定（竞价和收费的）规则，各广告主量的竞争完全采用市场竞争 需求方角度 合约广告 缺点：对广告主而言缺乏透明性 优点：可以保证量，对于品牌性质较强的广告有意义 竞价广告 特点：供给方不承诺量，需求方自行决定出价 意义：广告效果大幅提升 非常精细的受众定向可以无障碍的使用在交易中 市场规模扩张、大量中小广告主成为竞价主体 数据价值提升，整个市场开始以数据为核心来组织和运营广告产品","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"搜索广告","slug":"搜索广告","permalink":"https://www.notlate.net/tags/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A/"},{"name":"竞价广告","slug":"竞价广告","permalink":"https://www.notlate.net/tags/%E7%AB%9E%E4%BB%B7%E5%B9%BF%E5%91%8A/"}]},{"title":"计算广告-笔记04：合约广告","slug":"AdComputation/04.合约广告","date":"2020-10-01T04:00:10.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/a973a56.html","link":"","permalink":"https://www.notlate.net/posts/a973a56.html","excerpt":"","text":"广告位合约 最早产生的在线广告售卖方式 形式：指媒体和广告主约定\b在某一时间段内在某些广告位上固定投送广告主的广告 结算方式：CPT 广告排期系统 DFP(DoubleClick) 好耶Allyes 百度广告管家 优点：品牌冲击、橱窗效应、可提供附加服务(竞品排他) 缺点：受众定向之前，无法做到按照受众投放，难以深入优化效果 发展：随着受众定向技术发展，广告位合约也可以根据受众不同投送创意 变种：轮播售卖 受众定向 方法评价 效果：符合该定向方式的流量高出平均效果的水平 规模：流量占整体广告库存流量的比例 方法概览 地域定向 很直觉也很早广泛使用的方法 也可以认为是一种上下文定向 人口属性定向 标签包括:年龄、\b性别、受教育程度、收入水平等 效果未必突出，但是在传统广告种大量使用 准确进行人口属性定向不容易 专门的数据来源：实名制的SNS注册信息等 根据已知人口属性数据构造分类器对未知数据自动标注 上下文定向 概念：根据网页或应用的具体内容定向 粒度：关键词、主题、广告主需求确定的分类 优点：覆盖率高，往往可以根据当前浏览的页面推测用户的即时兴趣 应用：ADN中首选的定向方法之一 行为定向 概念：基于用户的历史访问行为挖掘用户兴趣 形式：可以认为是一系列上下文定向的融合结果 行为定向的框架、算法和评价指标奠定了在线广告数据驱动的本质特征，催生了数据加工和交易的衍生业务 精确位置定向 要素：移动蜂窝、WiFi、GPS、蓝牙 应用：大量区域性非常强的小广告主有机会投放精准广告，与地域定向有质的区别 重定向 概念：对广告主过去一段时间内的访客再次投放广告以提升效果 效果：公认的精准程度高、效果最突出，但是人群覆盖量较小 新客推荐定向 概念：根据广告主提供的种子访客，找到行为相似的潜在客户 动态定价 非定向广告技术 形式：直接显示打折或降价的促销信息 团购也是一种变相的广告形式 场景定向 移动环境下的新问题，基于用户使用手机时的不同背景状态 标签体系 背景：一些反映用户兴趣的受众定向方法(如行为定向)，需要一个标签体系，将用户映射到其中一个或者几个标签上去 组织形式 结构化层次标签体系：适用于将标签座位广告投放的直接标的，既方便广告主理解，又方便其选择 非结构化的标签体系：当标签仅仅是投放系统需要的中间变量，如CTR预测或其他模块的变量输入时 特殊形式：关键词，直接按照搜索或者浏览内容的关键词划分人群，比较精准 设计思路 分行业 关键思路：深入研究该行业的用户决策过程 汽车行业：价格&gt;车型&gt;品牌 游戏行业： 电商行业：不依靠分类，以单品+个性化推荐方法构建标签体系 展示量合约(GD) 概念介绍 未摆脱广告位标的物：媒体从固定广告位售卖变为CPM售卖，为了在流量变现的基础上加上数据变现，面向的仍然是原来的品牌广告主 无法把多个差别很大的广告位打包成同一标的物售卖，因为曝光有效性差别可能较大。 流量预测 展示量合约售卖的是某特定人群上的广告曝光次数，人群不同于确定的广告位，因此必须在合约中约定投放的量，继而产生了流量预测 流量预测的主要用途是：售前指导、在线流量分配、出价指导 流量塑形 主动影响流量，以利于合约的达成 在线分配 各个合约要求的人群可能重叠，需要设计合理的在线分配策略，使得各个合约得到满足 简化为一个二部图匹配问题 二部图介绍 一方标识广告库存的共计节点，每个节点代表人群标签对应的流量集合 另外一方表示广告合约的需求节点，每个节点代表的是一个广告合约的人群标签条件 常用方法：高水位方法HWM 典型产品场景 视频广告资源 全球主要门户网站的品牌性广告位","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"合约广告","slug":"合约广告","permalink":"https://www.notlate.net/tags/%E5%90%88%E7%BA%A6%E5%B9%BF%E5%91%8A/"}]},{"title":"计算广告-笔记03：在线广告产品概览","slug":"AdComputation/03.在线广告产品概览","date":"2020-10-01T03:50:40.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/a7fc312.html","link":"","permalink":"https://www.notlate.net/posts/a7fc312.html","excerpt":"","text":"在线广告产品形式 合约广告产品(后续效果不易直接衡量的品牌类广告主) 按时间段售卖的CPT广告 按展示量售卖的CPM广告 竞价广告产品 搜索广告 ADN 程序化交易广告产品 主要推动力：市场中广告主数据和第三方数据的使用与变现，也催生了相关的数据交易市场 原生广告产品 搜索广告和社交网络信息流广告作了非常有价值的探索 商业产品 广告产品属于商业产品 商业产品的关键点 需要有一个可优化的既定商业目标 特别关注产品的策略部分 以广告为例重要的策略：竞价中的机制设计、冷启动时的数据探索、受众定向的标签体系 特别关注数据，让运营和产品优化形成闭环 和用户打交道的产品界面上，追求便捷行 广告系统的产品接口 广告主层级组织和投放管理 广告主 广告推广计划 广告推广组 广告创意 供给方管理接口 \b主要用于一般的ADN或ADX 供需之间的接口形式 投放界面 JS SDK API","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"在线广告","slug":"在线广告","permalink":"https://www.notlate.net/tags/%E5%9C%A8%E7%BA%BF%E5%B9%BF%E5%91%8A/"}]},{"title":"计算广告-笔记02：计算广告基础","slug":"AdComputation/02.计算广告基础","date":"2020-10-01T03:15:17.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/aaa4de02.html","link":"","permalink":"https://www.notlate.net/posts/aaa4de02.html","excerpt":"","text":"广告有效性原理 曝光阶段：广告从物理上展现出来的过程 关注阶段：受众从物理上接触到广告，到意识上关注到它的过程 理解阶段：受众理解广告传达信息的过程 接受阶段：受众认可广告传达信息的过程 保持阶段： 决策阶段： 互联网广告的技术特点 技术和计算导向 数字媒体使在线广告可以进行精细化的受众定向，而技术又使得广告决策喝交易朝着计算驱动的方向发展 效果的可衡量性 以展示和点击的形势直接记录和优化广告效果 不同产品或时代，点击率绝对值比较没有\b那么重要 特定时期不同广告和算法表现出来的差异更有意义。 创意和投放方式的标准化 标准化的驱动力来自于受众定向和程序化交易 需求方关注的是人群而非广告位，因此创意尺寸的统一化与关键接口标准化非常关键 IAB：视频广告的VAST标准和实时竞价的OpenRTB标准 媒体概念的多样化 各种性质媒体相互搭配 数据驱动的投放决策 根本驱动力：数据的深入利用和加工 计算广告的核心问题 为一系列用户与上下文的组合找到最合适的广告投放策略以优化整体广告活动的利润 广告收入分解 点击率(Click Through Rate, CTR)：点击次数与展现次数的比例 到达率：落地页成功打开次数与点击次数的比例 转化率(Conversion Rate, CVR)：转化次数与到达次数的比例 eCPM(千次展示收益) = r(a,u,c) = μ(a,u,c) * \bν(a,u,c)，其中 μ: 点击率，ν: 点击价值(到达率、转化率、\b客单价乘积) 结算方式 CPT(Cost per Time) 需求方估计点击率和点击价值 优点：品牌效应和橱窗效应 缺点：无法利用受众定向技术 场景：占广告位与时间段，强曝光属性广告 CPM(Cost per Mille)千次展示 需求方估计点击率和点击价值 优点：可利用受众定向选择目标人群 缺点：合约售卖受众不能划分太细 场景：由需求方估计和控制风险，适合品牌广告，实时竞价广告 CPC(Cost per Click) 点击率：由供给方估计 点击价值：由需求方估计 优点：可精细划分人群 场景：竞价广告网络，最早用于搜索广告 CPS(Cost per Sale)/CPA(Action)/ROI(Return of Invesment) 需求方按照最终转化收益结算，最大程度规避风险 供给方估计点击率和点击价值，运营难度大 CPA有两种特殊类型 CPL(Lead)：收集销售线索的直接效果广告，按照线索数结算 CPI(Install)：移动应用下载的直接效果广告，按照安装数结算 oCPM(Optimized CPM) Fcebook主推，按照CPM结算，根据转化率优化 供给方承担估计点击率和点击价值\b任务 积累到一定转化量，数据上判断CPA可行后，转为CPA结算 极大的降低了广告主的投放工作量 场景：数据能力强的广告平台 在线广告相关行业协会 交互广告局(IAB) 供给方利益 \b展示广告创意尺寸标准 视频广告标准VAST 通用\b实时竞价接口标准OpenRTB 美国广告代理协会(4A) 没过线上线下各种广告代理商的行业协会 美国国家广告协会(ANA) 需求方利益组织","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"广告基础","slug":"广告基础","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E5%9F%BA%E7%A1%80/"}]},{"title":"计算广告-笔记01：在线广告综述","slug":"AdComputation/01.在线广告综述","date":"2020-10-01T02:15:34.000Z","updated":"2021-03-18T15:04:38.462Z","comments":true,"path":"posts/63879d85.html","link":"","permalink":"https://www.notlate.net/posts/63879d85.html","excerpt":"","text":"商业化 将流量、数据、影响力3项资产通过商业产品形式转变成收入的过程 广告定义 定义 广告是由已确定的出资人通过各种媒介进行的有关产品的，通常是有偿的、有组织的、综合的、劝服性的非人员的传播活动 两个主动参与方 需求方(如广告主) 供给方(如媒体) 一个被动参与方 受众 表现形式 横幅 文字链 富媒体 视频 交互式 移动 社交 邮件营销 激励 团购 游戏联运 固定位导航 在线广告简史 固定位合约—&gt;受众定向 展示位合约 定向广告/担保式投送(Guaranteed Delivery, GD) 展示量结算合约—&gt;竞价交易 搜索广告(付费搜索) 上下文广告 广告网络(AD Network, ADN) 交易终端(Trading Desc, TD) 实时竞价(Real Time Bidding, RTB) 供给方：广告交易平台(AD Exchange, ADX) 需求方：需求方平台(Demand Side Platform, DSP) 广告位与内容：独立–&gt;统一 原生广告","categories":[{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"在线广告","slug":"在线广告","permalink":"https://www.notlate.net/tags/%E5%9C%A8%E7%BA%BF%E5%B9%BF%E5%91%8A/"}]}],"categories":[{"name":"深度学习推荐系统","slug":"深度学习推荐系统","permalink":"https://www.notlate.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"论文笔记","slug":"论文笔记","permalink":"https://www.notlate.net/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"分布式","slug":"分布式","permalink":"https://www.notlate.net/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"计算广告","slug":"计算广告","permalink":"https://www.notlate.net/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"},{"name":"工程实践","slug":"工程实践","permalink":"https://www.notlate.net/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"},{"name":"知识图谱","slug":"知识图谱","permalink":"https://www.notlate.net/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"协同过滤","slug":"协同过滤","permalink":"https://www.notlate.net/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"name":"人工智能","slug":"人工智能","permalink":"https://www.notlate.net/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"发展趋势","slug":"发展趋势","permalink":"https://www.notlate.net/tags/%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/"},{"name":"GNN","slug":"GNN","permalink":"https://www.notlate.net/tags/GNN/"},{"name":"MMoE","slug":"MMoE","permalink":"https://www.notlate.net/tags/MMoE/"},{"name":"业界主流","slug":"业界主流","permalink":"https://www.notlate.net/tags/%E4%B8%9A%E7%95%8C%E4%B8%BB%E6%B5%81/"},{"name":"模型评估","slug":"模型评估","permalink":"https://www.notlate.net/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"},{"name":"推荐模型","slug":"推荐模型","permalink":"https://www.notlate.net/tags/%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"https://www.notlate.net/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"传统算法","slug":"传统算法","permalink":"https://www.notlate.net/tags/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95/"},{"name":"线上服务","slug":"线上服务","permalink":"https://www.notlate.net/tags/%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1/"},{"name":"Embedding","slug":"Embedding","permalink":"https://www.notlate.net/tags/Embedding/"},{"name":"特征工程","slug":"特征工程","permalink":"https://www.notlate.net/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"mongodb","slug":"mongodb","permalink":"https://www.notlate.net/tags/mongodb/"},{"name":"数据库","slug":"数据库","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.notlate.net/tags/redis/"},{"name":"测试框架","slug":"测试框架","permalink":"https://www.notlate.net/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"},{"name":"ADX","slug":"ADX","permalink":"https://www.notlate.net/tags/ADX/"},{"name":"DSP","slug":"DSP","permalink":"https://www.notlate.net/tags/DSP/"},{"name":"SSP","slug":"SSP","permalink":"https://www.notlate.net/tags/SSP/"},{"name":"点击率预测","slug":"点击率预测","permalink":"https://www.notlate.net/tags/%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E6%B5%8B/"},{"name":"探索与利用","slug":"探索与利用","permalink":"https://www.notlate.net/tags/%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8/"},{"name":"广告网络","slug":"广告网络","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E7%BD%91%E7%BB%9C/"},{"name":"受众定向","slug":"受众定向","permalink":"https://www.notlate.net/tags/%E5%8F%97%E4%BC%97%E5%AE%9A%E5%90%91/"},{"name":"核心技术","slug":"核心技术","permalink":"https://www.notlate.net/tags/%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.notlate.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"广告投放","slug":"广告投放","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E6%8A%95%E6%94%BE/"},{"name":"数据处理","slug":"数据处理","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"信息流","slug":"信息流","permalink":"https://www.notlate.net/tags/%E4%BF%A1%E6%81%AF%E6%B5%81/"},{"name":"原生广告","slug":"原生广告","permalink":"https://www.notlate.net/tags/%E5%8E%9F%E7%94%9F%E5%B9%BF%E5%91%8A/"},{"name":"数据加工","slug":"数据加工","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%B7%A5/"},{"name":"数据交易","slug":"数据交易","permalink":"https://www.notlate.net/tags/%E6%95%B0%E6%8D%AE%E4%BA%A4%E6%98%93/"},{"name":"程序化交易","slug":"程序化交易","permalink":"https://www.notlate.net/tags/%E7%A8%8B%E5%BA%8F%E5%8C%96%E4%BA%A4%E6%98%93/"},{"name":"搜索广告","slug":"搜索广告","permalink":"https://www.notlate.net/tags/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A/"},{"name":"竞价广告","slug":"竞价广告","permalink":"https://www.notlate.net/tags/%E7%AB%9E%E4%BB%B7%E5%B9%BF%E5%91%8A/"},{"name":"合约广告","slug":"合约广告","permalink":"https://www.notlate.net/tags/%E5%90%88%E7%BA%A6%E5%B9%BF%E5%91%8A/"},{"name":"在线广告","slug":"在线广告","permalink":"https://www.notlate.net/tags/%E5%9C%A8%E7%BA%BF%E5%B9%BF%E5%91%8A/"},{"name":"广告基础","slug":"广告基础","permalink":"https://www.notlate.net/tags/%E5%B9%BF%E5%91%8A%E5%9F%BA%E7%A1%80/"}]}